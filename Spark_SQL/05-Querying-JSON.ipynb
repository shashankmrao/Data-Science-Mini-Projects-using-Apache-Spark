{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Querying JSON & Hierarchical Data with SQL\n\nApache Spark&trade; and Databricks&reg; make it easy to work with hierarchical data, such as nested JSON records.\n\n## In this lesson you:\n* Use SQL to query a table backed by JSON data\n* Query nested structured data\n* Query data containing array columns\n\n## Audience\n* Primary Audience: Data Analysts\n* Additional Audiences: Data Engineers and Data Scientists\n\n## Prerequisites\n* Web browser: Chrome or Firefox\n* Lesson: <a href=\"$./02-Querying-Files\">Querying Files with SQL</a>\n* Concept: <a href=\"https://www.w3schools.com/sql/\" target=\"_blank\">Basic SQL</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/a3098jg2t0?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/a3098jg2t0?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Examining the contents of a JSON file\n\nJSON is a common file format in big data applications and in data lakes (or large stores of diverse data).  Datatypes such as JSON arise out of a number of data needs.  For instance, what if...  \n<br>\n* Your schema, or the structure of your data, changes over time?\n* You need nested fields like an array with many values or an array of arrays?\n* You don't know how you're going use your data yet so you don't want to spend time creating relational tables?\n\nThe popularity of JSON is largely due to the fact that JSON allows for nested, flexible schemas.\n\nThis lesson uses the `DatabricksBlog` table, which is backed by JSON file `dbfs:/mnt/training/databricks-blog.json`. If you examine the raw file, you can see that it contains compact JSON data. There's a single JSON object on each line of the file; each object corresponds to a row in the table. Each row represents a blog post on the <a href=\"https://databricks.com/blog\" target=\"_blank\">Databricks blog</a>, and the table contains all blog posts through August 9, 2017."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/1i3n3rb0vy?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/1i3n3rb0vy?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/databricks-blog.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 65536 bytes]\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html&quot;, &quot;authors&quot;: [&quot;Tomer Shiran (VP of Product Management at MapR)&quot;], &quot;id&quot;: 33, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Partners&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-10&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-10&quot;}, &quot;title&quot;: &quot;MapR Integrates the Complete Apache Spark Stack&quot;, &quot;slug&quot;: &quot;mapr-integrates-spark-stack&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;post-meta\\&quot;&gt;This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.&lt;/div&gt;\\n\\n&lt;hr /&gt;\\n\\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world\\u2019s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an &lt;a href=\\&quot;http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\\&quot; target=\\&quot;_blank\\&quot;&gt;interview&lt;/a&gt; with Stefan Groschupf, CEO of Datameer.\\n\\nToday, I am happy to &lt;a href=\\&quot;http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\\&quot; target=\\&quot;_blank\\&quot;&gt;announce&lt;/a&gt; and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\\n&lt;h2 id=\\&quot;why-spark\\&quot;&gt;Why Spark?&lt;/h2&gt;\\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\\n\\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\\n&lt;h2 id=\\&quot;why-databricks\\&quot;&gt;Why Databricks?&lt;/h2&gt;\\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\\n\\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\\n\\n&lt;a href=\\&quot;http://w.on24.com/r.htm?e=780379&amp;amp;s=1&amp;amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;amp;partnerref=MapR\\&quot; target=\\&quot;_blank\\&quot;&gt;Register for an upcoming joint webinar&lt;/a&gt; to learn more about the benefits of the complete Spark stack on MapR.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;tdas&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html&quot;, &quot;authors&quot;: [&quot;Tathagata Das&quot;], &quot;id&quot;: 35, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;, &quot;Machine Learning&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-10&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-10&quot;}, &quot;title&quot;: &quot;Apache Spark 0.9.1 Released&quot;, &quot;slug&quot;: &quot;spark-0_9_1-released&quot;, &quot;content&quot;: &quot;We are happy to announce the availability of &lt;a href=\\&quot;http://spark.apache.org/releases/spark-release-0-9-1.html\\&quot; target=\\&quot;_blank\\&quot;&gt;Apache Spark 0.9.1&lt;/a&gt;! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\\n\\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\\n\\nVisit the &lt;a href=\\&quot;http://spark.apache.org/releases/spark-release-0-9-1.html\\&quot; target=\\&quot;_blank\\&quot;&gt;release notes&lt;/a&gt; for more information about all the improvements and bug fixes. &lt;a href=\\&quot;http://spark.apache.org/downloads.html\\&quot; target=\\&quot;_blank\\&quot;&gt;Download&lt;/a&gt; it and try it out!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html&quot;, &quot;authors&quot;: [&quot;Steven Hillion&quot;], &quot;id&quot;: 37, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Partners&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-04-01&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-04-01&quot;}, &quot;title&quot;: &quot;Application Spotlight: Alpine Data Labs&quot;, &quot;slug&quot;: &quot;application-spotlight-alpine&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;post-meta\\&quot;&gt;This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \\&quot;Certified on Apache Spark\\&quot; program.&lt;/div&gt;\\n\\n&lt;hr /&gt;\\n\\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At &lt;a href=\\&quot;http://www.alpinenow.com\\&quot; target=\\&quot;_blank\\&quot;&gt;Alpine Data Labs&lt;/a&gt;, we think what we\\u2019re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you\\u2019re working with the most innovative and powerful technologies.\\n\\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. \\u201cWill I get to do that at Alpine?\\u201d he asked.\\n\\nIf it had been even a year earlier, I would have said \\u201cSure\\u2026at some point.\\u201d But in the meantime I\\u2019d met several of the members of the &lt;a href=\\&quot;https://amplab.cs.berkeley.edu\\&quot; target=\\&quot;_blank\\&quot;&gt;AMPLab&lt;/a&gt; research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I\\u2019d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. \\u201cYou\\u2019ll be working on Spark from day one.\\u201d\\n\\nLast week, Alpine announced at &lt;a href=\\&quot;http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\\&quot; target=\\&quot;_blank\\&quot;&gt;GigaOM&lt;/a&gt; that it\\u2019s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on &lt;a href=\\&quot;http://www.gopivotal.com/big-data/analytics-workbench\\&quot; target=\\&quot;_blank\\&quot;&gt;Pivotal\\u2019s Analytics Workbench&lt;/a&gt;, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\\n\\nFurthermore, we were officially certified on Spark by the team at Databricks. It\\u2019s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\\n\\nSpark is more to us than just speed. It\\u2019s really the entire ecosystem that represents such an exciting paradigm for working with data.\\n\\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\\n\\nWe\\u2019ve always had this mantra at Alpine: \\u201cAvoid multiple passes through the data!\\u201d And we\\u2019ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it\\u2019s rare that we can avoid it entirely. With Spark, it\\u2019s incredibly satisfying to watch the progress bar zip along as the system re-uses data it\\u2019s already seen before.\\n\\nAnother thing that\\u2019s getting our engineers excited is Spark\\u2019s &lt;a href=\\&quot;http://spark.apache.org/mllib/\\&quot; target=\\&quot;_blank\\&quot;&gt;MLLib&lt;/a&gt;, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the &lt;a href=\\&quot;http://madlib.net/\\&quot; target=\\&quot;_blank\\&quot;&gt;MADlib&lt;/a&gt; library of analytics functions for databases, and Alpine now uses it extensively.) So we\\u2019re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\\n\\nSo I\\u2019ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I\\u2019ll also congratulate the Spark team and Databricks for making it possible!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;michael&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html&quot;, &quot;authors&quot;: [&quot;Michael Armbrust&quot;, &quot;Reynold Xin&quot;], &quot;id&quot;: 42, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-27&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-27&quot;}, &quot;title&quot;: &quot;Spark SQL: Manipulating Structured Data Using Apache Spark&quot;, &quot;slug&quot;: &quot;spark-sql-manipulating-structured-data-using-spark-2&quot;, &quot;content&quot;: &quot;Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce &lt;a href=\\&quot;https://spark.apache.org/docs/latest/sql-programming-guide.html\\&quot;&gt;Spark SQL&lt;/a&gt;, a new component recently merged into the Spark repository.\\n\\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark\\u2019s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Import relational data from Parquet files and Hive tables&lt;/li&gt;\\n \\t&lt;li&gt;Run SQL queries over imported data and existing RDDs&lt;/li&gt;\\n \\t&lt;li&gt;Easily write RDDs out to Hive tables or Parquet files&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;h2 id=\\&quot;spark-sql-in-action\\&quot;&gt;Spark SQL In Action&lt;/h2&gt;\\nNow, let\\u2019s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark\\u2019s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Users(userId INT, name String, email STRING,\\nage INT, latitude: DOUBLE, longitude: DOUBLE,\\nsubscribed: BOOLEAN)&lt;/li&gt;\\n \\t&lt;li&gt;Events(userId INT, action INT)&lt;/li&gt;\\n&lt;/ul&gt;\\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\\n\\n[scala]\\n// Data can easily be extracted from existing sources,\\n// such as Apache Hive.\\nval trainingDataTable = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT e.action\\n         u.age,\\n         u.latitude,\\n         u.logitude\\n  FROM Users u\\n  JOIN Events e\\n  ON u.userId = e.userId&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Since `sql` returns an RDD, the results of the above\\n// query can be easily used in MLlib\\nval trainingData = trainingDataTable.map { row =&amp;gt;\\n  val features = Array[Double](row(1), row(2), row(3))\\n  LabeledPoint(row(0), features)\\n}\\n\\nval model =\\n  new LogisticRegressionWithSGD().run(trainingData)\\n[/scala]\\n\\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\\n\\n[scala]\\nval allCandidates = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT userId,\\n         age,\\n         latitude,\\n         logitude\\n  FROM Users\\n  WHERE subscribed = FALSE&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Results of ML algorithms can be used as tables\\n// in subsequent SQL statements.\\ncase class Score(userId: Int, score: Double)\\nval scores = allCandidates.map { row =&amp;gt;\\n  val features = Array[Double](row(1), row(2), row(3))\\n  Score(row(0), model.predict(features))\\n}\\nscores.registerAsTable(&amp;quot;Scores&amp;quot;)\\n\\nval topCandidates = sql(&amp;quot;&amp;quot;&amp;quot;\\n  SELECT u.name, u.email\\n  FROM Scores s\\n    JOIN Users u ON s.userId = u.userId\\n  ORDER BY score DESC\\n  LIMIT 100&amp;quot;&amp;quot;&amp;quot;)\\n\\n// Send emails to top candidates to promote the service.\\n[/scala]\\n\\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\\n\\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the &lt;a href=\\&quot;http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\\&quot;&gt;programming guide&lt;/a&gt;.\\n&lt;h2 id=\\&quot;optimizing-with-catalyst\\&quot;&gt;Optimizing with Catalyst&lt;/h2&gt;\\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\\n&lt;h2 id=\\&quot;future-of-shark\\&quot;&gt;Future of Shark&lt;/h2&gt;\\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\\n\\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\\n&lt;h2 id=\\&quot;whats-next\\&quot;&gt;What\\u2019s next&lt;/h2&gt;\\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Generating custom bytecode to speed up expression evaluation&lt;/li&gt;\\n \\t&lt;li&gt;Reading and writing data using other formats and systems, include Avro and HBase&lt;/li&gt;\\n \\t&lt;li&gt;API support for using Spark SQL in Python and Java&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;patrick&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/03/release-0_9_0.html&quot;, &quot;authors&quot;: [&quot;Patrick Wendell&quot;], &quot;id&quot;: 58, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-04&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-04&quot;}, &quot;title&quot;: &quot;Apache Spark 0.9.0 Released&quot;, &quot;slug&quot;: &quot;release-0_9_0&quot;, &quot;content&quot;: &quot;Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark\\u2019s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\\n\\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark\\u2019s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\\n\\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we\\u2019ve focused on stabilizing API\\u2019s ahead of Apache Spark\\u2019s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\\n\\nApache Spark 0.9.0 can be &lt;a href=\\&quot;http://spark.incubator.apache.org/downloads.html\\&quot;&gt;downloaded directly&lt;/a&gt; from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the &lt;a href=\\&quot;http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\\&quot;&gt;official release notes&lt;/a&gt;. Enjoy the newest release of Spark!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ali&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/01/01/simr.html&quot;, &quot;authors&quot;: [&quot;Ali Ghodsi&quot;, &quot;Ahir Reddy&quot;], &quot;id&quot;: 65, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Ecosystem&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-01-02&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-01-02&quot;}, &quot;title&quot;: &quot;Apache Spark In MapReduce (SIMR)&quot;, &quot;slug&quot;: &quot;simr&quot;, &quot;content&quot;: &quot;Apache Hadoop integration has always been a key goal of Apache Spark and &lt;a href=\\&quot;http://hortonworks.com/wp-content/uploads/2013/06/YARN.png\\&quot;&gt;YARN&lt;/a&gt; users have long been able to run &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\\&quot;&gt;Spark on YARN&lt;/a&gt;. However, up to now, it has been relatively hard to run Spark on Hadoop MapReduce v1 clusters, i.e. clusters that do not have YARN installed. Typically, users would have to get permission to install Spark/Scala on some subset of the machines, a process that could be time consuming. Enter &lt;a href=\\&quot;http://databricks.github.io/simr/\\&quot;&gt;SIMR (Spark In MapReduce)&lt;/a&gt;, which has been released in conjunction with &lt;a href=\\&quot;https://databricks.com/blog/2013/12/19/release-0_8_1.html\\&quot;&gt;Apache Spark 0.8.1&lt;/a&gt;.\\n\\nSIMR allows anyone with access to a Hadoop MapReduce v1 cluster to run Spark out of the box. A user can run Spark directly on top of Hadoop MapReduce v1 without any administrative rights, and without having Spark or Scala installed on any of the nodes. The only requirements are HDFS access and MapReduce v1. SIMR is open sourced under the &lt;a href=\\&quot;http://apache.org/licenses/LICENSE-2.0.html\\&quot;&gt;Apache license&lt;/a&gt; and was jointly developed by Databricks and UC Berkeley &lt;a href=\\&quot;http://amplab.cs.berkeley.edu\\&quot;&gt;AMPLab&lt;/a&gt;.\\n\\nThe basic idea is that a user can download the package of SIMR (&lt;a href=\\&quot;http://databricks.github.io/simr/#download\\&quot;&gt;3 files&lt;/a&gt;) that matches their Hadoop cluster and immediately start using Spark. SIMR includes the interactive Spark shell, and allows users to use the shell backed by the computational power of the cluster. It is a simple as &lt;code&gt;./simr --shell&lt;/code&gt;:\\n\\n&amp;nbsp;\\n\\n&lt;img src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/simrshell.png\\&quot; alt=\\&quot;simrshell\\&quot; width=\\&quot;90%\\&quot; /&gt;\\n\\nRunning a Spark program simply requires bundling it along with its dependencies into a jar and launching the job through SIMR. SIMR uses the following command line syntax for running jobs:\\n\\n&lt;pre&gt;./simr jar_file main_class parameters&lt;/pre&gt;\\n\\nSIMR simply launches a MapReduce job with the desired number of map slots, and ensures that Spark/Scala and your job gets shipped to all those nodes. It then designates one of the mappers as the master and runs the Spark driver inside it. On the rest of the mappers it launches Spark executors that will execute tasks on behalf of the driver. Voila, your Spark job is running inside MapReduce on the cluster.\\n\\nSIMR lets users interact with the driver program. For example, users can type into the Spark shell and see its output interactively. The way this works is that SIMR runs a relay server on the master mapper and a relay client on the machine that launched SIMR. Any input from the client and output from the driver are relayed back and forth between the client and the master mapper.\\n\\nTo make all this work, SIMR makes extensive use of HDFS. The master mapper is elected using leader election by writing to HDFS and picking the mapper that firsts gets to write to HDFS. Similarly, the executors launched inside the rest of the mappers find out the driver\\u2019s URL by reading it from a specific file from HDFS. Thus, SIMR uses MapReduce and HDFS in place of a cluster manager.\\n\\n&lt;img src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/simr-arch.png\\&quot; alt=\\&quot;simr-arch\\&quot; width=\\&quot;90%\\&quot; /&gt;\\n\\nSIMR is not intended to be used in production mode, but rather to enable users to explore and use Spark before running it on a proper resource manager, such as YARN, Mesos, or Standalone Mode. MapReduce 2 (YARN) users can of course use the existing &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\\&quot;&gt;Spark on YARN&lt;/a&gt; solution, or explore other &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/index.html#launching-on-a-cluster\\&quot;&gt;Spark deployment options&lt;/a&gt;.\\n\\nWe hope SIMR will enable users to try out Spark without any heavy operational burden. Towards this goal, we have pre-built several SIMR binaries for different versions of Hadoop. Please go ahead and try it and let us know if you have any feedback.\\n\\nSIMR resources:\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://databricks.github.io/simr\\&quot;&gt;Homepage&lt;/a&gt;&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://databricks.github.io/simr#download\\&quot;&gt;Download&lt;/a&gt;&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;https://github.com/databricks/simr\\&quot;&gt;Source code&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n\n*** WARNING: skipped 18619 bytes of output ***\n\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;roy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/18/spark-certification.html&quot;, &quot;authors&quot;: [&quot;Databricks Press Office&quot;], &quot;id&quot;: 2411, &quot;categories&quot;: [&quot;Announcements&quot;, &quot;Company Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-19&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-19&quot;}, &quot;title&quot;: &quot;Databricks announces \\&quot;Certified on Apache Spark\\&quot; Program&quot;, &quot;slug&quot;: &quot;spark-certification&quot;, &quot;content&quot;: &quot;&lt;strong&gt;BERKELEY, Calif. \\u2013 March 18, 2014 \\u2013&lt;/strong&gt; Databricks, the company founded by the creators of Apache Spark that is revolutionizing what enterprises can do with Big Data, today announced the Databricks &lt;a href=\\&quot;/certification/\\&quot;&gt;\\u201cCertified on Spark\\u201d Program&lt;/a&gt; for applications built on top of the Apache Spark platform. This program ensures that certified applications will work with a multitude of commercially supported Spark distributions.\\n\\n\\u201cPioneering application developers that are leveraging the power of Spark have had to choose between two sub-optimal choices: they either have to package Spark platform support with their application or attempt to maintain integration/certification individually with a rapidly increasing set of commercially supported Spark distributions,\\u201d said Ion Stoica, Databricks CEO. \\u201cThe Databricks \\u2018Certified on Spark\\u2019 program enables developers to certify solely against the 100% open-source Apache Spark distribution, and ensures interoperability with Apache Spark-compatible distributions. Databricks will handle the task of certifying the compatibility of each commercial Spark distribution with the Apache version and will soon announce the initial set of distributions that meet this criteria.\\u201d\\n\\nThe pioneering partners of the Databricks certification program include Adatao, Alpine Data Labs, and Tresata - advanced analytics application vendors that have been at the forefront in leveraging the power of Spark to deliver faster and deeper insights for their enterprise customers.\\n\\n\\u201cCertified on Spark\\u201d also provides multiple benefits for enterprise users including:\\n&lt;ul&gt;\\n \\t&lt;li&gt;Decoupling Spark distribution (and commercial support) from application licensing&lt;/li&gt;\\n \\t&lt;li&gt;Full transparency into which applications are truly designed to work with and leverage the power of Spark&lt;/li&gt;\\n \\t&lt;li&gt;A rapidly increasing set of certified applications to incentivize distribution vendors to maintain compatibility with Apache Spark and avoid forking and fragmentation, ultimately resulting in greater compatibility and shared innovation for users&lt;/li&gt;\\n&lt;/ul&gt;\\n\\u201cAt Databricks we are fully committed to keeping Spark 100% open source and to bringing it to the widest possible set of users,\\u201d said Matei Zaharia, Databricks CTO and the creator of Spark. \\u201cThe \\u2018Certified on Spark\\u2019 program is key to maintaining a healthy and unified Apache Spark platform, and it is an expression of our community focused efforts, such as organizing meetups and the upcoming Spark Summit, and driving the future of open-source development of Spark.\\u201d\\n\\nApplication developers that are interested in applying for the \\u201cCertified on Spark\\u201d program should visit &lt;a href=\\&quot;http://www.databricks.com\\&quot;&gt;www.databricks.com&lt;/a&gt; and select \\u201cApply for Certification\\u201d. Enterprise users can also visit the Databricks site regularly to see the latest set of certified application vendors and read \\u201capplication spotlight\\u201d blog articles that deep-dive into specific examples of Spark-powered applications.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ion&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/03/02/spark-apache-top-level-project.html&quot;, &quot;authors&quot;: [&quot;Ion Stoica&quot;], &quot;id&quot;: 2412, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-03-03&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-03-03&quot;}, &quot;title&quot;: &quot;Apache Spark Now a Top-level Apache Project&quot;, &quot;slug&quot;: &quot;spark-apache-top-level-project&quot;, &quot;content&quot;: &quot;&lt;div class=\\&quot;blogContent\\&quot;&gt;\\n\\nWe are delighted with the recent &lt;a href=\\&quot;https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\\&quot;&gt;announcement&lt;/a&gt; of the Apache Software Foundation that &lt;a href=\\&quot;http://spark.apache.org\\&quot;&gt;Apache Spark&lt;/a&gt; has become a top-level Apache project. This is a recognition of the fantastic work done by the Spark open source community, which now counts over 140 developers from 30+ companies.\\n\\nIn short time, Spark has become an increasingly popular solution for numerous big data applications, including machine learning, interactive queries, and stream processing. Spark now is an integral part of the Hadoop ecosystem, with many organizations employing Spark to perform sophisticated processing on their Hadoop data.\\n\\nAt Databricks we are looking forward to continuing our work with the open source community to accelerate the development and adoption of Apache Spark. Currently employing the lead developers and creators of many of the components of the Spark ecosystem, including Matei Zaharia, the creator of Spark, and Patrick Wendell, the release manager of Spark, we are committed to the success of Apache Spark.\\n\\n&lt;/div&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;rxin&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/12/big-data-benchmark.html&quot;, &quot;authors&quot;: [&quot;Ahir Reddy&quot;, &quot;Reynold Xin&quot;], &quot;id&quot;: 2413, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-13&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-13&quot;}, &quot;title&quot;: &quot;AMPLab updates the Big Data Benchmark&quot;, &quot;slug&quot;: &quot;big-data-benchmark&quot;, &quot;content&quot;: &quot;The AMPLab at UC Berkeley, with help from Databricks, recently released an update to the &lt;a href=\\&quot;https://amplab.cs.berkeley.edu/benchmark/\\&quot;&gt;Big Data Benchmark&lt;/a&gt;. This benchmark uses Amazon EC2 to compare performance of five popular SQL query engines in the Big Data ecosystem on common types of queries, which can be reproduced through publicly available scripts and datasets.\\n\\nIn the past year, the community has invested heavily in performance optimizations of query engines. We are glad to see that all projects have evolved in this area. Although the queries used in the benchmark are simple, we are proud that Shark remains one of the fastest engines for these workloads, and has improved significantly since the last run.\\n\\nWhile this benchmark reaffirms Shark as a highly performant SQL query engine, we are working hard at Databricks to push the boundaries further. Stay tuned for some exciting news we will share soon with the community.\\n&lt;ul&gt;\\n\\t&lt;li&gt;&lt;a href=\\&quot;https://amplab.cs.berkeley.edu/benchmark/\\&quot;&gt;Big Data Benchmark&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&amp;nbsp;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;pat.mcdonough&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/02/10/strata-santa-clara-2014.html&quot;, &quot;authors&quot;: [&quot;Pat McDonough&quot;], &quot;id&quot;: 2414, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Events&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-02-11&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-02-11&quot;}, &quot;title&quot;: &quot;Databricks at the O'Reilly Strata Conference 2014&quot;, &quot;slug&quot;: &quot;strata-santa-clara-2014&quot;, &quot;content&quot;: &quot;The Databricks team is excited to take part in a number of activities throughout the 2014 O\\u2019Reilly Strata Conference in Santa Clara. From hands-on training, to office hours, to several talks (including a keynote), there are plenty of chances for attendees to learn how Apache Spark is bringing ease of use and outstanding performance to your big data. The schedule for the Databricks team includes:\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://ampcamp.berkeley.edu/4/\\&quot;&gt;AMPCamp4&lt;/a&gt;, Hosted at Strata&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/content/office-hours\\&quot;&gt;Office Hours&lt;/a&gt; on Wednesday at 5:45pm&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/schedule/detail/33057\\&quot;&gt;How Companies are Using Spark, and Where the Edge in Big Data Will Be&lt;/a&gt;, a keynote talk presented by Matei Zaharia on Thursday at 9:15am&lt;/li&gt;\\n \\t&lt;li&gt;&lt;a href=\\&quot;http://strataconf.com/strata2014/public/schedule/detail/32375\\&quot;&gt;Querying Petabytes of Data in Seconds with BlinkDB&lt;/a&gt;, co-presented by Reynold Xin on Thursday at 1:30pm&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;h2 id=\\&quot;about-ampcamp-4\\&quot;&gt;About AMPCamp 4&lt;/h2&gt;\\nWe\\u2019ll be kicking off the conference on Tuesday by helping to host the fourth iteration of AMPCamp, where some of the newer components of the Berkeley Data Analytics stack will get some time in the spotlight. There are talks on BlinkDB, MLbase, GraphX, and Tachyon in the morning, and hands-on training for BlinkDB, MLbase, Spark, Spark Streaming, GraphX, and Shark in the afternoon.\\n&lt;h2 id=\\&quot;conference-talks--office-hours\\&quot;&gt;Conference Talks &amp;amp; Office Hours&lt;/h2&gt;\\nOn Wednesday afternoon, attendees can come speak directly with a few of the members from our team in Strata\\u2019s Office Hours. Office hours provide a chance to meet face-to-face with the Databricks team and to ask any questions about the Spark project and ecosystem.\\n\\nTo learn the latest about Apache Spark, attendees should listen to talks featuring the Databricks team, including a keynote by our CTO Matei Zaharia. On Thursday morning, Matei will describe how organizations must now compete on the speed and sophistication with which they can draw value from data, and how Apache Spark makes a new class of applications possible.\\n&lt;h2 id=\\&quot;well-see-you-there\\&quot;&gt;We\\u2019ll See You There!&lt;/h2&gt;\\nWe\\u2019re looking forward to seeing you at the conference!&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;ion&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2014/01/21/spark-and-hadoop.html&quot;, &quot;authors&quot;: [&quot;Ion Stoica&quot;], &quot;id&quot;: 2415, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Ecosystem&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2014-01-22&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2014-01-22&quot;}, &quot;title&quot;: &quot;Apache Spark and Hadoop: Working Together&quot;, &quot;slug&quot;: &quot;spark-and-hadoop&quot;, &quot;content&quot;: &quot;We are often asked how does &lt;a href=\\&quot;http://spark.incubator.apache.org\\&quot;&gt;Apache Spark&lt;/a&gt; fits in the Hadoop ecosystem, and how one can run Spark in a existing Hadoop cluster. This blog aims to answer these questions.\\n\\nFirst, Spark is intended to &lt;em&gt;enhance&lt;/em&gt;, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon\\u2019s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\\n\\nSecond, we have constantly focused on making it as easy as possible for &lt;em&gt;every Hadoop user&lt;/em&gt; to take advantage of Spark\\u2019s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\\n\\n&lt;img class=\\&quot;aligncenter\\&quot; src=\\&quot;https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png\\&quot; alt=\\&quot;SparkHadoop1.png\\&quot; /&gt;\\n\\n&lt;strong&gt;Standalone deployment&lt;/strong&gt;: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\\n\\n&lt;strong&gt;&lt;a href=\\&quot;https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\\&quot;&gt;Hadoop Yarn&lt;/a&gt; deployment&lt;/strong&gt;: Hadoop users who have already deployed or are planning to deploy &lt;a href=\\&quot;https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\\&quot;&gt;Hadoop Yarn&lt;/a&gt; can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\\n\\n&lt;strong&gt;Spark In MapReduce (&lt;a href=\\&quot;https://databricks.com/blog/2014/01/01/simr.html\\&quot;&gt;SIMR&lt;/a&gt;)&lt;/strong&gt;: For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\\n&lt;h2 id=\\&quot;interoperability-with-other-systems\\&quot;&gt;Interoperability with other Systems&lt;/h2&gt;\\nSpark interoperates not only with Hadoop, but with other popular big data technologies as well.\\n&lt;ul&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://hive.apache.org/\\&quot;&gt;Apache Hive&lt;/a&gt;&lt;/strong&gt;: Through &lt;a href=\\&quot;https://github.com/amplab/shark/wiki\\&quot;&gt;Shark&lt;/a&gt;, Spark enables Apache Hive users to run their unmodified queries much faster. Hive is a popular data warehouse solution running on top of Hadoop, while Shark is a system that allows the Hive framework to run on top of Spark instead of Hadoop. As a result, Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk.&lt;/li&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://aws.amazon.com/\\&quot;&gt;AWS EC2&lt;/a&gt;&lt;/strong&gt;: Users can easily run Spark (and Shark) on top of Amazon\\u2019s EC2 either using the &lt;a href=\\&quot;http://spark.incubator.apache.org/docs/latest/ec2-scripts.html\\&quot;&gt;scripts&lt;/a&gt; that come with Spark, or the hosted &lt;a href=\\&quot;http://aws.amazon.com/articles/4926593393724923\\&quot;&gt;versions of Spark and Shark&lt;/a&gt; on Amazon\\u2019s Elastic MapReduce.&lt;/li&gt;\\n \\t&lt;li&gt;&lt;strong&gt;&lt;a href=\\&quot;http://mesos.apache.org/\\&quot;&gt;Apache Mesos&lt;/a&gt;&lt;/strong&gt;: Spark runs on top of Mesos, a cluster manager system which provides efficient resource isolation across distributed applications, including &lt;a href=\\&quot;http://www.mcs.anl.gov/research/projects/mpi/\\&quot;&gt;MPI&lt;/a&gt; and Hadoop. Mesos enables &lt;em&gt;fine grained&lt;/em&gt; sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution. This leads to considerable performance improvements, especially for long running Spark jobs.&lt;/li&gt;\\n&lt;/ul&gt;&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;patrick&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2013/12/19/release-0_8_1.html&quot;, &quot;authors&quot;: [&quot;Patrick Wendell&quot;], &quot;id&quot;: 2416, &quot;categories&quot;: [&quot;Apache Spark&quot;, &quot;Engineering Blog&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2013-12-20&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2013-12-20&quot;}, &quot;title&quot;: &quot;Apache Spark 0.8.1 Released&quot;, &quot;slug&quot;: &quot;release-0_8_1&quot;, &quot;content&quot;: &quot;We are happy to announce the release of Apache Spark 0.8.1. In addition to performance and stability improvements, this release adds three new features. First, Spark now supports for the newest versions of YARN (2.2+). Second, the standalone cluster manager supports a high-availability mode in which it can tolerate master failures. Third, shuffles have been optimized to create fewer files, improving shuffle performance drastically in some settings.\\n\\nIn conjunction with the Apache Spark 0.8.1 release we are separately releasing &lt;a href=\\&quot;https://databricks.com/blog/2014/01/01/simr.html\\&quot;&gt;Spark In MapReduce (SIMR)&lt;/a&gt;, which enables seamlessly running Spark on Hadoop MapReduce v1 clusters without requiring the installation of Scala or Spark.\\n\\nWhile Apache Spark 0.8.1 is a minor release, it includes these larger features for the benefit of Scala 2.9 users. The next major release of Apache Spark, 0.9.0, will be based on Scala 2.10.\\n\\nThis release was a community effort, featuring contributions from more than 40 developers. For much more information about Apache Spark 0.8.1, head over to the &lt;a href=\\&quot;http://spark.incubator.apache.org/releases/spark-release-0-8-1.html\\&quot;&gt;release notes&lt;/a&gt; or &lt;a href=\\&quot;http://spark.incubator.apache.org/downloads.html\\&quot;&gt;download Apache Spark 0.8.1&lt;/a&gt; and try it out for yourself.&quot;}\n{&quot;status&quot;: &quot;publish&quot;, &quot;description&quot;: null, &quot;creator&quot;: &quot;andy&quot;, &quot;link&quot;: &quot;https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html&quot;, &quot;authors&quot;: [&quot;Andy Konwinski&quot;], &quot;id&quot;: 2417, &quot;categories&quot;: [&quot;Company Blog&quot;, &quot;Customers&quot;, &quot;Events&quot;], &quot;dates&quot;: {&quot;publishedOn&quot;: &quot;2013-12-19&quot;, &quot;tz&quot;: &quot;UTC&quot;, &quot;createdOn&quot;: &quot;2013-12-19&quot;}, &quot;title&quot;: &quot;Highlights From Spark Summit 2013&quot;, &quot;slug&quot;: &quot;spark-summit-2013-follow-up&quot;, &quot;content&quot;: &quot;Earlier this month we held the &lt;a href=\\&quot;http://spark-summit.org/2013\\&quot;&gt;first Spark Summit&lt;/a&gt;, a conference to bring the Apache Spark community together. We are excited to share some statistics and highlights from the event.\\n&lt;ul&gt;\\n \\t&lt;li&gt;450 participants from over 180 companies attended&lt;/li&gt;\\n \\t&lt;li&gt;Participants came from 13 countries&lt;/li&gt;\\n \\t&lt;li&gt;Spark training was sold out at 200 participants from 80 companies&lt;/li&gt;\\n \\t&lt;li&gt;20 organizations sponsored the event, including all major Hadoop platform vendors&lt;/li&gt;\\n \\t&lt;li&gt;20 different organizations gave talks&lt;/li&gt;\\n&lt;/ul&gt;\\nVideos and slides for all talks are now available on the &lt;a href=\\&quot;http://spark-summit.org/2013\\&quot;&gt;Summit 2013 page&lt;/a&gt;.\\n\\nThe Summit included Keynotes from Databricks, the UC Berkeley AMPLab, and Yahoo, as well as presentations from 18 other companies including Amazon, Red Hat, and Adobe. Talk topics covered a wide range including specialized applications such as mapping and manipulating the brain, product launches, and research projects about future directions for the project. We are very excited to see Spark and related projects come such a long way from research prototypes originally developed in AMPLab at Berkeley to being used in production by startups and large companies alike.\\n&lt;h2 id=\\&quot;the-state-of-spark-and-where-were-going-next\\&quot;&gt;The State of Spark, and Where We\\u2019re Going Next&lt;/h2&gt;\\n&lt;iframe style=\\&quot;float: left; padding: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/nU6vO2EJAb4\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nIn the first keynote of the day, Matei Zaharia, who started the Spark project and is now CTO at Databricks, gave a summary of recent growth in the projec&lt;span style=\\&quot;letter-spacing: 3px;\\&quot;&gt;t&lt;/span&gt;&lt;a style=\\&quot;vertical-align: super; font-size: 60%;\\&quot; href=\\&quot;#footnote-1\\&quot;&gt;1&lt;/a&gt;, highlighting key contributions from across the community. In particular, Spark recently reached 100 contributors, making it the second-largest open source development community in the Big Data space after Hadoop, and it\\u2019s also overtaken Hadoop in the past 6 months. Matei also previewed what is coming up in the Spark development roadmap, including features under development for the upcoming 0.8.1 and 0.9 releases including high availability for the Spark Master in standalone mode, external hashing and sorting, support for Scala 2.10, and more. Finally, Matei discussed features of Spark that differentiate it from other projects, such as the focus on unification of diverse programming models.\\n&lt;h2 id=\\&quot;spark-and-hadoop\\&quot;&gt;Spark and Hadoop&lt;/h2&gt;\\n&lt;iframe style=\\&quot;float: right; padding-left: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/qs40jiN2iwM\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nIt was exciting to hear from Eric Baldeschwieler, whose background includes leading the Yahoo team that took Hadoop from being a prototype project to what it is today, as well as serving as both CEO and CTO of Hortonworks. In his keynote, Eric presented his view of how Spark is on track to become the \\u201clingua franca\\u201d for Big Data. He also talked about how Spark updates Hadoop with important features such as effective utilization of RAM, low latency queries, and streaming ingest. Finally, he discussed how the Spark and Hadoop projects relate to each other now and going forward.\\n&lt;h2 id=\\&quot;spark-training-day\\&quot;&gt;Spark Training Day&lt;/h2&gt;\\nOn the sold-out second day, 200 attendees heard 4 talks on using, deploying, and administering Spark, and also participated in hands-on training led by the creators of Spark. Amazon, a sponsor of the Summit, donated EC2 resources so participants could each have their own 6 node Spark cluster to practice using Spark, Spark Streaming and Shark on real Wikipedia and Twitter data. We have made the hands-on exercises &lt;a href=\\&quot;http://spark-summit.org/exercises\\&quot;&gt;available on the summit website&lt;/a&gt; for free. Using these, anybody can use their own Amazon AWS account to launch a 6 node Spark cluster and get experience analyzing real data.\\n\\n&lt;iframe style=\\&quot;float: left; padding-right: 10px;\\&quot; src=\\&quot;//www.youtube.com/embed/zGW8glN-Mo8\\&quot; width=\\&quot;300\\&quot; height=\\&quot;173\\&quot; frameborder=\\&quot;0\\&quot; allowfullscreen=\\&quot;allowfullscreen\\&quot;&gt;&lt;/iframe&gt;\\n\\nThe final talk of the training day was given by core Spark developer and Databricks cofounder Patrick Wendell. Patrick\\u2019s talk was about Administering Spark and was prepared in response to requests for more advanced technical material as part of the training. In it he dove into the core software components of the project, the different resource managers that Spark runs on, what type of hardware to use when running Spark, how to link against Spark when writing applications, monitoring Spark clusters running in production, and more.\\n&lt;h2 id=\\&quot;other-interesting-talks\\&quot;&gt;Other Interesting Talks&lt;/h2&gt;\\nIn addition to those above, there were many more great talks at the Summit, here are a few more that highligh the diversity of topics covered:\\n&lt;ul class=\\&quot;talk-list\\&quot;&gt;\\n \\t&lt;li&gt;&lt;span class=\\&quot;talk-title\\&quot;&gt;Mapping and manipulating the brain at scale&lt;/span&gt; (&lt;a href=\\&quot;http://spark-summit.org/talk/freeman-mapping-and-manipulating-the-brain-at-scale/\\&quot;&gt;abstract&lt;/a&gt;, &lt;a h\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["To expose the JSON file as a table, use the standard SQL create table using syntax introduced in the previous lesson:"],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE TABLE IF NOT EXISTS DatabricksBlog\n  USING json\n  OPTIONS (\n    path \"dbfs:/mnt/training/databricks-blog.json\",\n    inferSchema \"true\"\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Take a look at the schema with the `DESCRIBE` function."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>authors</td><td>array<string></td><td>null</td></tr><tr><td>categories</td><td>array<string></td><td>null</td></tr><tr><td>content</td><td>string</td><td>null</td></tr><tr><td>creator</td><td>string</td><td>null</td></tr><tr><td>dates</td><td>struct<createdOn:string,publishedOn:string,tz:string></td><td>null</td></tr><tr><td>description</td><td>string</td><td>null</td></tr><tr><td>id</td><td>bigint</td><td>null</td></tr><tr><td>link</td><td>string</td><td>null</td></tr><tr><td>slug</td><td>string</td><td>null</td></tr><tr><td>status</td><td>string</td><td>null</td></tr><tr><td>title</td><td>string</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Run a query to view the contents of the table.\n\nNotice:\n* The `authors` column is an array containing multiple author names.\n* The `categories` column is an array of multiple blog post category names.\n* The `dates` column contains nested fields `createdOn`, `publishedOn` and `tz`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors, categories, dates, content \nFROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>authors</th><th>categories</th><th>dates</th><th>content</th></tr></thead><tbody><tr><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>List(Company Blog, Partners)</td><td>List(2014-04-10, 2014-04-10, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at MapR, announcing our new partnership to provide enterprise support for Apache Spark as part of MapR's Distribution of Hadoop.</div>\n\n<hr />\n\nWith over 500 paying customers, my team and I have the opportunity to talk to many organizations that are leveraging Hadoop in production to extract value from big data. One of the most common topics raised by our customers in recent months is Apache Spark. Some customers just want to learn more about the advantages of this technology and the use cases that it addresses, while others are already running it in production with the MapR Distribution. These customers range from the world’s largest cable telcos and retailers to Silicon Valley startups such as Quantifind, which recently talked about its use of Spark on MapR in an <a href=\"http://www.datameer.com/ceoblog/big-data-brews-with-erich-nachbar/\" target=\"_blank\">interview</a> with Stefan Groschupf, CEO of Datameer.\n\nToday, I am happy to <a href=\"http://www.businesswire.com/news/home/20140410005101/en/MapR-Adds-Complete-Apache-Spark-Stack-Distribution#.U0a0G61dXKI\" target=\"_blank\">announce</a> and share with you the beginning of our journey with Databricks, and the addition of the complete Spark stack to the MapR Distribution for Apache Hadoop. We are now the only Hadoop distribution to support the complete Spark stack, including Spark, Spark Streaming (stream processing), Shark (Hive on Spark), MLLib (machine learning) and GraphX (graph processing). This is a testament to our commitment to open source and to providing our customers with maximum flexibility to pick and choose the right tool for the job.\n<h2 id=\"why-spark\">Why Spark?</h2>\nOne of the challenges organizations face when adopting Hadoop is a shortage of developers who have experience building Hadoop applications. Our professional services organization has helped dozens of companies with the development and deployment of Hadoop applications, and our training department has trained countless engineers. Organizations are hungry for solutions that make it easier to develop Hadoop applications while increasing developer productivity, and Spark fits this bill. Spark jobs can require as little as 1/5th of code. Spark provides a simple programming abstraction allowing developers to design applications as operations on data collections (known as RDDs, or Resilient Distributed Datasets). Developers can build these applications in multiple programming languages, including Java, Scala and Python, and the same code can be reused across batch, interactive and streaming applications.\n\nIn addition to making developers happier and more productive, Spark provides significant benefits with respect to end-to-end application performance. To this end, Spark provides a general-purpose execution framework with in-memory pipelining. For many applications, this results in a 5-100x performance improvement, because some or all steps can execute in memory without unnecessarily writing to and reading from disk. The performance advantage of the Spark engine, combined with the industry-leading performance of the MapR Distribution, provides customers with the highest-performance platform for big data applications.\n<h2 id=\"why-databricks\">Why Databricks?</h2>\nDatabricks was founded by the creators of Apache Spark, and is currently the driving force behind the project. When we decided to add the Spark stack to our distribution and double down on our involvement in the Spark community, a strategic partnership with Databricks was a no-brainer. This partnership will benefit MapR customers who are interested in 24x7 support for Spark or any of the other projects in the stack, including Spark Streaming, Shark, MLLib and GraphX (with several other projects coming soon). In addition, MapR will be working closely with Databricks to drive the Spark roadmap and accelerate the development of new features, benefiting both MapR customers and the broader community.\n\nWe are very excited about the upcoming Apache Spark 1.0 release, expected later this month. We are looking forward to a great journey with Databricks and the other members of the Spark community.\n\n<a href=\"http://w.on24.com/r.htm?e=780379&amp;s=1&amp;k=A6FB573A31B1DD9D8AB6294A101383ED&amp;partnerref=MapR\" target=\"_blank\">Register for an upcoming joint webinar</a> to learn more about the benefits of the complete Spark stack on MapR.</td></tr><tr><td>List(Tathagata Das)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-04-10, 2014-04-10, UTC)</td><td>We are happy to announce the availability of <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">Apache Spark 0.9.1</a>! This is a maintenance release with bug fixes, performance improvements, better stability with YARN and improved parity of the Scala and Python API. We recommend all 0.9.0 users to upgrade to this stable release.\n\nThis is the first release since Spark graduated as a top level Apache project. Contributions to this release came from 37 developers.\n\nVisit the <a href=\"http://spark.apache.org/releases/spark-release-0-9-1.html\" target=\"_blank\">release notes</a> for more information about all the improvements and bug fixes. <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">Download</a> it and try it out!</td></tr><tr><td>List(Steven Hillion)</td><td>List(Company Blog, Partners)</td><td>List(2014-04-01, 2014-04-01, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at Alpine Data Labs, part of the 'Application Spotlight' series highlighting innovative applications that are part of the Databricks \"Certified on Apache Spark\" program.</div>\n\n<hr />\n\nEveryone knows how hard it is to recruit engineers and data scientists in Silicon Valley. At <a href=\"http://www.alpinenow.com\" target=\"_blank\">Alpine Data Labs</a>, we think what we’re up to is pretty fun and challenging, but we still have to compete with other start-ups as well as the big internet companies to attract the best talent. One thing that can help is to be able to say that you’re working with the most innovative and powerful technologies.\n\nLast year, I was interviewing a talented engineer with a strong background in machine learning. And he said that the one thing he wanted to do above all was to work with Apache Spark. “Will I get to do that at Alpine?” he asked.\n\nIf it had been even a year earlier, I would have said “Sure…at some point.” But in the meantime I’d met several of the members of the <a href=\"https://amplab.cs.berkeley.edu\" target=\"_blank\">AMPLab</a> research team at Berkeley, and been impressed with their mature approach to building a platform and ecosystem. And I’d seen enough companies installing Spark on their dev clusters that it was clear this was a technology to watch. In a remarkably short time, it went from experimental to very real. And now prospects in the Alpine pipeline were asking me if it was on the roadmap. So yes, I told my candidate. “You’ll be working on Spark from day one.”\n\nLast week, Alpine announced at <a href=\"http://www.cmswire.com/cms/big-data/alpine-turns-big-data-to-a-priceless-asset-fast-gigaomlive-024541.php\" target=\"_blank\">GigaOM</a> that it’s one of the first analytics companies to leverage Spark for building predictive models. We demonstrated the Alpine engine running on <a href=\"http://www.gopivotal.com/big-data/analytics-workbench\" target=\"_blank\">Pivotal’s Analytics Workbench</a>, where it ran an iterative classification algorithm (logistic regression) on 50 million rows in less than 50 seconds.\n\nFurthermore, we were officially certified on Spark by the team at Databricks. It’s been an honor to work with them and the research team at Berkeley. We think their technology will be a serious contender for the leading platform for data science.\n\nSpark is more to us than just speed. It’s really the entire ecosystem that represents such an exciting paradigm for working with data.\n\nStill, the core capability of caching data in memory was our primary consideration, and our iterative algorithms have been shown to speed up by one or even two orders of magnitude (thanks again to that Pivotal cluster).\n\nWe’ve always had this mantra at Alpine: “Avoid multiple passes through the data!” And we’ve designed many of our machine learning algorithms to avoid scanning the data too many times, packing on calculations into each MapReduce job like a waiter piling up plates to try and clear a table in one go. But it’s rare that we can avoid it entirely. With Spark, it’s incredibly satisfying to watch the progress bar zip along as the system re-uses data it’s already seen before.\n\nAnother thing that’s getting our engineers excited is Spark’s <a href=\"http://spark.apache.org/mllib/\" target=\"_blank\">MLLib</a>, the machine-learning library written on top of the Spark runtime. Alpine has long thought that machine learning algorithms should be open source. (I helped to kick off the <a href=\"http://madlib.net/\" target=\"_blank\">MADlib</a> library of analytics functions for databases, and Alpine now uses it extensively.) So we’re now beginning to contribute some of our code back into MLLib. And, moreover, we think MLLib and MLI have the potential to be a more general repository for open-source machine learning.\n\nSo I’ll congratulate the Alpine team for helping to bring the power of Spark to our users, and I’ll also congratulate the Spark team and Databricks for making it possible!</td></tr><tr><td>List(Michael Armbrust, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-27, 2014-03-27, UTC)</td><td>Building a unified platform for big data analytics has long been the vision of Apache Spark, allowing a single program to perform ETL, MapReduce, and complex analytics. An important aspect of unification that our users have consistently requested is the ability to more easily import data stored in external sources, such as Apache Hive. Today, we are excited to announce <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL</a>, a new component recently merged into the Spark repository.\n\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Spark’s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\n<ul>\n \t<li>Import relational data from Parquet files and Hive tables</li>\n \t<li>Run SQL queries over imported data and existing RDDs</li>\n \t<li>Easily write RDDs out to Hive tables or Parquet files</li>\n</ul>\n<h2 id=\"spark-sql-in-action\">Spark SQL In Action</h2>\nNow, let’s take a closer look at how Spark SQL gives developers the power to integrate SQL commands into applications that also take advantage of MLlib, Spark’s machine learning library. Consider an application that needs to predict which users are likely candidates for a service, based on their profile. Often, such an analysis requires joining data from multiple sources. For the purposes of illustration, imagine an application with two tables:\n<ul>\n \t<li>Users(userId INT, name String, email STRING,\nage INT, latitude: DOUBLE, longitude: DOUBLE,\nsubscribed: BOOLEAN)</li>\n \t<li>Events(userId INT, action INT)</li>\n</ul>\nGiven the data stored in in these tables, one might want to build a model that will predict which users are good targets for a new campaign, based on users that are similar.\n\n[scala]\n// Data can easily be extracted from existing sources,\n// such as Apache Hive.\nval trainingDataTable = sql(&quot;&quot;&quot;\n  SELECT e.action\n         u.age,\n         u.latitude,\n         u.logitude\n  FROM Users u\n  JOIN Events e\n  ON u.userId = e.userId&quot;&quot;&quot;)\n\n// Since `sql` returns an RDD, the results of the above\n// query can be easily used in MLlib\nval trainingData = trainingDataTable.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  LabeledPoint(row(0), features)\n}\n\nval model =\n  new LogisticRegressionWithSGD().run(trainingData)\n[/scala]\n\nNow that we have used SQL to join existing data and train a model, we can use this model to predict which users are likely targets.\n\n[scala]\nval allCandidates = sql(&quot;&quot;&quot;\n  SELECT userId,\n         age,\n         latitude,\n         logitude\n  FROM Users\n  WHERE subscribed = FALSE&quot;&quot;&quot;)\n\n// Results of ML algorithms can be used as tables\n// in subsequent SQL statements.\ncase class Score(userId: Int, score: Double)\nval scores = allCandidates.map { row =&gt;\n  val features = Array[Double](row(1), row(2), row(3))\n  Score(row(0), model.predict(features))\n}\nscores.registerAsTable(&quot;Scores&quot;)\n\nval topCandidates = sql(&quot;&quot;&quot;\n  SELECT u.name, u.email\n  FROM Scores s\n    JOIN Users u ON s.userId = u.userId\n  ORDER BY score DESC\n  LIMIT 100&quot;&quot;&quot;)\n\n// Send emails to top candidates to promote the service.\n[/scala]\n\nIn this example, Spark SQL made it easy to extract and join the various datasets preparing them for the machine learning algorithm. Since the results of Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. Furthermore, Spark SQL allows developers to close the loop, by making it easy to manipulate and join the output of these algorithms, producing the desired final result.\n\nTo summarize, the unified Spark platform gives developers the power to choose the right tool for the right job, without having to juggle multiple systems. If you would like to see more concrete examples of using Spark SQL please check out the <a href=\"http://spark.apache.org/docs/1.0.0/sql-programming-guide.html\">programming guide</a>.\n<h2 id=\"optimizing-with-catalyst\">Optimizing with Catalyst</h2>\nIn addition to providing new ways to interact with data, Spark SQL also brings a powerful new optimization framework called Catalyst. Using Catalyst, Spark can automatically transform SQL queries so that they execute more efficiently. The Catalyst framework allows the developers behind Spark SQL to rapidly add new optimizations, enabling us to build a faster system more quickly. In one recent example, we found an inefficiency in Hive group-bys that took an experienced developer an entire weekend and over 250 lines of code to fix; we were then able to make the same fix in Catalyst in only a few lines of code.\n<h2 id=\"future-of-shark\">Future of Shark</h2>\nThe natural question that arises is about the future of Shark. Shark was among the first systems that delivered up to 100X speedup over Hive. It builds on the Apache Hive codebase and achieves performance improvements by swapping out the physical execution engine part of Hive. While this approach enables Shark users to speed up their Hive queries without modification to their existing warehouses, Shark inherits the large, complicated code base from Hive that makes it hard to optimize and maintain. As Spark SQL matures, Shark will transition to using Spark SQL for query optimization and physical execution so that users can benefit from the ongoing optimization efforts within Spark SQL.\n\nIn short, we will continue to invest in Shark and make it an excellent drop-in replacement for Apache Hive. It will take advantage of the new Spark SQL component, and will provide features that complement it, such as Hive compatibility and the standalone SharkServer, which allows external tools to connect queries through JDBC/ODBC.\n<h2 id=\"whats-next\">What’s next</h2>\nSpark SQL will be included in Spark 1.0 as an alpha component. However, this is only the beginning of better support for relational data in Spark, and this post only scratches the surface of Catalyst. Look for future blog posts on the following topics:\n<ul>\n \t<li>Generating custom bytecode to speed up expression evaluation</li>\n \t<li>Reading and writing data using other formats and systems, include Avro and HBase</li>\n \t<li>API support for using Spark SQL in Python and Java</li>\n</ul></td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-02-04, 2014-02-04, UTC)</td><td>Our goal with Apache Spark is very simple: provide the best platform for computation on big data. We do this through both a powerful core engine and rich libraries for useful analytics tasks. Today, we are excited to announce the release of Apache Spark 0.9.0. This major release extends Spark’s libraries and further improves its performance and usability. Apache Spark 0.9.0 is the largest release to date, with work from 83 contributors, who submitted over 300 patches.\n\nApache Spark 0.9 features significant extensions to the set of standard analytical libraries packaged with Spark. The release introduces GraphX, a library for graph computation that comes with implementations of several standard algorithms, such as PageRank. Spark’s machine learning library (MLlib) has been extended to support Python, using the NumPy numerical library. A Naive Bayes Classifier has also been added to MLlib. Finally, Spark Streaming, which supports near-real-time continuous computation, has added a simplified high-availability mode and several significant optimizations.\n\nIn addition to higher-level libraries, Spark 0.9 features improvements to the core computation engine. Spark now now automatically spills reduce output to disk, increasing the stability of workloads with very large aggregations. Support for Spark in YARN mode has been hardened and improved. The standalone mode has added automatic supervision of applications and better support for sharing clusters amongst several users. Finally, we’ve focused on stabilizing API’s ahead of Apache Spark’s 1.0 release to make things easy for developers writing Spark applications. This includes upgrading to Scala 2.10, allowing applications written in Scala to use newer libraries.\n\nApache Spark 0.9.0 can be <a href=\"http://spark.incubator.apache.org/downloads.html\">downloaded directly</a> from the Apache Spark website. It will also be available to CDH users via a Cloudera parcel, which can automatically install Spark on existing CDH clusters. For a more detailed explanation of the features in this release, head on over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-9-0.html\">official release notes</a>. Enjoy the newest release of Spark!</td></tr><tr><td>List(Ali Ghodsi, Ahir Reddy)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(2014-01-02, 2014-01-02, UTC)</td><td>Apache Hadoop integration has always been a key goal of Apache Spark and <a href=\"http://hortonworks.com/wp-content/uploads/2013/06/YARN.png\">YARN</a> users have long been able to run <a href=\"http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\">Spark on YARN</a>. However, up to now, it has been relatively hard to run Spark on Hadoop MapReduce v1 clusters, i.e. clusters that do not have YARN installed. Typically, users would have to get permission to install Spark/Scala on some subset of the machines, a process that could be time consuming. Enter <a href=\"http://databricks.github.io/simr/\">SIMR (Spark In MapReduce)</a>, which has been released in conjunction with <a href=\"https://databricks.com/blog/2013/12/19/release-0_8_1.html\">Apache Spark 0.8.1</a>.\n\nSIMR allows anyone with access to a Hadoop MapReduce v1 cluster to run Spark out of the box. A user can run Spark directly on top of Hadoop MapReduce v1 without any administrative rights, and without having Spark or Scala installed on any of the nodes. The only requirements are HDFS access and MapReduce v1. SIMR is open sourced under the <a href=\"http://apache.org/licenses/LICENSE-2.0.html\">Apache license</a> and was jointly developed by Databricks and UC Berkeley <a href=\"http://amplab.cs.berkeley.edu\">AMPLab</a>.\n\nThe basic idea is that a user can download the package of SIMR (<a href=\"http://databricks.github.io/simr/#download\">3 files</a>) that matches their Hadoop cluster and immediately start using Spark. SIMR includes the interactive Spark shell, and allows users to use the shell backed by the computational power of the cluster. It is a simple as <code>./simr --shell</code>:\n\n&nbsp;\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/01/simrshell.png\" alt=\"simrshell\" width=\"90%\" />\n\nRunning a Spark program simply requires bundling it along with its dependencies into a jar and launching the job through SIMR. SIMR uses the following command line syntax for running jobs:\n\n<pre>./simr jar_file main_class parameters</pre>\n\nSIMR simply launches a MapReduce job with the desired number of map slots, and ensures that Spark/Scala and your job gets shipped to all those nodes. It then designates one of the mappers as the master and runs the Spark driver inside it. On the rest of the mappers it launches Spark executors that will execute tasks on behalf of the driver. Voila, your Spark job is running inside MapReduce on the cluster.\n\nSIMR lets users interact with the driver program. For example, users can type into the Spark shell and see its output interactively. The way this works is that SIMR runs a relay server on the master mapper and a relay client on the machine that launched SIMR. Any input from the client and output from the driver are relayed back and forth between the client and the master mapper.\n\nTo make all this work, SIMR makes extensive use of HDFS. The master mapper is elected using leader election by writing to HDFS and picking the mapper that firsts gets to write to HDFS. Similarly, the executors launched inside the rest of the mappers find out the driver’s URL by reading it from a specific file from HDFS. Thus, SIMR uses MapReduce and HDFS in place of a cluster manager.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/01/simr-arch.png\" alt=\"simr-arch\" width=\"90%\" />\n\nSIMR is not intended to be used in production mode, but rather to enable users to explore and use Spark before running it on a proper resource manager, such as YARN, Mesos, or Standalone Mode. MapReduce 2 (YARN) users can of course use the existing <a href=\"http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\">Spark on YARN</a> solution, or explore other <a href=\"http://spark.incubator.apache.org/docs/latest/index.html#launching-on-a-cluster\">Spark deployment options</a>.\n\nWe hope SIMR will enable users to try out Spark without any heavy operational burden. Towards this goal, we have pre-built several SIMR binaries for different versions of Hadoop. Please go ahead and try it and let us know if you have any feedback.\n\nSIMR resources:\n<ul>\n \t<li><a href=\"http://databricks.github.io/simr\">Homepage</a></li>\n \t<li><a href=\"http://databricks.github.io/simr#download\">Download</a></li>\n \t<li><a href=\"https://github.com/databricks/simr\">Source code</a></li>\n</ul></td></tr><tr><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>List(Company Blog, Customers)</td><td>List(2014-03-26, 2014-03-26, UTC)</td><td><div class=\"post-meta\">We're very happy to see our friends at Cloudera continue to get the word out about Apache Spark, and their latest blog post is a great example of how users are putting Spark Streaming to use to solve complex problems in real time. Thanks to Russell Cardullo and Michael Ruggiero, Data Infrastructure Engineers at <a href=\"http://engineering.sharethrough.com/\">Sharethrough</a>, for this <a href=\"http://blog.cloudera.com/blog/2014/03/letting-it-flow-with-spark-streaming/\">guest post on Cloudera's blog</a>, which we've cross-posted below</div>\n\n<hr />\n\nAt Sharethrough, which offers an advertising exchange for delivering in-feed ads, we’ve been running on CDH for the past three years (after migrating from Amazon EMR), primarily for ETL. With the launch of our exchange platform in early 2013 and our desire to optimize content distribution in real time, our needs changed, yet CDH remains an important part of our infrastructure.\n\nIn mid-2013, we began to examine stream-based approaches to accessing click-stream data from our pipeline. We asked ourselves: Rather than “warm up our cold path” by running those larger batches more frequently, can we give our developers a programmatic model and framework optimized for incremental, small batch processing, yet continue to rely on the Cloudera platform? Ideally, our engineering team focuses on the data itself, rather than worrying about details like consistency of state across the pipeline or fault recovery.\n<h2>Spark (and Spark Streaming)</h2>\n<a href=\"http://spark.incubator.apache.org/\">Apache Spark</a> is a fast and general framework for large-scale data processing, with a programming model that supports building applications that would be more complex or less feasible using conventional MapReduce. (Spark ships inside Cloudera Enterprise 5, and is already supported for use with CDH 4.4 and later.) With an in-memory persistent storage abstraction, Spark supports complete MapReduce functionality without the long execution times required by things like data replication, disk I/O, and serialization.\n\nBecause <a href=\"http://spark.incubator.apache.org/streaming/\">Spark Streaming</a> shares the same API as Spark’s batch and interactive modes, we now use Spark Streaming to aggregate business-critical data in real time. A consistent API means that we can develop and test locally in the less complex batch mode and have that job work seamlessly in production streaming. For example, we can now optimize bidding in real time, using the entire dataset for that campaign without waiting for our less frequently run ETL flows to complete. We are also able to perform real-time experiments and measure results as they come in.\n<h2>Before and After</h2>\nOur batch-processing system looks like this:\n<ol>\n \t<li>Apache Flume writes out files based on optimal HDFS block size (64MB) to hourly buckets.</li>\n \t<li>MapReduce (Scalding) jobs are scheduled N times per day.</li>\n \t<li>Apache Sqoop moves results into the data warehouse.</li>\n \t<li>Latency is ~1 hour behind, plus Hadoop processing time.</li>\n</ol>\n<img class=\"alignleft size-full wp-image-152\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-streaming11.png\" alt=\"spark-streaming11\" width=\"450px\" />\n\n<strong>Sharethrough’s former batch-processing dataflow</strong>\n\nFor our particular use case, this batch-processing workflow wouldn’t provide access to performance data while the results of those calculations would still be valuable. For example, knowing that a client’s optimized content performance is 4.2 percent an hour after their daily budget is spent, means our advertisers aren’t getting their money’s worth, and our publishers aren’t seeing the fill they need. Even when the batch jobs take minutes, a spike in traffic could slow down a given batch job, causing it to “bump into” newly launched jobs.\n\nFor these use cases, a streaming dataflow is the viable solution:\n<ol>\n \t<li>Flume writes out clickstream data to HDFS.</li>\n \t<li>Spark reads from HDFS at batch sizes of five seconds.</li>\n \t<li>Output to a key-value store, updating our predictive modeling.</li>\n</ol>\n<img class=\"alignleft size-full wp-image-152\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-streaming21.png\" alt=\"spark-streaming11\" width=\"450px\" />\n\n<strong>Sharethrough’s new Spark Streaming-based dataflow</strong>\n\nIn this new model, our latency is only Spark processing time and the time it takes Flume to transmit files to HDFS; in practice, this works out to be about five seconds.\n<h2>On the Journey</h2>\nWhen we began using Spark Streaming, we shipped quickly with minimal fuss. To get the most out of our new streaming jobs, we quickly adjusted to the Spark programming model.\n\nHere are some things we discovered along the way:\n<ul>\n \t<li>The profile of a 24 x 7 streaming app is different than an hourly batch job — you may need finer-grained alerting and more patience with repeated errors. And with a streaming application, good exception handling is your friend. (Be prepared to answer questions like: “What if the Spark receiver is unavailable? Should the application retry? Should it forget data that was lost? Should it alert you?”)</li>\n \t<li>Take time to validate output against the input. A stateful job that, for example, keeps a count of clicks, may return results you didn’t expect in testing.</li>\n \t<li>Confirm that supporting objects are being serialized. The Scala DSL makes it easy to close over a non-serializable variable or reference. In our case, a GeoCoder object was not getting serialized and our app became very slow; it had to return to the driver program for the original, non-distributed object.</li>\n \t<li>The output of your Spark Streaming job is only as reliable as the queue that feeds Spark. If the producing queue drops, say, 1 percent of messages, you may need a periodic reconciliation strategy (such as merging your lossy “hot path” with “cold path” persistent data). For these kinds of merges, the monoid abstraction can be helpful when you need certainty that associative calculations (counts, for example) are accurate and reliable. For more on this, see merge-able stores like Twitter’s <a href=\"https://github.com/twitter/storehaus\">Storehaus</a> or Oscar Boykin’s “<a href=\"https://speakerdeck.com/johnynek/algebra-for-analytics\">Algebra for Analytics</a>“.</li>\n</ul>\n<h2>Conclusion</h2>\nSharethrough Engineering intends to do a lot more with Spark Streaming. Our engineers can interactively craft an application, test it in batch, move it into streaming and it just works. We’d encourage others interested in unlocking real-time processing to look at Spark Streaming. Because of the concise Spark API, engineers comfortable with MapReduce can build streaming applications today without having to learn a completely new programming model.\n\nSpark Streaming equips your organization with the kind of insights only available from up-to-the-minute data, either in the form of machine-learning algorithms or real-time dashboards: It’s up to you!</td></tr><tr><td>List(Jai Ranganathan, Matei Zaharia)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-21, 2014-03-21, UTC)</td><td><div class=\"post-meta\">\n\nThis article was cross-posted in the <a href=\"http://blog.cloudera.com/blog/2014/03/apache-spark-a-delight-for-developers/\">Cloudera developer blog</a>.\n\n</div>\n<a href=\"http://spark.apache.org/\">Apache Spark</a> is well known today for its <a href=\"http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/\">performance benefits</a> over MapReduce, as well as its <a href=\"http://blog.cloudera.com/blog/2014/03/why-apache-spark-is-a-crossover-hit-for-data-scientists/\">versatility</a>. However, another important benefit — the elegance of the development experience — gets less mainstream attention.\n\nIn this post, you’ll learn just a few of the features in Spark that make development purely a pleasure.\n<h2>Language Flexibility</h2>\nSpark natively provides support for a variety of popular development languages. Out of the box, it supports Scala, Java, and Python, with some promising work ongoing <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">to support R</a>.\n\nOne common element among these languages (with the temporary exception of Java, which is due for a major update imminently in the form of Java 8) is that they all provide concise ways to express operations using “closures” and lambda functions. <a href=\"http://en.wikipedia.org/wiki/Closure_(computer_programming)\">Closures</a> allow users to define functions in-line with the core logic of the application, thereby preserving application flow and making for tight and easy-to-read code:\n\n<strong>Closures in Python with Spark:</strong>\n\n<pre>lines = sc.textFile(...)\nlines.filter(lambda s:\"ERROR\"in s).count()</pre>\n\n<strong>Closures in Scala with Spark:</strong>\n\n<pre>val lines = sc.textFile(...)\nlines.filter(s => s.contains(\"ERROR\")).count()</pre>\n\n<strong>Closures in Java with Spark:</strong>\n\n<pre>JavaRDD<String> lines = sc.textFile(...);\nlines.filter(newFunction<String,Boolean>()  {\n  Boolean call(String s){\n    return s.contains(\"error\");\n  }\n}).count();</pre>\n\nOn the performance front, a lot of work has been done to optimize all three of these languages to run efficiently on the Spark engine. Spark is written in Scala, which runs on the JVM, so Java can run efficiently in the same JVM container. Via the smart use of <a href=\"http://py4j.sourceforge.net/\">Py4J</a>, the overhead of Python accessing memory that is managed in Scala is also minimal.\n<h2>APIs That Match User Goals</h2>\nWhen developing in MapReduce, you are often forced to stitch together basic operations as custom Mapper/Reducer jobs because there are no built-in features to simplify this process. For that reason, many developers turn to the higher-level APIs offered by frameworks like Apache Crunch or Cascading to write their MapReduce jobs.\n\nIn contrast, Spark natively provides a rich and ever-growing library of operators. Spark APIs include functions for:\n\n<ul>\n<li><code>cartesian</code></li>\n<li><code>cogroup</code></li>\n<li><code>collect</code></li>\n<li><code>count</code></li>\n<li><code>countByValue</code></li>\n<li><code>distinct</code></li>\n<li><code>filter</code></li>\n<li><code>flatMap</code></li>\n<li><code>fold</code></li>\n<li><code>groupByKey</code></li>\n<li><code>join</code></li>\n<li><code>map</code></li>\n<li><code>mapPartitions</code></li>\n<li><code>reduce</code></li>\n<li><code>reduceByKey</code></li>\n<li><code>sample</code></li>\n<li><code>sortByKey</code></li>\n<li><code>subtract</code></li>\n<li><code>take</code></li>\n<li><code>union</code></li>\n</ul>\n\nand <a href=\"https://spark.apache.org/docs/0.9.0/api/core/#org.apache.spark.rdd.RDD\">many more.</a> In fact, there are more than 80 operators available out of the box in Spark!\n\nWhile many of these operations often boil down to Map/Reduce equivalent operations, the high-level API matches user intentions closely, allowing you to write much more concise code.\n\nAn important note here is that while scripting frameworks like Apache Pig provide many high-level operators as well, Spark allows you to access these operators in the context of a full programming language — thus, you can use control statements, functions, and classes as you would in a typical programming environment.\n<h2>Automatic Parallelization of Complex Flows</h2>\nWhen constructing a complex pipeline of MapReduce jobs, the task of correctly parallelizing the sequence of jobs is left to you. Thus, a scheduler tool such as Apache Oozie is often required to carefully construct this sequence.\n\nWith Spark, a whole series of individual tasks is expressed as a single program flow that is lazily evaluated so that the system has a complete picture of the execution graph. This approach allows the core scheduler to correctly map the dependencies across different stages in the application, and automatically parallelize the flow of operators without user intervention.\n\nThis capability also has the property of enabling certain optimizations to the engine while reducing the burden on the application developer. Win, and win again!\n\nFor example, consider the following job:\n\n<pre>rdd1.map(splitlines).filter(\"ERROR\")\nrdd2.map(splitlines).groupBy(key)\nrdd2.join(rdd1, key).take(10)</pre>\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-devs1.png\" alt=\"spark-devs1\" /></p>\n\nThis simple application expresses a complex flow of six stages. But the actual flow is completely hidden from the user — the system automatically determines the correct parallelization across stages and constructs the graph correctly. In contrast, alternate engines would require you to manually construct the entire graph as well as indicate the proper parallelization.\n\n<h2>Interactive Shell</h2>\n\nSpark also lets you access your datasets through a simple yet specialized Spark shell for Scala and Python. With the Spark shell, developers and users can get started accessing their data and manipulating datasets without the full effort of writing an end-to-end application. Exploring terabytes of data without compiling a single line of code means you can understand your application flow by literally test-driving your program before you write it up.\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-devs21.png\" alt=\"spark-devs21\" /></p>\n\nJust open up a shell, type a few commands, and you’re off to the races!\n\n<h2>Performance</h2>\n\nWhile this post has focused on how Spark not only improves performance but also programmability, we should’t ignore one of the best ways to make developers more efficient: performance!\n\nDevelopers often have to run applications many times over the development cycle, working with subsets of data as well as full data sets to repeatedly follow the develop/test/debug cycle. In a Big Data context, each of these cycles can be very onerous, with each test cycle, for example, being hours long.\n\nWhile there are various ways systems to alleviate this problem, one of the best is to simply run your program fast. Thanks to the performance benefits of Spark, the development lifecycle can be materially shortened merely due to the fact that the test/debug cycles are much shorter.\n\nAnd your end-users will love you too!\n\n<p align=\"center\"><img style=\"width: 90%; height: auto;\" src=\"https://databricks.com/wp-content/uploads/2014/03/spark-dev3.png\" alt=\"spark-dev3\" /></p>\n\n<h2>Example: WordCount</h2>\n\nTo give you a sense of the practical impact of these benefits in a concrete example, the following two snippets of code reflect a WordCount implementation in MapReduce versus one in Spark. The difference is self-explanatory:\n\n<strong>WordCount the MapReduce way:</strong>\n\n<pre>public static class WordCountMapClass extends MapReduceBase\n  implements Mapper<LongWritable, Text, Text, IntWritable> {\n  private final static IntWritable one = newIntWritable(1);\n  private Text word = newText();\n  public void map(LongWritable key,Text value,\n                  OutputCollector <Text, IntWritable> output,\n                  Reporter reporter) throws IOException {\n    String line = value.toString();\n    StringTokenizer itr = newStringTokenizer(line);\n    while (itr.hasMoreTokens()) {\n      word.set(itr.nextToken());\n      output.collect(word, one);\n    }\n  }\n}\n\npublic static class WordCountReduce extends MapReduceBase\n  implements Reducer<Text, IntWritable, Text, IntWritable> {\n  public void reduce(Text key,Iterator<IntWritable> values,\n                     OutputCollector<Text,IntWritable> output,\n                     Reporter reporter) throws IOException{\n    int sum = 0;\n    while (values.hasNext()) {\n      sum += values.next().get();\n    }\n    output.collect(key,newIntWritable(sum));\n  }\n}</pre>\n\n<strong>WordCount the Spark way:</strong>\n\n<pre>val spark = newSparkContext(master, appName, home, jars)\nval file = spark.textFile(\"hdfs://...\")\nval counts = file.flatMap(line => line.split(\" \"))\n                 .map(word =>(word,1))\n                 .reduceByKey(_ + _)\ncounts.saveAsTextFile(\"hdfs://...\")</pre>\n\nOne cantankerous data scientist at Cloudera, Uri Laserson, wrote his first PySpark job recently after several years of tussling with raw MapReduce. Two days into Spark, he declared his intent to never write another MapReduce job again.\n\nUri, we got your back, buddy: <a href=\"http://vision.cloudera.com/apache-spark-welcome-to-the-cdh-family/\">Spark will ship inside CDH 5.</a>\n\n<h2>Further Reading</h2>\n\n<ul>\n\t<li><a href=\"http://spark.apache.org/docs/latest/quick-start.html\">Spark Quick Start</a></li>\n\t<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\">Spark API for Scala</a></li>\n        <li><a href=\"http://spark.apache.org/docs/latest/api/java/index.html\">Spark API for Java</a></li>\n\t<li><a href=\"http://spark.apache.org/docs/latest/api/python/index.html\">Spark API for Python</a></li>\n</ul>\n\n<em>Jai Ranganathan is Director of Product at Cloudera.</em>\n\n<em>Matei Zaharia is CTO of Databricks.</em></td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(2014-03-19, 2014-03-19, UTC)</td><td><strong>BERKELEY, Calif. – March 18, 2014 –</strong> Databricks, the company founded by the creators of Apache Spark that is revolutionizing what enterprises can do with Big Data, today announced the Databricks <a href=\"/certification/\">“Certified on Spark” Program</a> for applications built on top of the Apache Spark platform. This program ensures that certified applications will work with a multitude of commercially supported Spark distributions.\n\n“Pioneering application developers that are leveraging the power of Spark have had to choose between two sub-optimal choices: they either have to package Spark platform support with their application or attempt to maintain integration/certification individually with a rapidly increasing set of commercially supported Spark distributions,” said Ion Stoica, Databricks CEO. “The Databricks ‘Certified on Spark’ program enables developers to certify solely against the 100% open-source Apache Spark distribution, and ensures interoperability with Apache Spark-compatible distributions. Databricks will handle the task of certifying the compatibility of each commercial Spark distribution with the Apache version and will soon announce the initial set of distributions that meet this criteria.”\n\nThe pioneering partners of the Databricks certification program include Adatao, Alpine Data Labs, and Tresata - advanced analytics application vendors that have been at the forefront in leveraging the power of Spark to deliver faster and deeper insights for their enterprise customers.\n\n“Certified on Spark” also provides multiple benefits for enterprise users including:\n<ul>\n \t<li>Decoupling Spark distribution (and commercial support) from application licensing</li>\n \t<li>Full transparency into which applications are truly designed to work with and leverage the power of Spark</li>\n \t<li>A rapidly increasing set of certified applications to incentivize distribution vendors to maintain compatibility with Apache Spark and avoid forking and fragmentation, ultimately resulting in greater compatibility and shared innovation for users</li>\n</ul>\n“At Databricks we are fully committed to keeping Spark 100% open source and to bringing it to the widest possible set of users,” said Matei Zaharia, Databricks CTO and the creator of Spark. “The ‘Certified on Spark’ program is key to maintaining a healthy and unified Apache Spark platform, and it is an expression of our community focused efforts, such as organizing meetups and the upcoming Spark Summit, and driving the future of open-source development of Spark.”\n\nApplication developers that are interested in applying for the “Certified on Spark” program should visit <a href=\"http://www.databricks.com\">www.databricks.com</a> and select “Apply for Certification”. Enterprise users can also visit the Databricks site regularly to see the latest set of certified application vendors and read “application spotlight” blog articles that deep-dive into specific examples of Spark-powered applications.</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-03-03, 2014-03-03, UTC)</td><td><div class=\"blogContent\">\n\nWe are delighted with the recent <a href=\"https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\">announcement</a> of the Apache Software Foundation that <a href=\"http://spark.apache.org\">Apache Spark</a> has become a top-level Apache project. This is a recognition of the fantastic work done by the Spark open source community, which now counts over 140 developers from 30+ companies.\n\nIn short time, Spark has become an increasingly popular solution for numerous big data applications, including machine learning, interactive queries, and stream processing. Spark now is an integral part of the Hadoop ecosystem, with many organizations employing Spark to perform sophisticated processing on their Hadoop data.\n\nAt Databricks we are looking forward to continuing our work with the open source community to accelerate the development and adoption of Apache Spark. Currently employing the lead developers and creators of many of the components of the Spark ecosystem, including Matei Zaharia, the creator of Spark, and Patrick Wendell, the release manager of Spark, we are committed to the success of Apache Spark.\n\n</div></td></tr><tr><td>List(Ahir Reddy, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-02-13, 2014-02-13, UTC)</td><td>The AMPLab at UC Berkeley, with help from Databricks, recently released an update to the <a href=\"https://amplab.cs.berkeley.edu/benchmark/\">Big Data Benchmark</a>. This benchmark uses Amazon EC2 to compare performance of five popular SQL query engines in the Big Data ecosystem on common types of queries, which can be reproduced through publicly available scripts and datasets.\n\nIn the past year, the community has invested heavily in performance optimizations of query engines. We are glad to see that all projects have evolved in this area. Although the queries used in the benchmark are simple, we are proud that Shark remains one of the fastest engines for these workloads, and has improved significantly since the last run.\n\nWhile this benchmark reaffirms Shark as a highly performant SQL query engine, we are working hard at Databricks to push the boundaries further. Stay tuned for some exciting news we will share soon with the community.\n<ul>\n\t<li><a href=\"https://amplab.cs.berkeley.edu/benchmark/\">Big Data Benchmark</a></li>\n</ul>\n&nbsp;</td></tr><tr><td>List(Pat McDonough)</td><td>List(Company Blog, Events)</td><td>List(2014-02-11, 2014-02-11, UTC)</td><td>The Databricks team is excited to take part in a number of activities throughout the 2014 O’Reilly Strata Conference in Santa Clara. From hands-on training, to office hours, to several talks (including a keynote), there are plenty of chances for attendees to learn how Apache Spark is bringing ease of use and outstanding performance to your big data. The schedule for the Databricks team includes:\n<ul>\n \t<li><a href=\"http://ampcamp.berkeley.edu/4/\">AMPCamp4</a>, Hosted at Strata</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/content/office-hours\">Office Hours</a> on Wednesday at 5:45pm</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/schedule/detail/33057\">How Companies are Using Spark, and Where the Edge in Big Data Will Be</a>, a keynote talk presented by Matei Zaharia on Thursday at 9:15am</li>\n \t<li><a href=\"http://strataconf.com/strata2014/public/schedule/detail/32375\">Querying Petabytes of Data in Seconds with BlinkDB</a>, co-presented by Reynold Xin on Thursday at 1:30pm</li>\n</ul>\n<h2 id=\"about-ampcamp-4\">About AMPCamp 4</h2>\nWe’ll be kicking off the conference on Tuesday by helping to host the fourth iteration of AMPCamp, where some of the newer components of the Berkeley Data Analytics stack will get some time in the spotlight. There are talks on BlinkDB, MLbase, GraphX, and Tachyon in the morning, and hands-on training for BlinkDB, MLbase, Spark, Spark Streaming, GraphX, and Shark in the afternoon.\n<h2 id=\"conference-talks--office-hours\">Conference Talks &amp; Office Hours</h2>\nOn Wednesday afternoon, attendees can come speak directly with a few of the members from our team in Strata’s Office Hours. Office hours provide a chance to meet face-to-face with the Databricks team and to ask any questions about the Spark project and ecosystem.\n\nTo learn the latest about Apache Spark, attendees should listen to talks featuring the Databricks team, including a keynote by our CTO Matei Zaharia. On Thursday morning, Matei will describe how organizations must now compete on the speed and sophistication with which they can draw value from data, and how Apache Spark makes a new class of applications possible.\n<h2 id=\"well-see-you-there\">We’ll See You There!</h2>\nWe’re looking forward to seeing you at the conference!</td></tr><tr><td>List(Ion Stoica)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(2014-01-22, 2014-01-22, UTC)</td><td>We are often asked how does <a href=\"http://spark.incubator.apache.org\">Apache Spark</a> fits in the Hadoop ecosystem, and how one can run Spark in a existing Hadoop cluster. This blog aims to answer these questions.\n\nFirst, Spark is intended to <em>enhance</em>, not replace, the Hadoop stack. From day one, Spark was designed to read and write data from and to HDFS, as well as other storage systems, such as HBase and Amazon’s S3. As such, Hadoop users can enrich their processing capabilities by combining Spark with Hadoop MapReduce, HBase, and other big data frameworks.\n\nSecond, we have constantly focused on making it as easy as possible for <em>every Hadoop user</em> to take advantage of Spark’s capabilities. No matter whether you run Hadoop 1.x or Hadoop 2.0 (YARN), and no matter whether you have administrative privileges to configure the Hadoop cluster or not, there is a way for you to run Spark! In particular, there are three ways to deploy Spark in a Hadoop cluster: standalone, YARN, and SIMR.\n\n<img class=\"aligncenter\" src=\"https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png\" alt=\"SparkHadoop1.png\" />\n\n<strong>Standalone deployment</strong>: With the standalone deployment one can statically allocate resources on all or a subset of machines in a Hadoop cluster and run Spark side by side with Hadoop MR. The user can then run arbitrary Spark jobs on her HDFS data. Its simplicity makes this the deployment of choice for many Hadoop 1.x users.\n\n<strong><a href=\"https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\">Hadoop Yarn</a> deployment</strong>: Hadoop users who have already deployed or are planning to deploy <a href=\"https://hadoop.apache.org/docs/current2/hadoop-yarn/hadoop-yarn-site/YARN.html\">Hadoop Yarn</a> can simply run Spark on YARN without any pre-installation or administrative access required. This allows users to easily integrate Spark in their Hadoop stack and take advantage of the full power of Spark, as well as of other components running on top of Spark.\n\n<strong>Spark In MapReduce (<a href=\"https://databricks.com/blog/2014/01/01/simr.html\">SIMR</a>)</strong>: For the Hadoop users that are not running YARN yet, another option, in addition to the standalone deployment, is to use SIMR to launch Spark jobs inside MapReduce. With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it! This tremendously lowers the barrier of deployment, and lets virtually everyone play with Spark.\n<h2 id=\"interoperability-with-other-systems\">Interoperability with other Systems</h2>\nSpark interoperates not only with Hadoop, but with other popular big data technologies as well.\n<ul>\n \t<li><strong><a href=\"http://hive.apache.org/\">Apache Hive</a></strong>: Through <a href=\"https://github.com/amplab/shark/wiki\">Shark</a>, Spark enables Apache Hive users to run their unmodified queries much faster. Hive is a popular data warehouse solution running on top of Hadoop, while Shark is a system that allows the Hive framework to run on top of Spark instead of Hadoop. As a result, Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk.</li>\n \t<li><strong><a href=\"http://aws.amazon.com/\">AWS EC2</a></strong>: Users can easily run Spark (and Shark) on top of Amazon’s EC2 either using the <a href=\"http://spark.incubator.apache.org/docs/latest/ec2-scripts.html\">scripts</a> that come with Spark, or the hosted <a href=\"http://aws.amazon.com/articles/4926593393724923\">versions of Spark and Shark</a> on Amazon’s Elastic MapReduce.</li>\n \t<li><strong><a href=\"http://mesos.apache.org/\">Apache Mesos</a></strong>: Spark runs on top of Mesos, a cluster manager system which provides efficient resource isolation across distributed applications, including <a href=\"http://www.mcs.anl.gov/research/projects/mpi/\">MPI</a> and Hadoop. Mesos enables <em>fine grained</em> sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution. This leads to considerable performance improvements, especially for long running Spark jobs.</li>\n</ul></td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2013-12-20, 2013-12-20, UTC)</td><td>We are happy to announce the release of Apache Spark 0.8.1. In addition to performance and stability improvements, this release adds three new features. First, Spark now supports for the newest versions of YARN (2.2+). Second, the standalone cluster manager supports a high-availability mode in which it can tolerate master failures. Third, shuffles have been optimized to create fewer files, improving shuffle performance drastically in some settings.\n\nIn conjunction with the Apache Spark 0.8.1 release we are separately releasing <a href=\"https://databricks.com/blog/2014/01/01/simr.html\">Spark In MapReduce (SIMR)</a>, which enables seamlessly running Spark on Hadoop MapReduce v1 clusters without requiring the installation of Scala or Spark.\n\nWhile Apache Spark 0.8.1 is a minor release, it includes these larger features for the benefit of Scala 2.9 users. The next major release of Apache Spark, 0.9.0, will be based on Scala 2.10.\n\nThis release was a community effort, featuring contributions from more than 40 developers. For much more information about Apache Spark 0.8.1, head over to the <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-8-1.html\">release notes</a> or <a href=\"http://spark.incubator.apache.org/downloads.html\">download Apache Spark 0.8.1</a> and try it out for yourself.</td></tr><tr><td>List(Andy Konwinski)</td><td>List(Company Blog, Customers, Events)</td><td>List(2013-12-19, 2013-12-19, UTC)</td><td>Earlier this month we held the <a href=\"http://spark-summit.org/2013\">first Spark Summit</a>, a conference to bring the Apache Spark community together. We are excited to share some statistics and highlights from the event.\n<ul>\n \t<li>450 participants from over 180 companies attended</li>\n \t<li>Participants came from 13 countries</li>\n \t<li>Spark training was sold out at 200 participants from 80 companies</li>\n \t<li>20 organizations sponsored the event, including all major Hadoop platform vendors</li>\n \t<li>20 different organizations gave talks</li>\n</ul>\nVideos and slides for all talks are now available on the <a href=\"http://spark-summit.org/2013\">Summit 2013 page</a>.\n\nThe Summit included Keynotes from Databricks, the UC Berkeley AMPLab, and Yahoo, as well as presentations from 18 other companies including Amazon, Red Hat, and Adobe. Talk topics covered a wide range including specialized applications such as mapping and manipulating the brain, product launches, and research projects about future directions for the project. We are very excited to see Spark and related projects come such a long way from research prototypes originally developed in AMPLab at Berkeley to being used in production by startups and large companies alike.\n<h2 id=\"the-state-of-spark-and-where-were-going-next\">The State of Spark, and Where We’re Going Next</h2>\n<iframe style=\"float: left; padding: 10px;\" src=\"//www.youtube.com/embed/nU6vO2EJAb4\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIn the first keynote of the day, Matei Zaharia, who started the Spark project and is now CTO at Databricks, gave a summary of recent growth in the projec<span style=\"letter-spacing: 3px;\">t</span><a style=\"vertical-align: super; font-size: 60%;\" href=\"#footnote-1\">1</a>, highlighting key contributions from across the community. In particular, Spark recently reached 100 contributors, making it the second-largest open source development community in the Big Data space after Hadoop, and it’s also overtaken Hadoop in the past 6 months. Matei also previewed what is coming up in the Spark development roadmap, including features under development for the upcoming 0.8.1 and 0.9 releases including high availability for the Spark Master in standalone mode, external hashing and sorting, support for Scala 2.10, and more. Finally, Matei discussed features of Spark that differentiate it from other projects, such as the focus on unification of diverse programming models.\n<h2 id=\"spark-and-hadoop\">Spark and Hadoop</h2>\n<iframe style=\"float: right; padding-left: 10px;\" src=\"//www.youtube.com/embed/qs40jiN2iwM\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nIt was exciting to hear from Eric Baldeschwieler, whose background includes leading the Yahoo team that took Hadoop from being a prototype project to what it is today, as well as serving as both CEO and CTO of Hortonworks. In his keynote, Eric presented his view of how Spark is on track to become the “lingua franca” for Big Data. He also talked about how Spark updates Hadoop with important features such as effective utilization of RAM, low latency queries, and streaming ingest. Finally, he discussed how the Spark and Hadoop projects relate to each other now and going forward.\n<h2 id=\"spark-training-day\">Spark Training Day</h2>\nOn the sold-out second day, 200 attendees heard 4 talks on using, deploying, and administering Spark, and also participated in hands-on training led by the creators of Spark. Amazon, a sponsor of the Summit, donated EC2 resources so participants could each have their own 6 node Spark cluster to practice using Spark, Spark Streaming and Shark on real Wikipedia and Twitter data. We have made the hands-on exercises <a href=\"http://spark-summit.org/exercises\">available on the summit website</a> for free. Using these, anybody can use their own Amazon AWS account to launch a 6 node Spark cluster and get experience analyzing real data.\n\n<iframe style=\"float: left; padding-right: 10px;\" src=\"//www.youtube.com/embed/zGW8glN-Mo8\" width=\"300\" height=\"173\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\nThe final talk of the training day was given by core Spark developer and Databricks cofounder Patrick Wendell. Patrick’s talk was about Administering Spark and was prepared in response to requests for more advanced technical material as part of the training. In it he dove into the core software components of the project, the different resource managers that Spark runs on, what type of hardware to use when running Spark, how to link against Spark when writing applications, monitoring Spark clusters running in production, and more.\n<h2 id=\"other-interesting-talks\">Other Interesting Talks</h2>\nIn addition to those above, there were many more great talks at the Summit, here are a few more that highligh the diversity of topics covered:\n<ul class=\"talk-list\">\n \t<li><span class=\"talk-title\">Mapping and manipulating the brain at scale</span> (<a href=\"http://spark-summit.org/talk/freeman-mapping-and-manipulating-the-brain-at-scale/\">abstract</a>, <a href=\"http://www.jeremyfreeman.net/share/talks/spark5/\">slides</a>, <a href=\"http://www.youtube.com/watch?v=7mmcEl_1CPw\">video</a>). <span class=\"speaker-info\">Jeremy Freeman, HHMI Janelia Farm Research Campus</span></li>\n \t<li><span class=\"talk-title\">Beyond Word Count – Productionalizing Spark Streaming</span> (<a href=\"http://spark-summit.org/talk/weald-beyond-word-count-productionalizing-spark-streaming/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Productionalizing-Spark-Streaming-Spark-Summit-2013-copy.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=OhpjgaBVUtU\">video</a>). <span class=\"speaker-info\">Ryan Weald, Sharethrough</span></li>\n \t<li><span class=\"talk-title\">Hadoop and Spark Join Forces in Yahoo</span> (<a href=\"http://spark-summit.org/talk/feng-hadoop-and-spark-join-forces-at-yahoo/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Feng-Andy-SparkSummit-Keynote.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=GbFtbIepk-s\">video</a>). <span class=\"speaker-info\">Andy Feng, Distinguished Architect, Cloud Services, Yahoo</span></li>\n \t<li><span class=\"talk-title\">Next-Generation Spark Scheduling with Sparrow</span> (<a href=\"http://spark-summit.org/talk/ousterhout-next-generation-spark-scheduling-with-sparrow/\">abstract</a>, <a href=\"http://spark-summit.org/wp-content/uploads/2013/10/Kay_Sparrow_Spark_Summit.pdf\">slides</a>, <a href=\"http://www.youtube.com/watch?v=ayjH_bG-RC0\">video</a>). <span class=\"speaker-info\">Kay Ousterhout, UC Berkeley AMPLab</span></li>\n</ul>\n<h2 id=\"the-upcoming-spark-summit-2014\">The Upcoming Spark Summit 2014</h2>\nThe next Summit will be this Summer 2014, and you can <a href=\"http://spark-summit.org/2014/pre-register\">pre-register now</a>. We look forward to seeing you there!\n<h2 id=\"other-summit-resources\">Other Summit Resources</h2>\n<ul>\n \t<li>Follow the <a href=\"http://twitter.com/spark_summit\">@spark_summit</a> twitter handle</li>\n \t<li>Like the <a href=\"http://facebook.com/ApacheSparkSummit\">Summit Facebook page</a></li>\n \t<li>More <a href=\"https://www.dropbox.com/sc/k6l01hiv4zbhhfg/hhCzeS_U9P\">Summit 2013 photos</a></li>\n \t<li>A look <a href=\"http://strata.oreilly.com/2013/11/behind-the-scenes-of-the-first-spark-summit.html\">behind the scenes look at the first Spark Summit</a></li>\n</ul>\n<h2 id=\"footnotes\">Footnotes</h2>\n<ol>\n \t<li>For more on this, check out <a href=\"/blog/2013/10/27/the-growing-spark-community.html\" name=\"footnote-1\">our recent blog post about the growth of the Spark community</a>.</li>\n</ol></td></tr><tr><td>List(Pat McDonough)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2013-11-22, 2013-11-22, UTC)</td><td>[sidenote]A version of this post appears on the <a href=\"http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/\">Cloudera Blog</a>.[/sidenote]\n\n<hr/>\n\nApache Hadoop has revolutionized big data processing, enabling users to store and process huge amounts of data at very low costs. MapReduce has proven to be an ideal platform to implement complex batch applications as diverse as sifting through system logs, running ETL, computing web indexes, and powering personal recommendation systems. However, its reliance on persistent storage to provide fault tolerance and its one-pass computation model make MapReduce a poor fit for low-latency applications and iterative computations, such as machine learning and graph algorithms.\n\nApache Spark addresses these limitations by generalizing the MapReduce computation model, while dramatically improving performance and ease of use.\n<h2 id=\"fast-and-easy-big-data-processing-with-spark\">Fast and Easy Big Data Processing with Spark</h2>\nAt its core, Spark provides a general programming model that enables developers to write application by composing arbitrary operators, such as mappers, reducers, joins, group-bys, and filters. This composition makes it easy to express a wide array of computations, including iterative machine learning, streaming, complex queries, and batch.\n\nIn addition, Spark keeps track of the data that each of the operators produces, and enables applications to reliably store this data in memory. This is the key to Spark’s performance, as it allows applications to avoid costly disk accesses. As illustrated in the figure below, this feature enables:\n\n<img style=\"float: left; width: 280px; margin: 15px 5px;\" src=\"/wp-content/uploads/2013/11/using-ram.png\" alt=\"Efficiently Leveraging Memory\" />\n<ul>\n \t<li>Low-latency computations by caching the working dataset in memory and then performing computations at memory speeds, and</li>\n \t<li>Efficient iterative algorithm by having subsequent iterations share data through memory, or repeatedly accessing the same dataset</li>\n</ul>\nSpark’s ease-of-use comes from its general programming model, which does not constrain users to structure their applications into a bunch of map and reduce operations. Spark’s parallel programs look very much like sequential programs, which make them easier to develop and reason about. Finally, Spark allows users to easily combine batch, interactive, and streaming jobs in the same application. As a result, a Spark job can be up to 100x faster and requires writing 2-10x less code than an equivalent Hadoop job.\n<h2 id=\"using-spark-for-advanced-data-analysis-and-data-science\">Using Spark for Advanced Data Analysis and Data Science</h2>\n<h3 id=\"interactive-data-analysis\">Interactive Data Analysis</h3>\nOne of Spark’s most useful features is the interactive shell, bringing Spark’s capabilities to the user immediately – no IDE and code compilation required. The shell can be used as the primary tool for exploring data interactively, or as means to test portions of an application you’re developing.\n\nThe screenshot to the right shows a Spark Python shell in which the user loads a file and then counts the number of lines that contain “Holiday”.\n\n<img style=\"display: block; width: 480px; margin: 1em auto;\" src=\"/wp-content/uploads/2013/11/spark-python-shell.png\" alt=\"Spark's Python Shell\" />\n\nAs illustrated in this example, Spark can read and write data from and to HDFS. Thus, as soon as Spark is installed, a Hadoop user can immediately start analyzing HDFS data. Then, by caching a dataset in memory, a user can perform a large variety of complex computations interactively!\n\nSpark also provides a Scala shell, and APIs in Java, Scala, and Python for stand-alone applications.\n<h3 id=\"faster-batch\">Faster Batch</h3>\nSome of the earliest deployments of Spark have focused on how to improve performance in existing MapReduce applications. Remember that MapReduce is actually a generic execution framework and is not exclusive to it’s most well-known implementation in core Hadoop. Spark provides MapReduce as well, and because it can efficiently use memory (while using lineage to recover from failure if necessary), some implementations are simply faster in Spark’s MapReduce as compared to Hadoop’s MapReduce right off the bat, before you even get in to leveraging cache for iterative programs.\n\nThe example below illustrates Spark’s implementation of MapReduce’s most famous example, word count. You can see that Spark supports operator chaining. This becomes very useful when doing a bit of pre- or post-processing on your data, such as filtering data prior to running a complex MapReduce job.\n\n<pre>val file = sc.textFile(\"hdfs://.../pagecounts-*.gz\");\nval counts = file.flatMap(line => line.split(\" \"));\n  .map(word => (word, 1))\n  .reduceByKey(_ + _)\ncounts.saveAsTextFile(\"hdfs://.../word-count\");</pre>\n\nSpark’s batch capabilities have been proven in real-world scenarios. A very large Silicon Valley Internet company did a plain-vanilla port of a single MR job implementing feature extraction in a model training pipeline, and saw a 3x speedup.\n<h3 id=\"iterative-algorithms\">Iterative Algorithms</h3>\n<img style=\"float: right; width: 140px; margin: 0 10px;\" src=\"https://databricks.com/wp-content/uploads/2013/11/logistic-regression-performance.png\" alt=\"Logistic Regression Performance\" />\nSpark allow users and applications to explicitly cache a dataset by calling the cache() operation. This means that your applications can now access data from RAM instead of disk, which can dramatically improve the performance of iterative algorithms that access the same dataset repeatedly. This use case covers an important class of applications, as all machine learning and graph algorithms are iterative in nature.\n\nTwo of the world’s largest Internet companies leverage Spark’s efficient iterative execution to provide content recommendations and ad targeting. Machine-learning algorithms such as logistic regression have run 100x faster than previous Hadoop-based implementations (see the plot to the right), while other algorithms such as collaborative filtering or alternating direction method of multipliers have run over 15x faster.\n\nThe following example uses logistic regression to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. Note the cached dataset “points” is accessed repeatedly from memory, whereas in MapReduce, each iteration will read data from the disk, which incurs a huge overhead.\n\n<pre>val points = sc.textFile(\"...\").map(parsePoint).cache()\nvar w = Vector.random(D) //current separating plane\nfor (i <- 1 to ITERATIONS) {\n  val gradient = points.map(p =>\n    (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y * p.x)\n    .reduce(_ + _)\n    w -= gradient\n}\nprintln(\"Final separating plane: \" + w)</pre>\n\n<h3 id=\"real-time-stream-processing\">Real-Time Stream Processing</h3>\nWith a low-latency data analysis system at your disposal, it’s natural to extend the engine towards processing live data streams. Spark has an API for working with streams, providing exactly-once semantics and full recovery of stateful operators. It also has the distinct advantage of giving you the same Spark APIs to process your streams, including reuse of your regular Spark application code.\n\nThe code snippet below shows a simple job processing a network stream, filtering for words beginning with a hashtag and performing a word count on every 10 seconds of data. Compare this to the previous word-count example and you’ll see how almost the exact same code is used, but this time processing a live data stream.\n\n<pre> val ssc = new StreamingContext(\n  args(0), \"NetworkHashCount\",\n  Seconds(10), System.getenv(\"SPARK_HOME\"),\n  Seq(System.getenv(\"SPARK_EXAMPLES_JAR\")))\n\nval lines = ssc.socketTextStream(\"localhost\", 9999)\nval words = lines.flatMap(_.split(\" \"))\n  .filter(_.startsWith(\"#\"))\nval wordCounts = words.map(x => (x, 1))\n  .reduceByKey(_ + _)\nwordCounts.print()\nssc.start()</pre>\n\nAlthough the Spark Streaming API was released less than a year ago, users have deployed it in production to provide monitoring and alerting against stateful, aggregated data from system logs, achieving very fast processing with only seconds of latency.\n<h3 id=\"faster-decision-making\">Faster Decision-Making</h3>\nMany companies use big data to make or facilitate user’s decisions in the form of recommendation systems, ad targeting, or predictive analytics. One of the key properties of any decision is latency — that is, the time it takes to make the decision from the moment the input data is available. Reducing decision latency can significantly increase their effectiveness, and ultimately increase the company’s return on investment. Since many of these decisions are based on complex computations (such as machine learning and statistical algorithms), Spark is an ideal fit to speed up decisions.\n\nNot surprisingly, Spark has been deployed to improve decision quality as well as to reduce latency. Examples range from ad targeting, to improving the quality of video delivery over the Internet.\n<h3 id=\"unified-pipelines\">Unified Pipelines</h3>\nMany of today’s Big Data deployments go beyond MapReduce by integrating other frameworks for streaming, batch, and interactive computation. Users can dramatically reduce the complexity of their data processing pipelines by replacing several systems with Spark.\n\nFor instance, today, many companies use MapReduce to generate reports and answer historical queries, and deploy a separate system for stream processing to follow key metrics in real-time. This approach requires one to maintain and manage two different systems, as well as develop applications for two different computation models. It would also require one to make sure the results provided by the two stacks are consistent (for example, a count computed by the streaming application and the same count computed by MapReduce).\n\nRecently, users have deployed Spark to implement stream processing as well as batch processing for providing historical reports. This not only simplifies deployment and maintenance, but dramatically simplifies application development. For example, maintaining the consistency of real-time and historical metrics is no longer a problem as they are computed using the same code. A final benefit of the unification is improved performance, as there is no need to move the data between different systems: once in-memory, the data can be shared between the streaming computations and historical (or interactive) queries.\n<h3 id=\"your-turn-go-get-started\">Your Turn: Go Get Started</h3>\nSpark is very easy to get started writing powerful Big Data applications. Your existing Hadoop and/or programming skills will have you productively interacting with your data in minutes. Go get started today:\n<ul>\n \t<li><a href=\"http://spark.incubator.apache.org/downloads.html\">Download Spark</a></li>\n \t<li><a href=\"http://spark.incubator.apache.org/docs/latest/quick-start.html\">Quick Start</a></li>\n \t<li><a href=\"http://spark-summit.org\">Spark Summit (2013 Conference Talks and Training)</a></li>\n</ul></td></tr><tr><td>List(Ion Stoica)</td><td>List(Company Blog, Partners)</td><td>List(2013-10-29, 2013-10-29, UTC)</td><td>Today, Cloudera announced that it will distribute and support Apache Spark. We are very excited about this announcement, and what it brings to the Spark platform and the open source community. So what does this announcement mean for Spark?\n\nFirst, it validates the maturity of the Spark platform. Started as a research project at UC Berkeley in 2009, Spark is the first general purpose cluster computing engine that can run sophisticated computations at memory speeds on Hadoop clusters. Spark started with the goal of providing efficient support for iterative algorithms (such as machine learning) and interactive queries, workloads not well supported by MapReduce. Since then, Spark has grown to support other applications such as streaming, and has gained rapid industry adoption. Today, Spark is used in production by numerous companies, and it counts on an ever growing open source community with over 90 contributors from 25 companies.\n\nSecond, it will make the Spark platform available to a wide range of enterprise customers both in US and internationally. By being distributed in conjunction with <a href=\"http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html\">Cloudera’s CDH</a>, Spark will enjoy the same enterprise-grade support as the other components in Cloudera’s stack. Databricks is fully committed to working with Cloudera to guarantee that its customers will have the best possible support. Furthermore, we are looking forward to this partnership to enable new categories of exciting applications, and address unique usage scenarios.\n\nThird, this partnership underlines and strengthens the integration of Spark into the Hadoop ecosystem. Spark has the ability to read Hadoop files, share data with other Hadoop frameworks, and support existing Hadoop workloads, including Hive queries. This integration is beneficial not only for Spark, but for the Hadoop ecosystem as a whole, as Spark brings new capabilities to the Hadoop ecosystem through its ability to run on top of Hadoop YARN. This could give Hadoop users the opportunity to run jobs up to 100x faster than MapReduce, while writing 2-5x less code. For example, a data scientist could leverage Spark’s simple yet powerful API to rapidly develop machine learning algorithms, and then run them at memory speeds on her Hadoop data.\n\nFinally, we want to reiterate our full commitment to open source. The success Spark has enjoyed thus far has only been possible because of a vibrant open source community, who has contributed a continuous stream of new functionality and bug fixes. We believe this partnership will ignite a new wave of growth of our community and accelerate the development of the Apache Spark platform to support an ever growing number of customers.\n\nWe have no doubt that we are just at the beginning of a journey to give users the tools to solve tomorrow’s big data challenges. The next stop in this journey is the <a href=\"http://spark-summit.org\">Spark Summit</a>. Sponsored by leading big data companies and Spark users, including Databricks and Cloudera, this is the first conference that will bring together the Spark community. Come and join us on this journey!</td></tr><tr><td>List(Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(2013-10-28, 2013-10-28, UTC)</td><td>This year has seen unprecedented growth in both the user and contributor communities around <a href=\"http://spark.incubator.apache.org\">Apache Spark</a>. This rapid growth validates the tremendous potential of the platform, and shows the great excitement around it.\n\nWhile Spark started as a research project by a few grad students at UC Berkeley in 2009, today <strong>over 90 developers from 25 companies have contributed to Spark</strong>. This is not counting contributors to Shark (Hive on Spark), of which there are 25. Indeed, out of the many new big data engines created in the past few years, <strong>Spark has the largest development community after Hadoop MapReduce</strong>. We believe that new components in the project, like <a href=\"http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming</a> and <a href=\"http://spark.incubator.apache.org/docs/latest/mllib-guide.html\">MLlib</a>, will only increase this growth.\n<h2>Growth by Numbers</h2>\nTo give a sense of the growth of the project, the graph below shows the number of contributors to each major Spark release in the past year.\n<div style=\"text-align: center;\"><strong>Past Year Spark Releases</strong>\n<img class=\"aligncenter wp-image-82\" src=\"/wp-content/uploads/2013/10/growth-of-spark-graphic.png\" alt=\"growth-of-spark-graphic\" width=\"507\" height=\"300\" /></div>\nThe number of contributors quadrupled between October 2012 and 2013, and each of our releases has been getting bigger in terms of features. In addition, an increasing number of Spark’s major features have been contributed by the user community. These include Hadoop YARN support in Apache Spark 0.8; metrics collection; new machine learning algorithms and examples; fair scheduling; and column-oriented compression in Shark. At Databricks, we plan to continue working with the open source community to expand Apache Spark.\n\nA final indicator of growth is conferences and events. The <a href=\"http://ampcamp.berkeley.edu\">AMP Camp</a> training camp at Berkeley was sold out with members from over 100 companies attending, while our <a href=\"http://www.meetup.com/spark-users/\">San Francisco user meetup</a> has grown to 1,300 members. We’re excited to continue organizing such events.\n<h2>Bringing the Community Together: The First Spark Summit</h2>\n<img style=\"float: right;\" src=\"/wp-content/uploads/2013/10/Summit-Logo-FINALtr-150x150px.png\" alt=\"Summit-Logo-FINALtr-150x150px\" width=\"150\" height=\"150\" />\nTo celebrate the growth around Spark and bring users and contributors together, we’re excited to host the <a href=\"http://spark-summit.org\">first Spark Summit</a>, on December 2nd and 3rd, 2013. Sponsored by leading big data companies and production users of Spark, this will be the first conference around the Spark stack. In addition to talks from users and developers, the Summit and will include a day of <a href=\"http://www.spark-summit.org/agenda/\">hands-on Spark training</a>. We’re looking forward to your continuous involvement to expand Spark and tackle tomorrow’s big data challenges. So whether you’re a big data veteran or new to Spark, come by to learn how to use it to solve your problems.</td></tr><tr><td>List(Ion Stoica, Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(2013-10-27, 2013-10-27, UTC)</td><td>When we announced that the original team behind <a href=\"http://spark.incubator.apache.org\">Apache Spark</a> is starting a company around the project, we got a lot of excited questions. What areas will the company focus on, and what will it mean for the open source project? Today, in our first blog post at Databricks, we’re happy to share some of our goals, and say a little about what we’re doing next with Spark.\n\nTo start with, our mission at Databricks is simple: we want to build the very best computing platform for extracting value from data. Big data is a tremendous opportunity that is still largely untapped, and we’ve been working for the past six years to transform what can be done with it. Going forward, we are fully committed to building out the open source Apache Spark platform to achieve this goal.\n<h2 id=\"how-we-think-about-big-data-speed-and-sophistication\">How We Think about Big Data: Speed and Sophistication</h2>\nIn the past few years, open source technologies like Hadoop have made it dramatically easier to store large volumes of data. This capability is transforming a wide range of industries, from brick-and-mortar enterprises to the web. Over time though, simply collecting big data will not be enough to maintain a competitive edge. The question will be what can you do with this data.\n\nWe believe that two axes will determine how well an organization can draw value from data: <strong>speed</strong> and <strong>sophistication</strong>. By speed, we mean not only the speed at which we compute and return answers, but also the speed of development: how quickly can users take a new idea from the drawing board to a production application? By sophistication, we mean what type of analysis can be done. Today’s big data systems do not support the sophisticated analysis functions in tools like R and Matlab, limiting their scope. Enabling these types of analyses would greatly increase their value.\n\nThrough the Apache Spark project, we’ve been working to address both axes in a way that works seamlessly with the Hadoop stack. Released in 2010, Spark remains the only widely deployed engine for Hadoop to support in-memory computing and general execution graphs, as well as the easiest way to program applications on Hadoop data, with APIs in Scala, Java and Python. Released shortly after, <a href=\"http://shark.cs.berkeley.edu\">Shark</a> was the first system to speed up Hive by 100x, and is the only one of the new “SQL on Hadoop” engines to retain full Hive compatibility (by building directly on Hive) and to support in-memory computation. Looking forward, libraries like <a href=\"http://spark.incubator.apache.org/docs/latest/mllib-guide.html\">MLlib</a> and <a href=\"http://www.meetup.com/spark-users/events/124935592/\">GraphX</a> are making it easy to call sophisticated machine learning and graph algorithms from Spark, while running them at memory speeds. These tools have already given numerous organizations the ability to do faster and richer data analysis, and we hope to bring them to hundreds more.\n<h2 id=\"what-were-working-on\">What We’re Working On</h2>\nAt Databricks, we’re committed to bringing Spark to an ever-wider set of users and greatly increasing its capabilities. Through both the recent <a href=\"http://spark.incubator.apache.org/releases/spark-release-0-8-0.html\">Apache Spark 0.8 release</a> and our ongoing work, we’ve been building out quite a few new features. Expect to see a focus in the following areas:\n<ul>\n \t<li><strong>Deployment:</strong> We want to make Spark effortless to deploy for any user, whether with or without an existing Hadoop cluster. Apache Spark 0.8 made significant strides in this respect with improved support for Mesos, EC2 and Hadoop YARN.</li>\n \t<li><strong>High availability</strong>: One exciting feature that we’ve already merged into Apache Spark 0.8.1 is <a href=\"https://github.com/apache/incubator-spark/pull/19\">high availability for the master node</a>. In general, due to the many users who are running Spark in availability-critical settings (e.g. streaming or user-facing applications), we want to make availability throughout the stack easier.</li>\n \t<li><strong>New features:</strong> Besides these top-level goals, we have an exciting roadmap of features, such as Scala 2.10 support, new machine learning algorithms, graph computation, and updates to <a href=\"http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming</a>, coming soon.</li>\n</ul>\nMost importantly, we believe that, despite the effort in the past few years, big data processing is still in its infancy, and there is tremendous room for tools that are faster, easier to use, and capable of richer computation. We hope you join us in defining the next generation of big data systems and unlocking the speed and sophistication that we believe is possible for big data.</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Partners)</td><td>List(2014-04-11, 2014-04-11, UTC)</td><td>Today, MapR announced that it will distribute and support the Apache Spark platform as part of the MapR Distribution for Hadoop in partnership with Databricks. We’re thrilled to start on this journey with MapR for a multitude of reasons.\n\nOne of our primary goals at Databricks is to drive broad adoption of Spark and ensure everybody who uses it has a fantastic experience. This partnership will enable all of MapR’s enterprise customers, existing and new, to leverage Spark with the backing of the same great enterprise support available for the rest of MapR’s Hadoop Distribution. As Tomer mentioned in his <a href=\"/blog/2014/04/10/MapR-Integrates-Spark-Stack.html\">blog post</a>, Spark is one of the most common topics in discussions with MapR’s existing customers and many are even already running it in production!\n\nA core part of Spark’s value proposition is the ability to easily build a unified end-to-end workflow where critical functions are first class citizens that are seamlessly integrated into the platform. An important part of this workflow is the ability to provide SQL-based interactive queries delivered by Shark, which also serves as the gateway for a wealth of traditional and new SQL-based tools to run on top of Spark. At Databricks, we continue to work on innovating across the entire Spark platform, including Shark, and customers now have an enterprise support option for Shark available.\n\nFinally, we see this partnership as continued validation of Spark’s emergence as the leading open source processing engine in the Big Data community. Spark is the most active project in the Hadoop ecosystem in the past year with over 170 contributors, and we’re heartened to see a rapidly growing attendance at community events pointing to entirely new classes of Spark enterprise use cases. The <a href=\"/certification/\">Databricks “Certified on Spark”</a> program has seen incredible interest from application developers who are leveraging Spark to deliver deeper insights, faster to their customers.\n\nAt Databricks we are fully committed to open source, and we’re excited to partner with MapR - a company with strong support for open source Big Data projects - to together help drive continued growth and innovation of Spark. Join us at the upcoming <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a>, the largest conference focused on Spark, to learn more!</td></tr><tr><td>List(Prashant Sharma, Matei Zaharia)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-04-15, 2014-04-15, UTC)</td><td>One of Apache Spark’s main goals is to make big data applications easier to write. Spark has always had concise APIs in Scala and Python, but its Java API was verbose due to the lack of function expressions. With the addition of <a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html\">lambda expressions</a> in Java 8, we’ve updated Spark’s API to transparently support these expressions, while staying compatible with old versions of Java. This new support will be available in Apache Spark 1.0.\n<h2 id=\"a-few-examples\">A Few Examples</h2>\nThe following examples show how Java 8 makes code more concise. In our first example, we search a log file for lines that contain “error”, using Spark’s <code>filter</code> and <code>count</code> operations. The code is simple to write, but passing a Function object to <code>filter</code> is clunky:\n<h5 id=\"java-7-search-example\">Java 7 search example:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\").filter(\n  new Function<String, Boolean>() {\n    public Boolean call(String s) {\n      return s.contains(\"error\");\n    }\n});\nlong numErrors = lines.count();</pre>\n\n(If you’re new to Spark, <a href=\"http://spark.apache.org/docs/latest/quick-start.html\">JavaRDD</a> is a distributed collection of objects, in this case lines of text in a file. We can apply operations to these objects that will automatically be parallelized across a cluster.)\n\nWith Java 8, we can replace the Function object with an inline function expression, making the code a lot cleaner:\n<h5 id=\"java-8-search-example\">Java 8 search example:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\")\n                          .filter(s -> s.contains(\"error\"));\nlong numErrors = lines.count();</pre>\n\nThe gains become even bigger for longer programs. For instance, the program below implements Word Count, by taking a file (read as a collection of lines), splitting each line into multiple words, then counting the words with a reduce function.\n<h5 id=\"java-7-word-count\">Java 7 word count:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\");\n\n// Map each line to multiple words\nJavaRDD<String> words = lines.flatMap(\n  new FlatMapFunction<String, String>() {\n    public Iterable<String> call(String line) {\n      return Arrays.asList(line.split(\" \"));\n    }\n});\n\n// Turn the words into (word, 1) pairs\nJavaPairRDD<String, Integer> ones = words.mapToPair(\n  new PairFunction<String, String, Integer>() {\n    public Tuple2<String, Integer> call(String w) {\n      return new Tuple2<String, Integer>(w, 1);\n    }\n});\n\n// Group up and add the pairs by key to produce counts\nJavaPairRDD<String, Integer> counts = ones.reduceByKey(\n  new Function2<Integer, Integer, Integer>() {\n    public Integer call(Integer i1, Integer i2) {\n      return i1 + i2;\n    }\n});\n\ncounts.saveAsTextFile(\"hdfs://counts.txt\");</pre>\n\nWith Java 8, we can write this program in just a few lines:\n<h5 id=\"java-8-word-count\">Java 8 word count:</h5>\n\n<pre>JavaRDD<String> lines = sc.textFile(\"hdfs://log.txt\");\nJavaRDD<String> words =\n    lines.flatMap(line -> Arrays.asList(line.split(\" \")));\nJavaPairRDD<String, Integer> counts =\n    words.mapToPair(w -> new Tuple2<String, Integer>(w, 1))\n         .reduceByKey((x, y) -> x + y);\ncounts.saveAsTextFile(\"hdfs://counts.txt\");</pre>\n\nWe are very excited to offer this functionality, as it opens up the simple, concise programming style that Scala and Python Spark users are familiar with to a much broader set of developers.\n<h2 id=\"availability\">Availability</h2>\nJava 8 lambda support will be available in Apache Spark 1.0, which will be released in early May. Although using this syntax requires Java 8, <em>Apache Spark 1.0 will still support older versions of Java through the old form of the API</em>. Lambda expressions are simply a shorthand for anonymous inner classes, so the same API can be used in any Java version.\n<h2 id=\"learn-more-about-spark\">Learn More About Spark</h2>\nIf you’d like to learn more about Spark, the <a href=\"http://spark.apache.org/docs/latest/index.html\">official documentation</a> can help you get started today in either Java, Scala or Python. Spark is easy to run on your laptop, without any installation other than <a href=\"http://spark.apache.org/downloads.html\">downloading</a> and unzipping a release.</td></tr><tr><td>List(Databricks Training Team)</td><td>List(Announcements, Company Blog, Events)</td><td>List(2014-06-02, 2014-06-02, UTC)</td><td>Databricks is excited to launch its training program, starting with <a title=\"Spark Training\" href=\"https://databricks.com/training\">a series of hands-on Apache Spark workshops</a> designed by the creators of Apache Spark.\n\nThe first workshop, <em>Introduction to Apache Spark</em>, establishes the fundamentals of using Spark for data exploration, analysis, and building big data applications. This one day workshop is hands-on, covering topics such as: interactively working with Spark's core APIs, learning the key concepts of big data, deploying applications on common Hadoop distributions, and unifying data pipelines with SQL, Streaming, and Machine Learning.\n\nWorkshops are currently scheduled in New York, San Jose, Austin, and Chicago, with workshops in more cities to come. Visit <a title=\"Databricks Training\" href=\"https://databricks.com/training\">Databricks' training page</a> to find more information and please leave feedback there if you'd like to see a workshop in your area.\n<ul class=\"content\">\n \t<li>Jun 11 - New York, NY - <a href=\"http://apache-spark-nyc.eventbrite.com\" target=\"_blank\">More Info &amp; Registration</a></li>\n \t<li>Jun 23 - San Jose, CA - <a href=\"http://spark-san-jose.eventbrite.com/\" target=\"_blank\">More Info &amp; Registration</a></li>\n \t<li>July 2 - San Francisco, CA - <a href=\"http://spark-summit.org/2014\">More Info &amp; Registration</a></li>\n \t<li>Jul 28 - Austin, TX - <a href=\"https://www.eventbrite.com/e/hands-on-apache-spark-workshop-austin-tickets-11663609169\" target=\"_blank\">More Info &amp; Registration</a></li>\n \t<li>Aug 25 - Chicago, IL - <a href=\"http://spark-workshop-chicago.eventbrite.com\" target=\"_blank\">More Info &amp; Registration</a></li>\n</ul>\nDatabricks will continue to expand the curriculum and coverage area in the near future. Check back soon as we announce new workshops, or send feedback to <a href=\"mailto:training-feedback@databricks.com\">training-feedback@databricks.com</a>.</td></tr><tr><td>List(Claudiu Barbura (Sr. Dir. of Engineering at Atigeo LLC))</td><td>List(Company Blog, Partners)</td><td>List(2014-05-23, 2014-05-23, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://atigeo.com\">Atigeo</a> announcing the certification of their xPatterns offering.</div>\n\n<hr />\n\nHere at <a href=\"http://atigeo.com/\">Atigeo</a>, we are always looking for ways to build on, improve, and expand our big data analytics platform, Atigeo xPatterns. More than that, both our development and product management team are focused on big data and on knowing what is right for our customers: data scientists and application developers at companies who are seeking to make the best possible use of their data assets. So we all stay on the lookout for the most useful, advanced, and best-performing set of technologies available.\n\nApache Spark, for us, was a standout: We could see that making a dramatic performance improvement available to our customers and users would mean that xPattern’s analytics, modeling, and machine learning would be more responsive, and that Spark in xPatterns would give our customers an even quicker path from data ingestion to useful insights.\n\nOur development team works on the 2 nearly opposite sides of the world, here in Bellevue, Washington just outside Seattle, and in Timisoara, Romania. These teams stay in nearly constant touch online, on social media, and even in person. So we all shared this vision of xPatterns enhanced with Spark. We found Spark useful across many stages of our xPatterns big data pipeline — some highlights of which include: ingestion, data transformation, interactive data exploration, and export to NoSQL.\n\nAnd as we made this vision a reality, with infrastructure improvements and corresponding improvements in data modeling, the Strata conference in Santa Clara, California, lay ahead as a goal and a chance to share what we’d learned and built with the wider community.\n\nAnd at Strata, sure enough, xPatterns with Spark — as well as Shark, Apache Mesos, and Tachyon — was a big hit with visitors to our booth and on social media.\n\nEven then, we knew there was still more to do: We wanted to provide objective evidence that our integration of Spark would be robust and offer excellent performance across a wide range of Spark distributions, so that users could be sure that the goodness of Spark would be available to them regardless of their choice in Spark distribution. So, we made an enormous investment across our far-flung team for further development, testing, profiling, and logging. After this broad effort, we reached out to Databricks because we saw that their certification program was in line with our vision: to prevent fragmentation and forking in the Spark community and help the Spark ecosystem grow. Additionally it was based on an entirely open process: open source testing tools along with the open source Apache Spark distribution that gave us additional comfort.\n\nSo, it is with both pride — as well as thanks to the team at Databricks — that we announce that xPatterns is “Certified on Spark”.</td></tr><tr><td>List(Sarabjeet Chugh (Head of Hadoop Product Management at Pivotal Inc.))</td><td>List(Company Blog, Partners)</td><td>List(2014-05-23, 2014-05-23, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.gopivotal.com\" target=\"_blank\">Pivotal</a> describing why they’re excited to deliver Apache Spark on their world class Pivotal HD big data analytics platform suite.</div>\n\n<hr />\n\nToday, we are excited to announce the immediate availability of the full Apache Spark stack on Pivotal HD.  We have been impressed with the rapid adoption of Spark as a replacement for Hadoop’s more traditional processing engines as well as its vibrant ecosystem, and are thrilled to make it possible for Pivotal customers to run Apache Spark on Pivotal HD Hadoop.  Just as important is how we’re doing it: Pivotal HD will be part of Databricks’ upcoming certification program – meaning a commitment to provide compatibility with Apache Spark and support the growing ecosystem of Spark applications.\n\n<h2>PivotalHD and Spark</h2>\nUnlike a multi-vendor patchwork of heterogeneous solutions, Pivotal brings together an integrated full stack of technologies to allow enterprises to create a Business Data Lake. Pivotal HD 2.0.1 consists of a Hadoop distribution that is compatible with Apache Hadoop 2.x, a market-leading SQL on Hadoop query engine in HAWQ, and GemfireXD for in-memory data serving and ultra-low latency transaction processing capabilities. Together these platforms extend Pivotal’s differentiation in both the Hadoop ecosystem and the more established data warehousing markets, meeting the full spectrum of analytics requirements from batch to ultra-low latency.\n \nWith Spark, Pivotal aims to further extend this differentiation by leveraging Spark’s cutting edge capabilities and integrating it with the rest of Pivotal’s world-class platform.  Much has been written about Spark’s benefits, but what really drew us to it were the following characteristics:\n<ul>\n\n\t<li><strong>Speed: </strong>Spark can process HDFS data in-place up to 100x faster than Hadoop MapReduce using it’s in-memory-optimized architecture<p></li>\n\n\t<li><strong>Unification: </strong>Out-of-the-box Spark provides a wide breath of functionality – including streaming data support, machine learning, and graph computation – which when combined with PivotalHD give customers a full end-to-end experience<p></li>\n\n\t<li><strong>Ease of use: </strong>Spark enables developers to use Java, Scala, or Python across their entire workflow; additionally it exposes 80+ high-level operators that allow it to have 2-5x less code than similar MapReduce jobs</li>\n\n</ul>\n\nThough the traditional Hadoop processing components such as MapReduce, Pig, and Hive will remain part of PivotalHD, we imagine many customers will begin using Spark instead because of these benefits.\n\n<h2>Pivotal and Open Source</h2>\nOpen source is a critical part of Pivotal’s DNA. Pivotal has been committed to open source software through active involvement in open-source projects such as Tomcat, RabbitMQ, Redis, Hadoop, Cloud Foundry, Spring, Grails, MADlib, Chorus, and Groovy – Spark will be no different. \n\nOne of the main attractions of Spark for us is the growing community and ecosystem.  With nearly 200 contributors over the past 12 months, it is one of the most active projects in the Apache and Hadoop open-source ecosystem.  Even more exciting is the potential that the ecosystem of applications built on top of Spark holds (something that we’re obviously passionate about at Pivotal); new “powered-by-spark” applications seem to be emerging daily.\n\nHowever, we’ve seen how quickly this potential can diminished with forking and fragmentation in open source projects before.  That is why we’re excited to join Databricks in their efforts to unify the community.  Pivotal’s distribution of Spark provides full compatibility with Apache Spark, enabling the growing set of “Certified on Spark” applications to run on it out of the box.  Given the benefits for our customers, and the open and transparent nature of the process, this was an easy decision.  This effort is yet another testament of Pivotal’s commitment to open source innovation that brings value to customers. \n\n<h2>Pivotal and Databricks</h2>\nDatabricks was founded by the original team that developed Apache Spark, and is currently the driving force behind the project. When Pivotal decided to certify its distribution with full Apache Spark stack with PivotalHD and increase our involvement in the Spark community, we could think of no better ally than Databricks with whom to embark on this exciting journey. Furthermore, we’re thrilled to join their effort to maintain compatibility across the growing Spark ecosystem.  We’re excited to be making this announcement on the Databricks blog, and look forward to a long and deep relationship with our friends at Databricks.\n\n<h2>Getting Started</h2>\nTry out Pivotal’s Spark bundle on Pivotal HD 2.0.1 by obtaining the Pivotal Spark tarball and quick-start instructions <a href=\"https://support.gopivotal.com/hc/en-us/articles/203271897-Spark-on-Pivotal-Hadoop-2-0-Quick-Start-Guide\" target=\"_blank\">here</a>. The Pivotal HD 2.0.1 release is available for download <a href=\"https://network.gopivotal.com/products/pivotal-hd\" target=\"_blank\">here</a>. We would love to hear from you and welcome the opportunity to engage in a dialog. Please feel free to drop us a note at <a href=\"mailto:spark@gopivotal.com\">spark@gopivotal.com</a> if you have any questions, or if we can be of any help for your intended use case.\n\nAdditionally, make sure you come visit our booth at the upcoming <a href=\"http://www.spark-summit.org/\" target=\"_blank\">Spark Summit</a> to hear more about using Pivotal HD – now with Spark!</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-05-30, 2014-05-30, UTC)</td><td>Today, we’re very proud to announce the release of <a title=\"Spark 1.0.0 Release Notes\" href=\"http://spark.apache.org/releases/spark-release-1-0-0.html\">Apache Spark 1.0</a>. Apache Spark 1.0 is a major milestone for the Spark project that brings both numerous new features and strong API compatibility guarantees. The release is also a huge milestone for the Spark developer community: with more than 110 contributors over the past 4 months, it is Spark’s largest release yet, continuing a trend that has quickly made Spark the most active project in the Hadoop ecosystem.\n<h2>New Features</h2>\nWhat features are we most excited about in Apache Spark 1.0? While there are dozens of new features in the release, we’d like to highlight three.\n\n<b>Spark SQL</b>\n\nThe biggest single addition to Apache Spark 1.0 is Spark SQL, a new module that <a title=\"Spark SQL\" href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">we’ve previously blogged about</a>. Spark SQL offers integrated support for SQL queries alongside existing Spark code, making it seamless to write applications that load structured data (from sources like Hive and Parquet) and run advanced analytics or ETL. Spark SQL will also be the backend for future versions of Shark, providing a simpler, more agile, and optimized execution engine.\n\n<b>Management and Deployment</b>\n\nApache Spark 1.0 also includes major improvements to management and deployment. It adds full support for the Hadoop/YARN security model, running seamlessly in secured Hadoop clusters. It also drastically simplifies job submission, allowing users to easily deploy the same application and on a single machine, a Spark cluster, EC2, Mesos, or YARN. Packaging and deploying a Spark app has never been easier!\n\n<b>Java 8 API</b>\n\nSpark’s Java API has been extended to support <a title=\"Making Spark Easier to Use in Java with Java 8\" href=\"https://databricks.com/blog/2014/04/14/spark-with-java-8.html\">Java 8 lambda expressions</a>, allowing much more concise programming for users of Java 8. Spark still supports Java 6 and 7 through the older API.\n<h2>Community Growth</h2>\nWe are especially excited about the continued growth of the Spark community. Apache Spark 1.0 is the work of more than 110 individuals over the past 4 months, the most to ever contribute to a Spark release. More impressively, the rapid growth in the community has now made Spark the most active project in the Hadoop ecosystem by a wide margin, and one of the <a href=\"https://www.ohloh.net/orgs/apache\">most active projects at Apache</a>. This rapid pace of innovation allows us add features, stability improvements, optimizations and fixes at an unprecedented rate.\n\n<center><a href=\"https://databricks.com/wp-content/uploads/2014/05/chart2.png\"><img class=\"alignnone wp-image-516 size-medium\" style=\"margin-top: 5px; margin-bottom: 5px;\" src=\"https://databricks.com/wp-content/uploads/2014/05/chart2-300x228.png\" alt=\"\" width=\"300\" height=\"228\" /></a></center>After the 1.0 release, Spark will target a quarterly cadence for minor releases (1.1, 1.2, 1.3) and will continue to make maintenance releases as-needed to provide stable versions to users.\n<h2>Databricks’ Commitment to Open Source</h2>\nAt Databricks, we are proud to do all our Apache Spark development in the open -- every new feature and improvement we have made to Spark has been open source. Many of our distribution partners are quickly moving to include 1.0 -- for example, Apache Spark 1.0 will appear in CDH 5.1 in June.\n<h2>More Information</h2>\nThis post has only scratched the surface: Apache Spark 1.0 includes dozens of features not mentioned here, including major improvements to MLLib, GraphX, and Spark Streaming. Head over to the official release notes for a longer write-up. Over the next few weeks, we’ll also be writing more blog posts on select new features here.\n\nFinally, if you would like to learn more about Apache Spark or see how it is being used, join us at the <a href=\"http://spark-summit.org/2014\">Spark Summit</a> on June 30th--July 2nd. With over 50 talks from organizations using Spark and a full day of training, the Summit will be the largest Spark community event yet.</td></tr><tr><td>List(Michael Armbrust, Zongheng Yang)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-06-02, 2014-06-02, UTC)</td><td>With <a title=\"Announcing Spark 1.0\" href=\"https://databricks.com/blog/2014/05/30/announcing-spark-1-0.html\">Apache Spark 1.0</a> out the door, we’d like to give a preview of the next major initiatives in the Spark project. Today, the most active component of Spark is <a title=\"Spark SQL: Manipulating Structured Data Using Spark\" href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Spark SQL</a> - a tightly integrated relational engine that inter-operates with the core Spark API. Spark SQL was released in Spark 1.0, and will provide a lighter weight, agile execution backend for future versions of Shark. In this post, we’d like to highlight some of the ways in which tight integration into Scala and Spark provide us powerful tools to optimize query execution with Spark SQL. This post outlines one of the most exciting features, dynamic code generation, and explains what type of performance boost this feature can offer using queries from a well-known benchmark, TPC-DS. As a baseline, we compare performance against the current Shark release. We’re happy to report that in these cases Spark SQL outperforms Shark, sometimes dramatically. Note that the following tests were run on a development branch of Spark SQL, which includes several new performance features.\n\n<a href=\"https://databricks.com/wp-content/uploads/2014/06/SharkVsSparkSql.png\"><img class=\"alignnone size-full wp-image-546\" src=\"https://databricks.com/wp-content/uploads/2014/06/SharkVsSparkSql.png\" alt=\"SharkVsSparkSql\" width=\"518\" height=\"371\" /></a><em>While this is only a few queries from the TPC-DS benchmark, we plan to release more comprehensive results in the future.</em>\n\nNow that we have seen where things are headed, let's dive into the technical details of how we plan to get there.  This is the first in a series of blog posts about optimizations coming in Spark SQL.\n<h2>Runtime Bytecode Generation</h2>\nOne of the more expensive operations that needs to be performed by a database is the evaluation of expressions in the query. The memory model of the JVM can increase the cost of this evaluation significantly.  To understand why this is the case, let’s look at a concrete example:\n\n<pre>SELECT a + b FROM table</pre>\n\nIn the above query, there is one expression that is going to be evaluated for each row of the table, <tt>a + b</tt>.  Typically, this expression would be represented by an expression tree. <a href=\"https://databricks.com/wp-content/uploads/2014/06/ExpressionTree.png\"><img class=\"wp-image-532 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2014/06/ExpressionTree-300x202.png\" alt=\"ExpressionTree\" width=\"200\" height=\"135\" /></a>\n\nEach node of the tree is represented by an object with an evaluation method that knows how to calculate the result of the expression given an input row.  In this example, when the evaluate method is called on the Add object, it would in turn call evaluate on each of its children and then compute the sum of the returned values. What is wrong with this approach from a performance perspective?  In practice, the performance hit comes from several details of this interpreted execution:\n<ul>\n\t<li><strong>Virtual Function Calls</strong> - Each time evaluate is called on a given expression object,  a <a href=\"http://en.wikipedia.org/wiki/Virtual_function\">virtual function</a> call is made.  These types of function calls can disrupt pipelining in the processor, slowing down execution.  The problem worsens when the expressions are very complex, such as those in the TPC-DS benchmark.</li>\n\t<li><strong>Boxing of Primitive Values</strong> - Since evaluate needs to be able to return many different types of values depending on the expression (Integer, String, Float, etc.) it needs to have a generic return type of Object.  This means that an extra object needs to be allocated for each step of the evaluation.  While modern JVMs have gotten better at cheaply allocating short-lived objects, this cost can really add up.</li>\n\t<li><strong>Cost of Genericity</strong> - These expression trees need to be able to handle many different data types.  However, the actual function that is required to add two integers is different from that that is required to add two doubles, for example.  This genericity means that the evaluation code often requires extra if-statements that branch based on the type of data being processed.</li>\n</ul>\nAltogether, the above issues can lead to a significant increase in query execution time. Fortunately, there is another way!  In a development branch of Spark SQL, we have implemented a version of our expression evaluator that dynamically generates custom bytecode for each query.  While such generation may sound like a difficult task, its actually straightforward to implement using a new feature in Scala 2.10, <a href=\"http://docs.scala-lang.org/overviews/reflection/overview.html\">runtime reflection</a>. At a high level, bytecode generation means that, instead of using an expression tree to evaluate <tt>a + b</tt>, Spark SQL will create new classes at runtime that have custom code similar to the following:\n\n<pre>val a: Int = inputRow.getInt(0)\nval b: Int = inputRow.getInt(1)\nval result: Int = a + b\nresultRow.setInt(0, result)</pre>\n\nCompared to the interpreted evaluation, the generated code works on primitive values (and thus doesn't allocate any objects) and includes no extra function calls.\n\n<h2>Using Quasiquotes</h2>\n\nWhile Spark SQL is not the only system to perform dynamic generation of code for query execution, the use of Scala reflection greatly simplifies the implementation, making it much easier to extend and improve.  Key to this functionality a new feature in Scala, known as <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\">Quasiquotes</a>.  Quasiquotes make it easy to build trees of Scala code at runtime, without having to build complex ASTs by hand. To use a quasiquote, you simple prefix a string in Scala with the letter \"q\".  In doing so, you tell the Scala compiler to treat the contents of the string as code, instead of text.  You can also used $ variables to splice together different fragments of code. For example, the code to generate the above addition expression could be expressed simply as follows:\n\n<pre>def generateCode(e: Expression): Tree = e match {\n  case Attribute(ordinal) =&>\n    q\"inputRow.getInt($ordinal)\"\n  case Add(left, right) =&>\n    q\"\"\"\n      {\n        val leftResult = ${generateCode(left)}\n        val rightResult = ${generateCode(right)}\n        leftResult + rightResult\n      }\n    \"\"\"\n}</pre>\n\nIn practice, the generated code is a little more complicated, as it also needs to handle null values.  If you'd like to see the full version of the generated code for our example expression, it is available <a href=\"https://gist.github.com/marmbrus/9efb31d2b5154aea6652\">here</a>.\n\n<h2>Conclusion</h2>\n\nDynamic code generation is just the tip of the iceberg, and we have a lot more improvements in store, including: improved Parquet integration, the ability to automatically query semi-structured data (such as JSON), and JDBC access to Spark SQL.  Stay tuned for more updates on other optimization we are making to Spark SQL!\n\n<span style=\"color: #777777;\">Finally, if you would like to learn more about Spark SQL or see how it is being used, join us at the </span><a style=\"color: #428bca;\" href=\"http://spark-summit.org/2014\">Spark Summit</a><span style=\"color: #777777;\"> on June 30th–July 2nd. With over 50 talks from organizations using Spark and a full day of training, the Summit will be the largest Spark community event yet.</span></td></tr><tr><td>List(Michael Hiskey (VP at MicroStrategy Inc.))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-04, 2014-06-04, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.microstrategy.com\" target=\"_blank\">MicroStrategy</a> describing why they're excited to have their platform \"Certified on Apache Spark\".</div>\n\n<hr />\n\n<h2>The Need for Speed</h2>\nOver the past few years, we have seen Hadoop emerge as an effective foundation for many organizations’ big data management frameworks, but as the volume and varieties of data increase, speed continues to be a challenge. More and more of our customers are embracing Big Data, and the value of their investment is dependent on (and limited by) how quickly they can take data to action. We’ve been listening to our clients to understand how we can innovate to stay ahead of the curve to help solve these challenges. Apache Spark grabbed our attention because it addresses many of the limitations of Hadoop’s traditional functionality. Plus, Spark is simply impossible to ignore. The active, growing community of developers and enterprises that have adopted Spark has made this project the hottest open source item from the last year. We want to see the open source community continue to flourish and are excited to see Spark open doors for our customers as the application ecosystem grows and reaches wider audiences.\n<h2>MicroStrategy Analytics Platform Certified on Spark</h2>\nAs a leader in enterprise BI and analytics, MicroStrategy is devoted to helping people maximize the value of their data quickly and easily. With this in mind, we are excited to say that MicroStrategy Analytics PlatformTM is one of the first comprehensive enterprise-grade BI and analytics platforms to be officially certified to work with Apache Spark through the Databricks “Certified on Spark” program. On top of this, MicroStrategy has decided to begin a key partnership with Databricks—the creators, advocates, and community-drivers of Spark. This was a no-brainer for us. We share Databricks’ belief that widespread adoption of Spark will be a huge benefit to the community. Also, this partnership will facilitate close collaboration and deep integration of our technologies in the long run. Microstrategy’s certification with Spark is just the first step of a hopefully long, productive relationship. We are thrilled to be an early adopter of Spark and excited for this journey.\n<h2>MicroStrategy/Spark Advantages</h2>\nRunning Microstrategy on Spark can be boiled down to three significant advantages: speed, ease-of-use, and enhanced advanced analytics. By running MicroStrategy on top of Spark, users will be able to accomplish data analytics projects up to 100x faster. The key is Spark’s in-memory processing, which avoids the constant access to the disk required by MapReduce, a costly and time-consuming exercise. Spark has native support of Java, Scala, and Python and requires far less coding for comparable Hadoop jobs. Also, Shark (Hive on Spark) delivers real-time query capabilities for improved ad-hoc data exploration, making Hadoop reasonably interactive for the first time. Spark’s fully unified platform provides a wide array of advanced analytics capabilities as first-class citizens, including streaming data support, machine learning, and graph computation. These features benefit end-to-end application performance and reduce decision latency—in other words, once data is available, users can uncover insights and act, faster. In effect, users can enhance and fine-tune the performance of their big data framework and as a result get more value from their data at every level.\n\nCustomers will benefit enormously from Spark/MicroStrategy integration, and some are already leading the charge: Yahoo! Taiwan is currently using Spark/Microstrategy configurations in a testing environment. This partnership is an extension of MicroStrategy’s commitment to giving our clients the most comprehensive advanced analytics and business intelligence solutions that not only provide fast visual insights on Big Data, but also carries enterprise-class scalability, reliability, and data governance.\n\nMicroStrategy is committed to solving the biggest data challenges. For more information on MicroStrategy Big Data Solutions, visit our website at:\n\n<a href=\"http://www.microstrategy.com/us/platforms/analytics/big-data-solutions\" target=\"_blank\">http://www.microstrategy.com/us/platforms/analytics/big-data-solutions</a>\n\nAlso visit our blog at:\n<a href=\"http://www.microstrategy.com/us/blog\" target=\"_blank\">http://www.microstrategy.com/us/blog</a></td></tr><tr><td>List(Christopher Nguyen (CEO &amp; Co-Founder of Adatao))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-11, 2014-06-11, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.arimo.com\" target=\"_blank\">Arimo</a> describing why and how they bet on Apache Spark.</div>\n\n<hr />\n\nIn early 2012, a group of engineers with background in distributed systems and machine learning came together to form Arimo. We saw a major unsolved problem in the nascent Hadoop ecosystem: it was largely a storage play. Data was sitting passively on HDFS, with very little value being extracted. To be sure, there was MapReduce, Hive, Pig, etc., but value is a strong function of (a) speed of computation, (b) sophistication of logic, and (c) ease of use. While Hadoop ecosystem was being developed well at the substrate, there was enormous opportunities above it left uncaptured.\n\n<strong>On speed:</strong> we had seen data move at-scale and at enormously faster rates in systems like Dremel and PowerDrill at Google. It enabled interactive behavior simply not available to Hadoop users. Without doubt, we knew that interactive speed was necessary, and that in-memory computing was key to the solution. As Cloudera’s Mike Olson has quipped, “We’re lucky to live in an age where there’s a Google. They live about 5 years in the future, and occasionally send messages back to the rest of us.” Google does indeed “live in the future”, in terms of the demands of scale and the value it is extracting from data.\n\n<strong>On sophistication:</strong> for Arimo, the essential difference between “small” and “big” data is whether data is big enough to learn from. For some questions, such as “Does it hurt to hit my head against a brick wall?”, 100 samples suffice. To classify large images, a million samples aren’t enough. We knew this was the second missing key in Big Data: aggregates and descriptives were necessary but insufficient. The Big-Data world needed the sophistication of machine learning. Big Data needed Big Compute. “Predictive” isn’t just another adjective in a long string of X-analytics; it is the quantum change, separating the value of big from small.\n\nThus Arimo was born as a “Big Data/Machine Learning” company. Our exact product features would be driven by customer conversations, but the core thesis was clear. We wanted to bring “Data Intelligence for All”, specifically with the speed and sophistication discussed above.\n\nIf in-memory compute and machine-learning logic were the key to unlocking the value of Big Data, why hadn’t this been solved already in 2012? Because cost/benefit trade-offs matter, in any technology transition. In the chart below, the crossover points happened at different times for different endeavors; it hit critical mass on Wall Street about 2000-2005, at Google c. 2006-2010, and we project for the enterprise world at-large: about now (2013-2015).\n<p style=\"margin-bottom: 10px;\"><img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/MemoryCosts.png\" alt=\"MemoryCosts\" width=\"472\" /></p>\n<em>Fig 1. Cross-over points for transitions to in-memory computing</em>\n\nIf this isn’t clearly happening for your organization or industry yet, relax. It will, soon. Because as the latency and bandwidth trend charts below show, the future increasingly favors RAM.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/06/Latency.png\" alt=\"Latency\" />\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/06/BandwidthTrends.png\" alt=\"BandwidthTrends\" />\n\n<em>Fig 2. The future increasingly favors a shift to RAM</em><!--more-->\n\nAs the Arimo team set out to build this big-compute analytic stack on Hadoop, we wanted our solution to reach all the way to the business users, while also exposing convenient APIs for data engineers and scientists. This required a combination of a great collaborative user interface, solid data-mining and machine-learning logic, backed by a powerful big-compute engine. We did a survey of the in-memory landscape, and found a small number of teams also working in the same direction. But virtually all were either too incremental or too aggressive. Some were developing work-arounds such as caching data between MR iterations, or maintaining a low-level memory cache with no persistent, high-level data structures. Others promoted yet-slow &amp; expensive “virtualized memory” architectures, still too early for prime time.\n\nThen we came across Spark and the Berkeley AMPLab team. Immediately, we knew they had identified the right problem statements, and made the right architectural decisions for the times. Here are some key design choices correctly made for widespread adoption c. 2012:\n<ol>\n \t<li style=\"margin-bottom: 14px;\"><strong>Data model:</strong> Spark was the only architecture that supported the concept of a high-level, persistent distributed in-memory dataset. All “in-memory” systems are not equivalent. Spark’s RDDs exist independently of any given compute step, allowing for not only speedy iterative algorithms, with high-level data sets readily available to each iteration without delay. Equally importantly, they made long-running interactive memory-speed applications possible.</li>\n \t<li style=\"margin-bottom: 14px;\"><strong>Resiliency by recomputation</strong>: with replication being the other option, Spark made the timely choice to prefer recomputation. Memory had gotten cheaper, but not yet cheap enough for replication to be the order of the day, as it is with HDFS disks.</li>\n \t<li style=\"margin-bottom: 14px;\"><strong>General DAG support</strong>: while it was possible to build dedicated SQL query engines to overcome Hadoop MapReduce’s limitations (and others did choose this path), Spark’s general DAG model meant we could build arbitrary algorithms and applications on it.</li>\n</ol>\nWe were ecstatic. Spark represented years of R&amp;D we didn’t have to spend building an engine before building sophisticated, user-facing applications. When we made the decision to support the AMPLab Spark effort, there were only 1 or 2 others that had made similar commitments. We were seriously betting the company on Spark.\n\nBut thanks to Spark, we were able to move ahead quietly and quickly on Arimo <a href=\"http://adatao.com/pinsights.html\" target=\"_blank\">pInsights</a> and <a href=\"http://arimo.com/panalytics.html\" target=\"_blank\">pAnalytics</a>, iterating on customer feedback while passing our inputs and market data along to the Spark team. We promoted Spark’s goodness in every relevant conversation. By late summer 2013, <a href=\"http://www.databricks.com/\" target=\"_blank\">Databricks</a> was about to be born, further increasing our confidence on the Spark-on-Hadoop ecosystem. There was now going to be an official, commercial entity with an existence predicated on developing the growth of the ecosystem and maintaining its health. And the team at Databricks is doing an excellent job at that stewardship.\n\nToday, Arimo is one of the first applications to be <em><a href=\"https://databricks.com/certification\" target=\"_blank\">Certified on Spark</a></em>. We’re seeing remarkable enterprise adoption speeds for Arimo-on-Spark. The most sophisticated customers tend to be companies that have already deployed Hadoop, who are all too familiar with the failed promises of Big Data. We see immediate excitement in customers the moment they see the Arimo solution: a user-facing analytics application that is interactive, easy-to-use, supports both basic analytics and machine learning, and is actually running in seconds of real time over large Hadoop datasets. Finally, users are truly able to extract data intelligence from data storage. Value creation is no longer just about Big Data. It’s about Big Compute, and Spark has delivered that capability for us.\n\nSpark has made it as a top-level Apache project, going from incubation to graduation in record time. It is also one of Apache’s most active projects with hundreds of contributors. This is because of its superior architecture and timeliness of engineering choices, as discussed above. With that plus appropriate care and feeding, Apache Spark will have a bright future even as it evolves and adapts to changing technology and business drivers.\n\n<hr />\n\n<em>Christopher Nguyen is co-founder and CEO of Arimo. He is a former engineering director of Google Apps, a Stanford PhD who co-founded two enterprise startups with successful exits, and a professor and co-founder of the Computer Engineering program at HKUST. He graduated from U.C. Berkeley summa cum laude. Christopher has extensive experience building technology companies that solve enterprise business challenges. Come hear his talk on <a href=\"https://spark-summit.org/2014/talk/distributed-dataframe-ddf-on-apache-spark-simplifying-big-data-for-the-rest-of-us\" target=\"_blank\">Distributed DataFrames (DDF) on Spark: Simplifying Big Data for the Rest of Us</a> at the <a href=\"http://www.spark-summit.org/\" target=\"_blank\">Spark Summit 2014</a>.</em></td></tr><tr><td>List(Databricks Press Office)</td><td>List(Company Blog, Events)</td><td>List(2014-06-12, 2014-06-12, UTC)</td><td><ul>\n \t<li>Three-Day Event in San Francisco Invites Attendees to Gain Insights from the Leading Organizations in Big Data</li>\n \t<li>Keynote Speakers Include Executives from Databricks, Cloudera, MapR, DataStax, Jawbone and More</li>\n \t<li>Spark Summit Features Different Tracks for Applications, Development, Data Science and Research</li>\n</ul>\n&nbsp;\n\nBERKELEY, Calif.--(BUSINESS WIRE)-- Databricks and the sponsors of Spark Summit 2014 today announced the full agenda for the summit, including a host of exciting keynotes and community talks. The event will be held June 30–July 2, 2014, at The Westin St. Francis in San Francisco.\n\nSpark Summit 2014 arrives at an exciting time for the Apache Spark platform, which has become the most active open source project in the Hadoop ecosystem with more than 200 contributors in the past year. Now available in all major Hadoop distributions, Spark has fostered a fast-growing community on the strength of its technical capabilities, which make big data analysis simpler with a unified platform, easy-to-use APIs in Java, Scala and Python, and drastically faster in-memory computation.\n\n“The Spark Summit presents an exciting opportunity to bring together the fast-growing Spark community and collaborate to push this platform forward,” said Databricks CEO Ion Stoica. “We look forward to discussing Spark’s impact on the big data landscape and how this rapidly adopted technology will play an instrumental role in shaping the future of big data analysis.”\n\nIn its second year, the Spark Summit will gather enterprise users, developers and data scientists to examine best practices for development and learn how to use the Spark stack in a variety of applications across multiple industries. Attendees can explore various tracks on topics such as applications, development, data science and research. Keynotes will be delivered by Stoica, Databricks Chief Technology Officer Matei Zaharia, Cloudera Chief Strategy Officer Mike Olson, MapR Chief Technology Officer M.C. Srivas and others from DataStax, Jawbone, Howard Hughes Medical Institute and UC Berkeley’s AMPLab.\n\nAdditionally, Databricks will be hosting an introductory level Spark training on the final day of the three-day event, and the Application Spotlight segment will also showcase promising applications that have been “Certified on Spark.”\n\nOther Spark Summit sponsors to date include IBM, UC Berkeley’s AMPLab, Cloudera, Pivotal, Sharethrough, Guavus, Stratio, Red Hat, Ooyala, MapR, DataStax, ClearStory Data, Shopify, SanDisk, Hortonworks, NTT DATA, Yahoo, Platfora, Atigeo and Brain Corporation.\n\nTo view the full agenda of keynotes and 50+ talks, click <a title=\"here\" href=\"http://spark-summit.org/2014/agenda\">here</a>.\n\nTo purchase tickets to the Spark Summit, visit:\n<a title=\"http://spark-summit.org/register\" href=\"http://spark-summit.org/register\">http://spark-summit.org/register</a>\n\nTo register for a live stream of Spark Summit, click <a title=\"here\" href=\"https://spark-summit.org/2014/live-stream\">here</a>.\n\nAbout Databricks\n\nDatabricks (<a href=\"https://databricks.com\">databricks.com</a>) was founded by the creators of Apache Spark, and are using cutting-edge technology based on years of research to build next-generation software for analyzing and extracting value from Big Data. They believe Big Data is a tremendous opportunity that is still largely untapped, and are working to revolutionize what enterprises can do with it. They are venture-backed by Andreessen Horowitz.\n\n&nbsp;\n\nDatabricks Public Relations\nAlex Koritz, 801-461-9795\nalex@methodcommunications.com\nor\nJoshua Heath, 801-461-9794\njoshua@methodcommunications.com</td></tr><tr><td>List(Dean Wampler (Typesafe))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-13, 2014-06-13, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.lightbend.com\" target=\"_blank\">Lightbend</a> after having their Lightbend Activator Apache Spark templates be \"Certified on Apache Spark\".</div>\n\n<hr />\n\n<h2>Apache Spark and the Lightbend Reactive Platform: A Match Made in Heaven</h2>\nWhen I started working with Hadoop several years ago, it was frustrating to find that writing Hadoop jobs was hard to do. If your problem fits a query model, then <a title=\"Hive\" href=\"http://hive.apache.org\" target=\"_blank\">Hive</a> provides a SQL-based scripting tool. For many common dataflow problems, <a href=\"http://pig.apache.org\" target=\"_blank\">Pig</a> provides useful abstractions, but it isn't a full-fledged, \"Turing-complete\" language. Otherwise, you had to use the low-level <a href=\"http://wiki.apache.org/hadoop/MapReduce\" target=\"_blank\">Hadoop MapReduce</a> API. Some third-party APIs exist that wrap the MapReduce API, such as <a href=\"http://cascading.org\" target=\"_blank\">Cascading</a> and <a href=\"https://github.com/twitter/scalding\" target=\"_blank\">Scalding</a>, but they couldn't fix MapReduce's performance problems.\n<h2>Spark - The New Big Data Compute Engine</h2>\nBut interest in an alternative, <a href=\"http://spark.apache.org\" target=\"_blank\">Apache Spark</a>, was growing. Now, Spark has emerged as the next-generation platform for writing Big Data applications for Hadoop and Mesos clusters.\n\nSpark is replacing the venerable Hadoop MapReduce for several reasons:\n<h4>Performance</h4>\nSpark's <a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\" target=\"_blank\">Resilient Distributed Datasets</a> (RDDs), which are fault-tolerant, distributed collections of data that can be manipulated in parallel. RDDs exploit intelligent, in-memory caching of data that avoids unnecessary round trips to disk, writes followed by reads, which are common in non-trivial MapReduce jobs where map and reduce steps are sequenced together.\n<h4>Natural Data Processing Idioms</h4>\nSpark provides a powerful set of composable building blocks for writing concise, yet powerful queries and dataflows. While the MapReduce API can be used to write a wide-range of computations, translating many algorithms to the API can be very difficult, requiring special expertise. In contrast, the concise Scala, Java, and Python APIs provided by Spark make developers highly productive.\n<h4>Streaming vs. Batch-mode Operations</h4>\nMapReduce only supports batch-mode operations. Increasingly, data teams need more \"real-time\" processing of event streams. Rather than turning to yet another tool for this purpose, Spark lets you writes streaming and batch-mode applications with very similar logic and APIs.\n<h2>What Makes Spark so Successful?</h2>\nPart of Spark's success is due to the foundation it is built upon, components of the <a href=\"http://lightbend.com/platform\" target=\"_blank\">Lightbend Reactive Platform</a>. First, there's <a href=\"http://scala-lang.org\" target=\"_blank\">Scala</a>, the flexible, object-functional language for the JVM. People often ask Matei Zaharia, the creator of Spark and the co-founder of Databricks, why he chose Scala. Here is a <a href=\"http://apache-spark-user-list.1001560.n3.nabble.com/Why-Scala-td6536.html\" target=\"_blank\">recent answer</a> he gave to the question:\n\nQuite a few people ask this question and the answer is pretty simple. When we started Spark, we had two goals — we wanted to work with the Hadoop ecosystem, which is JVM-based, and we wanted a concise programming interface similar to Microsoft’s <a href=\"http://research.microsoft.com/en-us/projects/dryadlinq/\" target=\"_blank\">DryadLINQ</a> (the first language-integrated big data framework I know of, that begat things like <a href=\"http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf\" target=\"_blank\">FlumeJava</a> and <a href=\"http://crunch.apache.org/\" target=\"_blank\">Crunch</a>). On the JVM, the only language that would offer that kind of API was Scala, due to its ability to capture functions and ship them across the network. Scala’s static typing also made it much easier to control performance compared to, say, Jython or Groovy.\n\nThe second Lightbend component in Spark's foundation is <a href=\"http://akka.io\" target=\"_blank\">Akka</a>, a toolkit and runtime for building highly-concurrent, distributed, and fault tolerant event-driven applications on the JVM.\n\nSpark exploits Akka's distributed, fine-grained, flexible, and dynamic <a href=\"http://akka.io/#actors\" target=\"_blank\">Actor model</a> to build resilient, distributed components for managing and processing data.\n<h2>Lightbend and Databricks, Working Together</h2>\nThe combination of Apache Spark and the Lightbend Reactive Platform, including Scala, Akka, Play, and Slick, gives Enterprise developers a comprehensive suite of tools for building Certified on Spark applications with minimal effort that are highly scalable and resilient.\n\nLightbend will continue to build tools that help make Spark great and Databricks successful. We'll also work to make the developer experience seamless between our tools.\n\nFor starters, I encourage you to check out our growing <a href=\"http://lightbend.com/activator\" target=\"_blank\">Lightbend Activator</a> templates for Spark, especially my introductory <a href=\"http://lightbend.com/activator/template/spark-workshop\" target=\"_blank\">Spark Workshop</a>, which is our first Certified on Spark application.</td></tr><tr><td>List(Hari Kodakalla (EVP at Apervi Inc.))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-23, 2014-06-23, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.apervi.com\" target=\"_blank\">Apervi</a> after having their Conflux Director™ application be \"Certified on Apache Spark\".</div>\n\n<hr />\n\n<h2>Big Data on Steroids with Apache Spark</h2>\nAs big data takes center stage in the new data explosion, Hadoop has emerged as one the leading technologies addressing the challenges in the space. As the data processing needs of enterprises are growing newer technologies like Apache Spark have emerged as significant options that consistently offer expanded capabilities for the big data space. As these enterprise needs are met, so is the increased appetite for faster processing, low latency requirements for high velocity data and an iterative demand for processing where leading technologies like Hadoop fall short of expectations or at times seem cumbersome to implement due to its inherent design.\n\nDelivering on this growing need of enterprises is where Spark plays a very crucial role and is emerging as the platform of choice. Why? Apache Spark offers built in support for in-memory processing, support for HDFS and the ability to work with data using SQL unlike any other open source big data technology and much more, all along offering superior performance gains over traditional choices. Spark thus supercharges the big data processing landscape and is truly a powerful complementary technology stack that reinforces the big data platform for enterprises, offering an expanded ability to do far more than Hadoop alone could deliver upon, thus the term Big Data on steroids with Spark.\n<h2>Apervi Conflux Director™ &amp; Databricks</h2>\nApervi, a leading innovator in the space of data engineering for big data, is completely focused on making it easy for users to leverage the benefits of big data technologies without the complexity associated with the respective technologies (namely Hadoop, Storm, &amp; Spark). In the same spirit Apervi identified Spark very early on as one of the strong offerings capable of catering to the growing needs of enterprises to process diverse forms of data beyond the limitations of MapReduce for superior performance. Apervi started work to build support for Spark within its offering the Conflux Director™, a unified orchestration platform for big data, to quickly expand what users can do with their data without being bound by the processing paradigm limitations, be it batch / real-time. By offering support for Spark standard within Conflux Director™, Apervi is able to deliver the power of Spark readily to its users to build high performance data engineering workflows in an intuitive fashion consistent with its support for other Big Data technologies, all without a steep learning curve.\n<h2>Spark and Conflux for Telecom</h2>\nSpark support in Conflux has enabled one of our customers in the telecom industry to quickly build and test the viability of a real-time promotion targeting program of wireless customers based on location and status in a relatively short period of time. Combining Spark with Conflux for ETL and stream processing has enabled this customer to compress processing windows drastically, realizing swift gains, and conduct rapid prototyping to A/B test promotion effectiveness efficiently.\n<h2>Conflux “Certified on Spark”</h2>\nAs we progressed in our journey of building support for Spark, the introduction of the Databricks “Certified on Spark “ program got us really excited. It allowed us to both showcase our close integration with Spark and highlight the need to promote Spark to the user community in a systematic fashion. We are proud and excited to be participating in the “Certified on Spark “ program by Databricks because we feel that working with Databricks will help the Spark ecosystem grow, thus encouraging broader adoption of such a fantastic technology to meet the diverse needs of our customers to tame the challenges of big data.\n\nSo, it is with both pride — as well as thanks to the team at Databricks — that we announce that Conflux Director™ is now “Certified on Spark”.\n\nApervi will continue to build tools that help make Spark easy to use and support Databricks in its mission. We’ll strive to make the developer experience seamless between our tools and the Spark technology stack thus delivering on the broader promise of enabling enterprises to derive value from Big Data easier and faster.</td></tr><tr><td>List(Bill Kehoe (Big Data Architect at Qlik))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-24, 2014-06-24, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.qlik.com\" target=\"_blank\">Qlik</a> describing how Apache Spark enables the full power of QlikView, recently Certified on Apache Spark, and its Associative Experience feature over the entire HDFS data set.</div>\n\n<hr />\n\n<h2>The Power of Qlik</h2>\nQlik provides software and services that help make understanding data a natural part of how people make decisions. Our product, QlikView, is the leading Business Discovery platform that incorporates a unique, associative experience that empowers business users to follow their own path to formulate and answer questions that lead to better decisions. Traditional, query-based BI tools force users thru pre-defined navigation paths which limit the kinds of questions that can be answered and require costly and time consuming revisions to address evolving business needs. In contrast, when a user selects data items using QlikView, all the fields and charts are immediately updated to reflect the relationships between the selected items and the other data items in the business model.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Associative.png\" width=\"472\" />\n\nUsing QlikView, it is always apparent what data is associated since the selected items are presented with a green background, all related values have a white background and unrelated items are shown with a gray background.\n<h2>Expanding to larger data sets</h2>\nRecognizing that many customers need the power of QlikView to analyze volumes of data too large to load into the QlikView in-memory engine, Qlik expanded its engine by adding Direct Discovery, a hybrid approach that combines both in-memory data with dynamic access to data residing in external, big data sources. With this hybrid model, the QlikView engine dynamically formulates and submits queries for external data whenever a user selects in-memory data values that are associated with external data. With Direct Discovery, users are taking full advantage of the QlikView Associative Experience without having to first ingest the raw data from big data sources into QlikView.\n\nNaturally, the ability to run Direct Discovery applications on Hadoop was one of the very first use cases Qlik anticipated so we’ve worked closely with our Hadoop partners to ensure the fastest and easiest means to access HDFS data. We knew we had existing customers using MapReduce to aggregate HDFS data for loading into QlikView and that these same customers would not only want faster load times but also the option of using Direct Discovery on their pre-aggregated data.\n<h2>Leveraging Apache Spark</h2>\nSince the advent of Direct Discovery, we’ve followed the Apache Spark project noting its obvious performance benefit of using RAM for inter-stage caching rather than the much higher latency, disk-based approach of traditional MapReduce. We’ve been particularly impressed by the way Spark provides a high performance platform not just for SQL but other critical big data needs such as iterative machine learning algorithms and streaming.\n\nQlik is thrilled to participate in the Databricks “Certified on Spark” program because we know that Spark is accelerating our customers’ ability to deliver high performance applications that operate on the full spectrum of their HDFS data. For starters, using Shark, QlikView users are now able to use Direct Discovery to utilize the full power of the QlikView Associative Experience over their entire HDFS data set. Using Spark’s RDD API, customers are able to build very succinct, pipeline-style flows that dramatically reduce the time needed to execute their HDFS data reload tasks for batch loaded data.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Shark.png\" width=\"472\" />\n\nQlik understands that fast, easy access to increasingly large and diverse sources of data is one of the most critical enablers for deriving maximal business value from data. Clearly Spark significantly advances the accessibility of big data and, by certifying QlikView on Apache Spark, Qlik delivers this enhanced data accessibility to its customers while encouraging the widespread adoption of this critical big data platform.</td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(2014-06-26, 2014-06-26, UTC)</td><td><em>Certified distributions maintain compatibility with open source Apache Spark distribution and thus support the growing ecosystem of Apache Spark applications</em>\n\n<hr />\n\n<strong>BERKELEY, Calif. -- June 26, 2014 --</strong> Databricks, the company founded by the creators of Apache Spark, the next generation Big Data engine, today announced the <a href=\"https://databricks.com/spark/certification/certified-spark-distribution\" target=\"_blank\">“Certified Spark Distribution” </a>program for vendors with a commercial Spark distribution. Certification indicates that the vendor’s Spark distribution is compatible with the open source Apache Spark distribution, enabling “Certified on Spark” applications - certified to work with Apache Spark - to run on the vendor’s Spark distribution out-of-the-box.\n\n“One of Databricks’ goals is to ensure users have a fantastic experience. Our belief is that having the community work together to maintain compatibility and therefore facilitate a vibrant application ecosystem is crucial to this vision,” said Ion Stoica, Databricks CEO. “We first launched the ‘Certified on Spark’ program to help build a robust ecosystem of innovative applications on top of Apache Spark. The ‘Certified Spark Distribution’ program is the other half of the equation, recognizing vendors that are committed to providing a home for these applications to allow the ecosystem to flourish.”\n\nIn keeping with the open source nature of Spark, the certification process is fully transparent with open-source tests, lightweight, and 100% free - a mirror image of the “Certified on Spark” process for Spark applications. Vendors fill out a short questionnaire and then simply execute a set of open-source tests - developed and maintained by the community and used to test each release of Apache Spark - against their build of Spark to demonstrate compatibility.\n\n“Certification shouldn’t be used as a tool for lock-in: Certified Spark Distributions are not required to ship all the bits of Apache Spark, or be open source, or prevented from innovating significantly within and around Spark,” said Arsalan Tavakoli-Shiraji, Business Development Lead at Databricks. “They simply need to maintain compatibility with Apache Spark to provide support for the application ecosystem.”\n\nAs part of the certification program launch, five vendors have completed the certification process: <a title=\"DataStax\" href=\"http://www.datastax.com\" target=\"_blank\">DataStax</a>, <a title=\"Hortonworks\" href=\"http://www.hortonworks.com\" target=\"_blank\">Hortonworks</a>, <a title=\"IBM\" href=\"http://www.ibm.com\" target=\"_blank\">IBM</a>, <a title=\"Oracle\" href=\"http://www.oracle.com\" target=\"_blank\">Oracle</a>, and <a title=\"Pivotal\" href=\"http://www.gopivotal.com\" target=\"_blank\">Pivotal</a> - industry leaders that have recognized and embraced the power of Spark when integrated with their respective platforms. Each of these vendors put their distributions through the certification process, which included a host of integration tests to ensure full compatibility with the latest Apache Spark release.\n\n“One of the big risks faced by open source projects is fragmentation among distributors. Fragmentation is bad for both users and application developers, and ultimately for the growth of the project,” said Matei Zaharia, Databricks CTO and VP of the Spark project at Apache. “We are delighted that these partners - along with others in the certification pipeline - share our vision of an undivided Spark platform based directly around Apache, and will ensure that all applications built on Apache Spark run on their distributions.”\n\nVendors interested in certifying their Spark distribution should visit <a title=\"databricks.com\" href=\"http://www.databricks.com\" target=\"_blank\">www.databricks.com</a> and select ‘Apply for Certification’. Enterprise users can also visit the Databricks site regularly to see the latest set of certified distributions and applications, and read “spotlight” blog articles that provide deep-dives on the Spark ecosystem by newly certified vendors.\n\nAll the inaugural members will be on hand at the upcoming <a title=\"Spark Summit\" href=\"http://www.spark-summit.org\" target=\"_blank\">Spark Summit</a> from June 30th to July 2nd in San Francisco to provide greater information on the role of Spark in helping better serve their customers. Additionally, there will be an <a title=\"Application Spotlight\" href=\"http://www.spark-summit.org/2014/agenda\" target=\"_blank\">“Application Spotlight”</a> segment that will highlight innovative “Certified on Spark” applications.\n<h2>Supporting Quotes:</h2>\n\"DataStax is strongly committed to making Cassandra and Spark the best combination for today's online applications,\" said Robin Schumacher, VP of products at DataStax, \"We have demonstrated that commitment with the integration work we have contributed back to both open source communities as well as the certified versions of Spark and Cassandra we provide in DataStax Enterprise for production environments.\"\n\n“We support the fact that Apache Spark project provides enterprises with an additional processing engine in Hadoop to execute in-memory algorithms for advanced analytics,” said John Kreisa, vice president of strategic marketing at Hortonworks. “We applaud Databricks’ vision to ensure Spark is fully integrated on YARN, which enterprises have adopted as the data OS for Hadoop.”\n\n\"IBM has a long history of supporting open source projects and the communities that develop around them,\" said Anjul Bhambhri, vice president, IBM Big Data. \"Our work on the Spark Distribution Certification program extends IBM's commitment to delivering innovations on Hadoop to help our clients manage massive amounts of data from multiple sources and perform interactive analytics through applications that consumers and businesses are building.\"\n\n\"Pivotal's open source credentials are quite extensive - Apache-compatible Hadoop, MADLib, RabbitMQ, CloudFoundry - and now we've added Spark to that set,\" said Sarabjeet Chugh, Head of Hadoop Product Management at Pivotal. \"Additionally, we recognize the importance of a unified community to enable the ecosystem to grow and so are thrilled to back this effort\"\n<h2>About Databricks</h2>\nDatabricks was founded by the creators of Apache Spark, who have been working for the past six years on cutting-edge systems to analyze and process Big Data. They believe that Big Data is a tremendous opportunity that is still largely untapped, and are actively working to revolutionize what enterprises can do with it. Databricks is venture-backed by Andreessen Horowitz. For more information, visit <a title=\"http://www.databricks.com\" href=\"http://www.databricks.com\" target=\"_blank\">http://www.databricks.com</a></td></tr><tr><td>List(Costin Leau (Engineer at Elasticsearch))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-28, 2014-06-28, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.elasticsearch.com\" target=\"_blank\">Elasticsearch</a> announcing Elasticsearch is now \"Certified on Apache Spark\", the first step in a collaboration to provide tighter integration between Elasticsearch and Spark.</div>\n\n<hr />\n\n<h2>Elasticsearch Now “Certified on Spark”</h2>\nHelping businesses get insights out of their data, fast, is core to the mission of Elasticsearch. Being able to live wherever a business stores their data is obviously critical to that mission, and Hadoop is one of the leaders in providing a way for businesses to store massive amounts of data at scale. Over the course of the past year, we have been working hard to bring the power of our real-time search and analytics engine to the Hadoop ecosystem. Our Hadoop connector, Elasticsearch for Apache Hadoop, is compatible with the top three Hadoop distributions – Cloudera, Hortonworks and MapR – and today has achieved another exciting milestone: Spark certification.\n<h2>Elasticsearch + Spark = Rich Search, Immediate Insights</h2>\nSpark is rapidly emerging as a popular processing and analysis tool for Hadoop-like and other data stores. We continue to see it in many of our customers' Hadoop distributions and beyond, and have been working together with Databricks as well as our respective open source communities to bring better connectivity between the two technologies. The combination of Elasticsearch with Spark adds the capabilities of a full-blown search engine that enhances data discovery and exploration - whether it be in a live, customer-facing environment, or behind the scenes for internal analysis - to Spark's unified processing engine. Through Elasticsearch for Apache Hadoop Map/Reduce support, Spark applications can interact with Elasticsearch just as they would with an HDFS resource, allowing them to index and analyze data transparently, in real-time. Our data visualization tool, Kibana, can also be used to explore massive amounts of data in Elasticsearch through easy-to-generate pie charts, bar graphs, scatter plots, histograms and more.\n\nBusinesses continue to adopt Elasticsearch to help them get to the last mile of their Hadoop deployments by providing the ability to ask, iterate and extract actionable insights from their data. A lot of them are in industries like healthcare, finance and telecommunications and have extremely large and sensitive amounts of data they need to mine. Elasticsearch for Apache Hadoop lets them access data, like log files, in minutes instead of hours, so they can detect fraud, identify service issues and analyze customer behavior, letting them come to resolutions faster and giving their rockstar developers the tools they need to directly impact the bottom line of their business.\n\nWe couldn’t be more thrilled to be officially <a href=\"https://databricks.com/certification\" target=\"_blank\">“Certified on Spark”</a>; our Hadoop connector is the first step in our roadmap to make the two more natively integrated, bringing businesses even more advanced search and analytics capabilities to their data.\n<h2>Check Us Out</h2>\nIf you’re going to <a href=\"http://www.spark-summit.org\" target=\"_blank\">Spark Summit</a>, Holden Karau from Databricks will be showing how to streamline search indexing with Elasticsearch and Spark in <a href=\"https://spark-summit.org/2014/talk/streamlining-search-indexing-using-elastic-search-and-spark\" target=\"_blank\">this session</a> on Monday, June 30 at 3:00pm.\n\nWe are also holding a webinar about how Elasticsearch can used for real-time insights on your Hadoop and Spark deployments on Wednesday, August 20th - you can register for that <a href=\"http://www.elasticsearch.org/webinars/elasticsearch-and-apache-hadoop/\" target=\"_blank\">here</a>.\n\nAnd last but not least, if you’d like to get started, download Elasticsearch for Apache Hadoop <a href=\"http://www.elasticsearch.org/overview/hadoop/download/\" target=\"_blank\">here</a> and <a href=\"http://www.elasticsearch.org/community/\" target=\"_blank\">let us know what you think!</a></td></tr><tr><td>List(Jake Cornelius (SVP of Product Management at Pentaho))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-30, 2014-06-30, UTC)</td><td>[sidenote]This post is guest authored by our friends at <a href=\"http://www.pentaho.com\" target=\"_blank\">Pentaho</a> after having their data integration and analytics platform <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a>[/sidenote]\n\n<hr />\n\nOne of Pentaho’s great passions is to empower organizations to take advantage of amazing innovations in <a href=\"http://www.pentaho.com/what-is-big-data\" target=\"_blank\">Big Data</a> to solve new challenges using the existing skill sets they have in their organizations today.  Our Pentaho Labs prototyping and innovation efforts around natively integrating data engineering and analytics with Big Data platforms like <a href=\"http://www.pentaho.com/what-is-hadoop\" target=\"_blank\">Hadoop</a> and <a href=\"http://www.pentaho.com/storm\" target=\"_blank\">Storm</a> have already led dozens of customers to deploy next-generation Big Data solutions. Examples of these solutions include <a href=\"http://www.pentaho.com/Optimize-the-Data-Warehouse\" target=\"_blank\">optimizing data warehousing</a> architectures, leveraging <a href=\"http://www.pentaho.com/solutions/hadoop\" target=\"_blank\">Hadoop</a> as a cost effective <a href=\"http://www.pentaho.com/Streamlined-Data-Refinery\" target=\"_blank\">data refinery</a>, and performing advanced analytics on diverse data sources to achieve a broader <a href=\"http://www.pentaho.com/Customer-360-Degree-View\" target=\"_blank\">360-degree view of customers</a>.\n\nNot since the early days of Hadoop have we seen so much excitement around a new Big Data technology as we see right now with <a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a>.  <a href=\"http://www.pentaho.com/what-is-apache-spark\" target=\"_blank\">Spark</a> is a Hadoop-compatible computing system that makes big data analysis drastically faster, through in-memory computation, and simpler to write, through easy APIs in Java, Scala and Python.  With the second annual <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a> taking place this week in San Francisco, I wanted to share some of the early proof-of-concept work Pentaho Labs and our partners over at <a href=\"https://databricks.com/\">Databricks</a> are collaborating on to integrate Pentaho and Spark for delivering high performance, Big Data Analytics solutions.\n\n<h2>Big Data Integration on Spark</h2>\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Flow-Picture.png\" width=\"472\" />\n\nAt the core of Pentaho Data Integration (PDI) is a portable ‘data machine’ for ETL which today can be deployed as a stand-alone Pentaho cluster or inside your Hadoop cluster though MapReduce and YARN.  The Pentaho Labs team is now taking this same concept and working on the ability to deploy inside Spark for even faster Big Data ETL processing.  The potential benefit for ETL designers is the ability to design, test and tune ETL jobs in PDI’s easy-to-use graphical design environment, and then run them at scale on Spark.  This dramatically lowers the skill sets required, increases productivity, and reduces maintenance costs when to taking advantage of Spark for Big Data Integration.\n\n<h2>Advanced Analytics on Spark</h2>\n\nLast year Pentaho Labs introduced a distributed version of Weka, Pentaho’s machine learning and data mining platform. The goal was to develop a platform-independent approach to using Weka with very large data sets by taking advantage of distributed environments like Hadoop and Spark. Our first pilot implementation proved out this architecture by enabling <a href=\"http://markahall.blogspot.com/2013/10/weka-and-hadoop-part-1.html\" target=\"_blank\">parallel, in-cluster model training with Hadoop</a>.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Advanced-Analytics.png\" width=\"472\" />\n\nWe are now working on a similar level of integration with Spark that includes data profiling and evaluating classification and regression algorithms in Spark.  The early feedback from Pentaho Labs confirms that developing solutions on Spark is faster and easier than with MapReduce. In just a couple weeks of development, we have demonstrated a proof-of-concept to perform in-cluster Canopy clustering and are very close to having k-means++ working in Spark as well!\n\n<h2>Next up: Exploring Data Science Pack Integration with MLlib</h2>\n\n<a href=\"https://spark.apache.org/mllib/\" target=\"_blank\">MLlib</a> is already one of the most popular technologies for performing advanced analytics on Big Data.  By integrating Pentaho Data Integration with Spark and MLlib, Data Scientists will benefit by having an easy-to-use environment (PDI) to prepare data for use in MLlib-based solutions.  Furthermore, this integration will make it easier for IT to operationalize the work of the Data Science team by orchestrating the entire end-to-end flow from data acquisition, to data preparation, to execution of MLlib-based jobs to sharing the results, all in one simple PDI Job flow.  To get a sense for how this integration might work, I encourage you to look at a similar integration with R we recently launched as part of the <a href=\"http://www.pentaho.com/press-release/pentaho-data-science-pack-operationalizes-use-r-and-weka\" target=\"_blank\">Data Science Pack</a> for Pentaho Business Analytics 5.1.\n\n<h3>Experiment Today with Pentaho and Spark!</h3>\n\nYou can experiment with Pentaho and Spark today for both ETL and Reporting.  In conjunction with our partners at <a href=\"https://databricks.com/\">Databricks</a>, we recently developed prototypes for the following use cases combining Pentaho and Spark*:\n\n<ul>\n \t<li>Reading data from Spark as part of an ETL workflow by using Pentaho Data Integration’s Table Input step with <a href=\"http://shark.cs.berkeley.edu/\" target=\"_blank\">Apache Shark</a> (Hive SQL layer runs on Spark)</li>\n \t<li>Reporting on Spark data using Pentaho Reporting against Apache Shark</li>\n</ul>\n\nWe are excited about this first step in what we both hope to be a collaborative journey towards deeper integration.\n\nJake Cornelius Sr. Vice President, Product Management Pentaho\n\n<em>* Note that these Databricks integrations constitute a proof-of-concept and are not currently supported for Pentaho customers.</em></td></tr><tr><td>List(SriSatish Ambati (CEO of 0xData))</td><td>List(Company Blog, Partners)</td><td>List(2014-06-30, 2014-06-30, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.0xdata.com\" target=\"_blank\">0xData</a> discussing the release of Sparkling Water - the integration of their H20 offering with the Apache Spark platform.</div>\n\n<hr />\n\n<h3>H20 – The Killer-App on Apache Spark</h3>\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Spark-+-H20.png\" width=\"472\" />\n\nIn-memory big data has come of age. The Apache Spark platform, with its elegant API, provides a unified platform for building data pipelines. H2O has focused on scalable machine learning as the API for big data applications. Spark + H2O combines the capabilities of H2O with the Spark platform – converging the aspirations of data science and developer communities. H2O is the Killer-Application for Spark.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/H20-the-Killer-App.png\" width=\"472\" />\n<h3>Backdrop</h3>\nOver the past few years, we watched Matei and the team behind Spark build a thriving open-source movement and a great development platform optimized for in-memory big data, Spark. At the same time, H2O built a great open source product with a growing customer base focused on scalable machine learning and interactive data science. These past couple of months the Spark and H2O teams started brainstorming on how to best combine H2O's Machine Learning capabilities with the power of the Spark platform. The result: <strong>Sparkling Water</strong>.\n<h3>Sparkling Water</h3>\nUsers can in a single invocation and process, get the best of Spark - its elegant APIs, RDDs, multi-tenant Context and H2O's speed, columnar-compression and fully-featured Machine Learning and Deep-Learning algorithms.\n\nOne of the primary draws for Spark is its unified nature, enabling end-to-end building of API’s within a single system. This collaboration is designed to seamlessly enable H20’s advanced capabilities to be part of that data pipeline. The first step in this journey is enabling in-memory sharing through Tachyon and RDDs. The roadmap includes deeper integration where H2O’s columnar-compressed capabilities can be natively leveraged through ‘H2ORDD’.\n\nToday, data gets parsed and exchanged between Spark and H2O via Tachyon. Users can interactively query big data both via SQL and ML from within the same context.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/Tachyon-integration.png\" width=\"472\" />\n\nSparkling Water enables use of H2O's Deep Learning and Advanced Algorithms for Spark's user community. H2O as the killer-application provides a robust machine learning engine and API for the Spark Platform. This will further empower application developers on Spark to build intelligent and smarter applications.\n\n<img class=\"aligncenter size-full wp-image-62\" src=\"https://databricks.com/wp-content/uploads/2014/06/H20RDD.png\" width=\"200\" />\n<h3>MLlib and H2O: The Triumph of Open Source!</h3>\nMLlib is a library of efficient implementations of popular algorithms directly built using Spark. We believe that enterprise customers should have the choice to select the best tool for meeting their needs in the context of Spark. Over time, H2O will accelerate the community’s efforts towards production ready scalable machine learning. Fast fully featured algorithms in H2O will add to growing open source efforts in R, MLlib, Mahout and others, disrupting closed and proprietary vendors in machine-learning and predictive analytics.\n\nNatural integration of H2O with the rest of Spark's capabilities is a definitive win for enterprise customers.\n<h3>More info</h3>\n<ul>\n \t<li><a href=\"http://www.slideshare.net/0xdata/sparkling-water-5-2814\" target=\"_blank\">Slides of the first Sparkling Water meetup</a></li>\n \t<li>Sparkling Water code is <a href=\"https://github.com/0xdata/h2o-sparkling\" target=\"_blank\">here</a></li>\n \t<li><a href=\"https://github.com/0xdata/h2o-sparkling/blob/master/README.md\" target=\"_blank\">Install and Test Instructions</a></li>\n</ul>\n<h3>Demo Code</h3>\n<pre>package water.sparkling.demo\n\nimport water.fvec.Frame\nimport water.util.Log\nimport hex.gbm.GBMCall.gbm\n\nobject AirlinesDemo extends Demo {\n  override def run(conf: DemoConf): Unit = {\n    // Prepare data\n    // Dataset\n    val dataset = \"data/allyears2k_headers.csv\"\n    // Row parser\n    val rowParser = AirlinesParser\n    // Table name for SQL\n    val tableName = \"airlines_table\"\n    // Select all flights with destination == SFO\n    val query = \"\"\"SELECT * FROM airlines_table WHERE dest=\"SFO\" \"\"\"\n\n    // Connect to shark cluster and make a query over prostate, transfer data into H2O\n    val frame:Frame = executeSpark[Airlines](dataset, rowParser, conf.extractor, tableName, query, local=conf.local)\n    Log.info(\"Extracted frame from Spark: \")\n    Log.info(if (frame!null) frame.toString + \"\\nRows: \" + frame.numRows() else \"<nothing>\")\n\n    // Now make a blocking call of GBM directly via Java API\n    val model = gbm(frame, frame.vec(\"isDepDelayed\"), 100, true)\n    Log.info(\"Model built!\")\n  }\n\n  override def name: String = \"airlines\"\n}</pre></td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(2014-06-30, 2014-06-30, UTC)</td><td><ul>\n \t<li>Databricks Cloud Allows Users to Get Value from Apache Spark without the Challenges Normally Associated with Big Data Infrastructure</li>\n \t<li>Ease-of-Use of Turnkey Solution Brings the Power of Spark to a Wider Audience and Fuels the Growth of the Spark Ecosystem</li>\n \t<li>Funding Led by NEA with Follow-on Investment from Andreessen Horowitz</li>\n</ul>\n<strong>Berkeley, Calif. (June 30, 2014)</strong>—Databricks, the company founded by the creators of Apache Spark—the powerful open-source processing engine that provides blazingly fast and sophisticated analytics—announced today the launch of <a title=\"Databricks Cloud\" href=\"https://databricks.com/cloud\">Databricks Cloud</a>, a cloud platform built around Apache Spark. In addition to this launch, the company is announcing the close of $33 million in series B funding led by New Enterprise Associates (NEA) with follow-on investment from Andreessen Horowitz.\n\n“Getting the full value out of their Big Data investments is still very difficult for organizations. Clusters are difficult to set up and manage, and extracting value from your data requires you to integrate a hodgepodge of disparate tools, which are themselves hard to use,” said Ion Stoica, CEO of Databricks. “Our vision at Databricks is to dramatically simplify big data processing and free users to focus on turning data into value. Databricks Cloud delivers on this vision by combining the power of Spark with a zero-management hosted platform and an initial set of applications built around common workflows.”\n\nDatabricks Cloud is powered by Spark, a unified processing engine that eliminates the need to stitch together a disjoint set of tools. Spark provides support for interactive queries (Spark SQL), streaming data (Spark Streaming), machine learning (MLlib) and graph computation (GraphX) natively with a single API across the entire pipeline. Additionally, Databricks Cloud reaps the benefit of the rapid pace of innovation in Spark, driven by the 200+ contributors that have made it the most active project in the Hadoop ecosystem.\n\nThe hosted platform also dramatically simplifies the pain of provisioning a Spark cluster. Users simply specify the desired capacity of a new cluster, and the platform handles all the details: provisioning servers on the fly, streamlining import and caching of data, handling all elements of security, and continually patching and updating Spark—freeing users of all the typical headaches and allowing them to explore and harness the power of Spark. The platform is currently available on Amazon Web Services, though expanding to additional cloud providers is on the roadmap.\n\nDatabricks Cloud comes with a set of built-in applications for those eager to immediately begin using Spark to access and analyze data to better compete in the marketplace:\n<ul>\n \t<li><strong>Notebooks</strong>. Provides a rich interface that allows users to perform data discovery and exploration and to plot the results interactively, execute entire workflows as scripts, and enable advanced collaboration features.</li>\n \t<li><strong>Dashboards</strong>. Create and host dashboards quickly and easily. Users can pick any outputs from previously created notebooks, assemble these outputs in a one-page dashboard with a WISIWYG editor, and publish the dashboard to a broader audience. The data and queries underpinning these dashboards can be regularly updated and refreshed.</li>\n \t<li><strong>Job Launcher</strong>. Enables anyone to run arbitrary Apache Spark jobs and trigger their execution, simplifying the process of building data products.</li>\n</ul>\n“One of the common complaints we heard from enterprise users was that big data is not a single analysis; a true pipeline needs to combine data storage, ETL, data exploration, dashboards and reporting, advanced analytics, and creation of data products. Doing that with today’s technology is incredibly difficult,” continues Stoica. “We built Databricks Cloud to enable the creation of end-to-end pipelines out of the box while supporting the full spectrum of Spark applications for enhanced and additional functionality. It was designed to appeal to a whole new class of users who will adopt big data now that many of the complexities of using it have been alleviated.”\n\nBeyond the built-in applications, Databricks Cloud enables users to seamlessly deploy and leverage the rapidly growing ecosystem of third-party Spark applications. Databricks Cloud is powered by the 100 percent open source Apache Spark, meaning that it will support all current and future “Certified on Spark” applications out of the box, and that all applications developed on Databricks Cloud will work across any of the “Certified Spark Distributions.”\n\n“Databricks remains committed to developing and expanding Apache Spark fully in the open and continuing to add to the capabilities that made it a vital big data platform,” said Matei Zaharia, CTO of Databricks. “We will continue to commit significant resources to drive open-source innovation in Spark alongside the community. Furthermore, we look forward to enabling a whole new set of users and developers to experience and leverage the power of Spark to drive enterprise value.”\n\nDatabricks Cloud is currently in limited availability with several beta users. Databricks is gradually opening up more capacity so visit <a href=\"http://www.databricks.com/cloud\" target=\"_blank\">www.databricks.com/cloud</a> to learn more about the platform and to get on the waiting list for getting access to the platform that is redefining how enterprises utilize big data.</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Events)</td><td>List(2014-04-29, 2014-04-29, UTC)</td><td>At Databricks, we’ve been thrilled to see the rapid pace of adoption of Apache Spark, as it has been embraced by an increasing number of enterprise vendors and has grown to be the most active open source project in the Hadoop ecosystem. We also know that a critical piece of enabling enterprises to unlock its potential is a strong ecosystem of applications built on top of or integrated with Spark.\n\nWe launched the <a href=\"http://www.databricks.com/certification/\">“Certified on Apache Spark”</a> program to support these application developer efforts, and have been blown away at the diverse set of applications being built on top of Spark, and want this great work to be exposed to the broader community. In that light, this year’s Spark Summit will have an “Application Spotlight” segment that will highlight some of the best we’ve seen.\n\nRead on for details on how to apply and what selection entails. All applications eligible (even if not yet certified) for the Databricks “Certified on Spark” program are encouraged to apply!\n<p style=\"text-align: center;\">[btn href=\"https://sparksummit.submittable.com/submit/29109842-c784-475a-b609-131ccadf15a6\" target=\"_blank\"]Apply Here[/btn]</p>\n\n<h2>Benefits of Application Spotlight</h2>\nDevelopers that are chosen to be part of the Application Spotlight segment will enjoy the following benefits:\n<ul>\n \t<li><strong>High visibility at Spark Summit:</strong> Each selected Application Spotlight awardee will be on stage for a QA session (in front of ~1000 attendees + live stream) to discuss their application and how they’re leveraging Spark.</li>\n \t<li><strong>Marketing events around Spark Summit:</strong> As part of the Spark Summit newsletter, the Application Spotlight awardees will be highlighted with a short blurb, and will appear on the Spark Summit website. For those who have a booth, we will have an ‘Application Spotlight’ tag attached to their booth to draw in attendees for a demo.</li>\n \t<li><strong>Attendance:</strong> The designated speaker for each awardee will receive a free 2-day pass to the Spark Summit.</li>\n \t<li><strong>General benefits of “Certified on Spark”:</strong> Application developer can tell customers their application works across a wide range of distributions, placed prominently on the Databricks website, issue press release, and get the chance to pen a guest article.</li>\n</ul>\nAdditionally, for those that have already completed the certification process, the additional work is minimal, and for those who have not the certification process is relatively lightweight once the technical integration has been completed.\n<h2>Process</h2>\nThe process that application developers would go through is the following:\n<ol>\n \t<li><strong>Submission:</strong> An application must be submitted through the online submission form by <strong>May 23, 2014</strong>, that contains the following:\n<ul>\n \t<li>General questionnaire filled out for product (e.g., product description, target audience, what makes the application unique)</li>\n \t<li>Spark-centric questionnaire (e.g., how does the product leverage / integrate with Spark, why the decision to leverage spark)</li>\n \t<li>Up to 5 screen-shots of the product</li>\n \t<li>[Optional] Link to a video up to 3 minutes in length</li>\n</ul>\n</li>\n \t<li><strong>Review:</strong> Databricks reviews submissions, and can optionally:\n<ul>\n \t<li>Send an email requesting a limited amount of additional detail for clarification</li>\n \t<li>Request a video chat session with a brief demo of the product for clarification</li>\n</ul>\n</li>\n \t<li><strong>Selection:</strong> Databricks will then select a handful of applications as “Application Spotlight” awardees and notify them by June 6th.</li>\n \t<li><strong>Certification:</strong> All awardees must submit a certification application by June 20th otherwise they risk being disqualified from the “Application Spotlight” program</li>\n</ol>\n<p style=\"text-align: center;\">[btn href=\"https://sparksummit.submittable.com/submit/29109842-c784-475a-b609-131ccadf15a6\" target=\"_blank\"]Submit Your App[/btn]</p>\n\n<h2>Requirements</h2>\nAny application that is eligible for the “Certified on Spark” program is eligible for the Application Spotlight program. In practical terms, this means:\n<ul>\n \t<li>Application / technology can be deployed on top of a customer’s stock Apache Spark distribution (e.g., bundled solutions that are internally powered by Spark are not eligible)</li>\n</ul>\nAdditionally, vendor must be will willing to submit certification application by deadline.</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Partners)</td><td>List(2014-05-08, 2014-05-08, UTC)</td><td><p>Today, Datastax and Databricks announced a partnership in which Apache Spark becomes an integral part of the Datastax offering, tightly integrated with Cassandra. We’re very excited to be embarking on this journey with Datastax for a multitude of reasons:</p>\n\n<h2 id=\"integrating-operational-systems-with-analytics\">Integrating operational systems with analytics</h2>\n<p>One of the use cases that we’ve increasingly been asked about by Spark users is the ability to create a closed loop system: perform advanced analytics directly on operational data that is then fed back into the operational system to drive necessary adaptation. The tight integration of Cassandra and Spark will enable users to achieve this goal by leveraging Cassandra as the high-performance transactional database that powers online applications and Spark as a next generation processing engine that can deliver deeper insights, faster while seamlessly moving between the two.</p>\n\n<h2 id=\"spark-beyond-hadoop\">Spark beyond Hadoop</h2>\n<p>The most talked about usage model for Spark to date has been within Hadoop deployments - Spark can operate directly over data in HDFS (without needing to move the data first) and natively supports YARN and Mesos, popular resource managers for Hadoop. However, Spark’s applicability is much broader: it is designed to be a general Big Data processing engine, and the Spark / Cassandra integration is a prime example of this - native processing without requiring a batch movement of data to Hadoop first (or even a Hadoop cluster). Furthermore, the recently announced SparkSQL will help optimize this integration further - not only will Spark be able to directly access data stored in Cassandra, but it will also be able to execute selected parts of the query in Cassandra itself. It can then pull the resulting data set into Spark for performing machine learning and other advanced analytics.</p>\n\n<h2 id=\"innovation-in-the-open\">Innovation in the Open</h2>\n<p>This partnership also brings together two groups with very strong open source commitments and heritage. Databricks is focused on keeping Apache Spark 100% open source and Datastax has invested numerous resources in growing the Apache Cassandra community, so it should be no surprise that a key tenet of this partnership is delivering joint innovation back to the open source community to help drive greater integration between the Spark and Cassandra communities over time. Look for significant contributions as we move forward on this journey.</p>\n\n<p>Please join us at the upcoming <a href=\"http://spark-summit.org/2014\" target=\"_blank\">Spark Summit</a> to hear more about the value of using Spark and Cassandra together and additional innovations on the horizon in a keynote talk by Martin Van Ryswyk, Datastax’s VP of Engineering.</p></td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(2014-04-30, 2014-04-30, UTC)</td><td>        <p><strong>VANCOUVER, BC. – April 30, 2014 –</strong> Simba Technologies Inc., the industry’s expert for Big Data connectivity, announced today that Databricks has licensed Simba’s ODBC Driver as its standards-based connectivity solution for Shark, the SQL front-end for Apache Spark, the next generation Big Data processing engine. Founded by the creators of Apache Spark and Shark, Databricks is developing cutting-edge systems to enable enterprises to discover deeper insights, faster.</p>\n\n<p>“We believe that Big Data is a tremendous opportunity that is still largely untapped, and we are working to revolutionize what organizations can do with it,” says Ion Stoica, Chief Executive Officer at Databricks, and Professor of Computer Science at UC Berkeley. “As part of this mission, we understand that BI tools will continue to be a key medium for consuming data and analytics and are excited to announce the availability of an enterprise-grade connectivity option for users of BI tools. Simba is the trusted name for enterprise Big Data connectivity, and was the clear partner choice for Databricks as we work to reach new heights in Big Data analytics and query speeds.”</p>\n\n<p>“When it comes to distributed data, Shark is cutting edge,” notes Simba Technologies CTO George Chow. “Its innovative distributed memory abstraction enables SQL queries on Big Data at speeds up to 100 times faster than current industry norms. Pair that velocity with Simba’s Shark ODBC Driver to connect industry-leading BI tools (like Tableau and SAP Lumira) with Apache Hadoop distributions, and you’ve got an enterprise solution that revolutionizes Big Data and enables incredibly powerful business insight.”</p>\n\n<p>Shark is an open-source distributed SQL query engine for Hadoop data that was originally developed at UC Berkeley’s AMPLab, delivering state-of-the-art performance and advanced analytics by using the powerful Apache Spark engine to speed up computations. Users can run Hive queries up to 100 times faster in memory or 10 times faster on disk. Shark can run unmodified Hive queries on existing warehouses, is fully compatible with existing Hive data, queries, and UDFs, and can call complex analytics functions like machine learning right from SQL. Shark supports mid-query fault tolerance, letting it scale to very large jobs and serve as the single tool for addressing the spectrum of SQL-query workloads. Furthermore, Shark is an integral part of building end-to-end data workflows with Spark that, in addition to SQL, including streaming data, graph computation, and machine-learning functionality.</p>\n\n<p>Simba Technologies’ standards-based ODBC drivers power business intelligence (BI), analytics, and reporting on Hive-based data for global F2000 leaders like Alteryx, Cloudera, DataStax, Hortonworks, MapR, and Microsoft. Simba’s drivers and providers are available for individual, enterprise, and OEM licensing. For more information about Simba’s Big Data ODBC &amp; JDBC Drivers and for a free 30-day trial, visit: <a href=\"http://www.simba.com/connectors\" target=\"_blank\">www.simba.com/connectors</a>.</p></td></tr><tr><td>List(Databricks Press Office)</td><td>List(Announcements, Company Blog, Partners)</td><td>List(2014-07-01, 2014-07-01, UTC)</td><td><strong>SAN FRANCISCO — July 1, 2014</strong> — Databricks, the company founded by the creators of Apache Spark – the popular open-source processing engine - today announced a new partnership with <a href=\"http://www.sap.com\" target=\"_blank\">SAP (NYSE: SAP)</a> and to deliver a Databricks-certified Apache Spark distribution offering for the SAP HANA® platform. The full production-ready distribution offering, based on Apache Spark 1.0, is deployable in the cloud or on premise and available for immediate download from SAP at no cost at <a href=\"http://spr.ly/SAP_and_Spark\" target=\"_blank\">spr.ly/SAP_and_Spark</a>. The announcement was made at the Spark Summit 2014, being held June 30 – July 2 in San Francisco.\n\nThe Databricks-certified distribution offering for SAP HANA contains the Spark processing engine that works with any Hadoop distribution out of the box, providing a more complete data store and processing layer for Hadoop.  Certified by Databricks to be compatible with the Apache Spark Distribution, this enables the rapidly growing set of “Certified on Spark” applications to run out of the box and on SAP HANA. This production-ready distribution offering is the first result of Databrick’s new partnership with SAP.\n\n“We’re thrilled to be embarking on this journey with SAP to bring together two powerful technologies to better enable enterprises to derive value from their data,” said Ion Stoica, CEO of Databricks. “SAP HANA is both an incredibly powerful and fast analytics engine, as well as a repository for some of the most valuable enterprise data by virtue of the enterprise applications that it helps run. This integration will help enable the large and growing community of Hadoop and Spark developers and applications to harness these capabilities immediately via Spark as well as extend the reach of SAP HANA.”\n\nSAP HANA integrated with Spark will help enable real-time applications and interactive analysis across corporate application data with content stored in Hadoop Distributed File System (HDFS). Developers and data scientists developing on Spark can also benefit from end-to-end data processing acceleration in SAP HANA by leveraging its comprehensive suite of in-memory engines and libraries for transactional applications, analytics, predictive, machine learning, text, graph and geospatial analysis. This helps simplify the integration of mission-critical applications with contextual data stored in Hadoop-like data stores. As a result, in-memory computation is enabled to happen where data resides and can help minimize costly and time-consuming data movement. \n\n“SAP has continually been at the forefront of innovation to simplify and better serve customers, and bringing together Spark and SAP HANA is simply the latest example of this,” said Steve Lucas, president, Platform Solutions, SAP. “This can allow enterprises to build on SAP HANA’s value proposition by providing some of the best-of-breed capabilities across the full spectrum of data and processing needs without the need to painstakingly stitch together independent solutions.”\n\nDevelopers and data scientists will be enabled to more easily create a new class of applications with SAP HANA and Spark. For example, they can span data domains, such as applications that integrate inventory analysis with social media trends for retailers; combine sensor data with billing systems to deliver personalized resource and cost-saving recommendations for utilities; or converge patient data with epidemiological information to construct better staffing decisions for healthcare providers.</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji)</td><td>List(Company Blog, Partners)</td><td>List(2014-07-01, 2014-07-01, UTC)</td><td>This morning SAP released its own “Certified Spark Distribution” as part of a brand new partnership announced between Databricks and SAP. We’re thrilled to be embarking on this journey with them, not just because of what it means for Databricks as a company, but just as importantly because of what it means for Apache Spark and the Spark community.\n<h2>Access to the full corpus of data</h2>\nFundamentally, every enterprise's big data vision is to convert data into value; a core ingredient in this quest is the availability of the data that needs to be mined for insights. Although the growth in volume of data sitting in HDFS has been incredible and continues to grow exponentially, much of this has been contextual data - e.g., social data, click-stream data, sensor data, logs, 3rd party data sources - and historical data. Real-time operational data - e.g., data from foundational enterprise applications such as ERP (Enterprise Resource Planning), CRM (Customer Relationship Management), and Supply Chain and Inventory Management (SCM) systems - has historically been maintained separately and moving data across in either direction to allow for analytics across the data set is cumbersome at best. The union of Spark and HANA is designed to change that.\n\nWith over 200,000 customers and among the largest portfolios of enterprise applications, SAP’s software serves as the gateway to one of the most valuable treasure troves of enterprise data globally, and SAP HANA is the cornerstone of SAP’s platform strategy underpinning these enterprise applications moving forward. Now, the community of Spark developers and users will have full access as well, enabling a richness of analytical possibilities that has been hard to achieve otherwise.\n\nBy the same token, enterprise applications built on top of HANA will now move closer to achieving the holy grail of a fully closed-loop operational system. Many of these applications are critical decision support systems for enterprises that directly drive day-to-day business. Having access to the full corpus of data would enable more accurate and effective decisions across verticals - sensor and weather data for utilities, social trends for retailers, traffic patterns and political events for commodity players. In practice, however, much of this data is held outside of HANA; with the Spark + HANA integration, enterprises can make mission-critical decisions across ‘100% of the data’.\n<h2>More than just data stores - two powerful engines</h2>\nMuch of the Big Data paradigm has been built on the notion of bringing ‘compute to the data’, as opposed to simply trying to ETL all the data to a single central repository. Spark and HANA embody this principle by providing powerful engines that operate on data in place. Beyond being a repository for valuable corporate data, HANA also provides a wide variety of advanced analytics packages - including predictive, text/NLP, and geospatial - at blazing fast speeds. Spark also provides advanced analytics capabilities - SQL, streaming data, machine learning, and graph computation - that work natively on HDFS and other data stores such as Cassandra.\n\nBeyond their individual capabilities, the true power of this integration is the ability of Spark and HANA to work closely together. Rather than performing a simple ‘select *’ query to grab a full data set, Spark can push down more advanced queries (e.g., complex joins, aggregates, and classification algorithms) - leveraging HANA’s horsepower and reducing expensive shuffles of data. A similar mechanism works for HANA users, where TGFs (Table Generating Functions) and Custom UDFs (User Defined functions) provide access to the full breadth of Spark’s capabilities through the Smart Data Access functionality.\n<h2>Certified Spark Distribution - the value of ecosystem</h2>\nCandidly, SAP is not known for its long history of open software use and contributions. That said, they certainly respect the value of it and what it can deliver - their bet on Spark is certainly a testament to that. More importantly, they understand the value of a vibrant ecosystem, and that a unified community is a key ingredient in enabling this. That’s why they’ve been adamant that any SAP Spark distribution is a Certified Spark Distribution - and hence capable of supporting the rapidly growing set of “Certified on Spark” applications and the development ecosystem. This action by a global powerhouse - along with the other Certified Distributions - is a strong testament that maintaining compatibility with the community-driven Apache Spark distribution can be achieved without sacrificing innovation and growth.\n<h2>Significant potential for the road ahead</h2>\nSAP’s distribution of Spark and its current integration with HANA is undoubtedly a terrific start and opens up a wealth of new opportunities for the Spark community. As we look forward, we see no shortage of potential opportunities for deeper integration - greater cross-functional performance, incorporating elements of SAP and HANA's security model in Spark, and facilitating the deployment of Spark and HANA together against a multitude of environments - and are excited to see where the journey leads.</td></tr><tr><td>List(Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-07-02, 2014-07-02, UTC)</td><td>With the introduction of Spark SQL and the new Hive on Apache Spark effort (<a href=\"https://issues.apache.org/jira/browse/HIVE-7292\">HIVE-7292</a>), we get asked a lot about our position in these two projects and how they relate to Shark. At the <a href=\"http://spark-summit.org/2014\">Spark Summit</a> today, we announced that we are ending development of Shark and will focus our resources towards Spark SQL, which will provide a superset of Shark’s features for existing Shark users to move forward. In particular, Spark SQL will provide both a seamless upgrade path from Shark 0.9 server and new features such as integration with general Spark programs.\n\n<img class=\"alignnone wp-image-818 size-large\" src=\"https://databricks.com/wp-content/uploads/2014/07/sql-directions-1024x691.png\" alt=\"Future of SQL on Spark\" width=\"400\" />\n<h2>Shark</h2>\nWhen the Shark project started 3 years ago, Hive (on MapReduce) was the only choice for SQL on Hadoop. Hive compiled SQL into scalable MapReduce jobs and could work with a variety of formats (through its SerDes). However, it delivered less than ideal performance. In order to run queries interactively, organizations deployed expensive, proprietary enterprise data warehouses (EDWs) that required rigid and lengthy ETL pipelines.\n\nThe stark contrast in performance between Hive and EDWs led to a huge debate in the industry questioning the inherent deficiency of query processing on general data processing engines. Many believed SQL interactivity necessitates an expensive, specialized runtime built for query processing (i.e. EDWs). Shark became <a href=\"https://amplab.cs.berkeley.edu/publication/shark-sql-and-rich-analytics-at-scale/\">one of the first interactive SQL on Hadoop systems</a>, and was the only one built on top of a general runtime (Spark). It demonstrated that none of the deficiencies that made Hive slow were fundamental, and a general engine such as Spark could marry the best of both worlds: it can be as fast as an EDW, and scales as well as Hive/MapReduce.\n\nWhy should you care about this seemingly academic debate? As organizations are looking for ways to give them an edge in businesses, they are employing techniques beyond the simple roll-up and drill-down capabilities that SQL provides. Building a SQL query engine on top of a general runtime unifies many disparate, powerful models, such as batch, streaming, machine learning. It enables data scientists and engineers to employ more sophisticated methods faster. Ideas from Shark were embraced quickly and even inspired some of the major efforts in speeding up Hive.\n<h2>From Shark to Spark SQL</h2>\nShark built on the Hive codebase and achieved performance improvements by swapping out the physical execution engine part of Hive. While this approach enabled Shark users to speed up their Hive queries, Shark inherited a large, complicated code base from Hive that made it hard to optimize and maintain. As we moved to push the boundary of performance optimizations and integrating sophisticated analytics with SQL, we were constrained by the legacy that was designed for MapReduce.\n\nIt is for this reason that we are ending development in Shark as a separate project and moving all our development resources to Spark SQL, a new component in Spark. We are applying what we learned in Shark to Spark SQL, designed from ground-up to leverage the power of Spark. This new approach enables us to innovate faster, and ultimately deliver much better experience and power to users.\n\nFor <b>SQL users</b>, Spark SQL provides state-of-the-art SQL performance and maintains compatibility with Shark/Hive. In particular, like Shark, Spark SQL supports all existing Hive data formats, user-defined functions (UDF), and the Hive metastore. With features that will be introduced in Apache Spark 1.1.0, Spark SQL beats Shark in <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">TPC-DS performance</a> by almost an order of magnitude.\n\nFor <b>Spark users</b>, Spark SQL becomes the narrow-waist for manipulating (semi-) structured data as well as ingesting data from sources that provide schema, such as JSON, Parquet, Hive, or EDWs. It truly unifies SQL and sophisticated analysis, allowing users to mix and match SQL and more imperative programming APIs for advanced analytics.\n\nFor <b>open source hackers</b>, Spark SQL proposes a novel, elegant way of building query planners. It is incredibly easy to add new optimizations under this framework. We have been completely overwhelmed by the support and enthusiasm that the open source community has shown Spark SQL, largely thanks to this new design. Already after merely three months, over 40 contributors have contributed code to it. Thank you.\n<h2>Hive on Spark Project (HIVE-7292)</h2>\nWhile Spark SQL is becoming the standard for SQL on Spark, we do realize many organizations have existing investments in Hive. Many of these organizations, however, are also eager to migrate to Spark. The Hive community proposed a <a href=\"https://issues.apache.org/jira/browse/HIVE-7292\">new initiative</a> to the project that would add Spark as an alternative execution engine to Hive. For these organizations, this effort will provide a clear path for them to migrate the execution to Spark. We are delighted to work with and support the Hive community to provide a smooth experience for end-users.\n\nIn short, we firmly believe Spark SQL will be the future of not only SQL, but also structured data processing on Spark. We are hard at work and will bring you a lot more in the next several releases. And for organizations with legacy Hive deployments, Hive on Spark will provide them a clear path to Spark.</td></tr><tr><td>List(Ion Stoica)</td><td>List(Company Blog, Product)</td><td>List(2014-07-14, 2014-07-14, UTC)</td><td>Our vision at Databricks is to <strong>make big data easy</strong> so that we enable <strong>every</strong> organization to turn its data into value. At Spark Summit 2014, we were very excited to unveil <a href=\"https://databricks.com/cloud\" target=\"_blank\">Databricks</a>, our first product towards fulfilling this vision.\n\nIn this post, I’ll briefly go over the challenges that data scientists and data engineers face today when working with big data, and then show how Databricks addresses these challenges.\n<h2>Today’s Big Data Challenges</h2>\nWhile the promise of big data to <a href=\"http://spark-summit.org/2014/talk/using-spark-to-generate-analytics-for-international-cable-tv-video-distribution\" target=\"_blank\">improve businesses</a>, <a href=\"http://spark-summit.org/2014/talk/david-patterson\" target=\"_blank\">save lives</a>, and <a href=\"http://spark-summit.org/2014/talk/A-platform-for-large-scale-neuroscience\" target=\"_blank\">advance science</a> is becoming more and more real, analyzing and processing data remains as hard as ever. Software engineers, data engineers, data scientists at both small and large organizations continue to struggle with setting up and maintaining clusters, dealing with a zoo of systems, which are not only hard to integrate but also hard to use.\n<h3>Set-up and maintain clusters</h3>\nWhen an organization starts a big data initiative, typically, the first step is setting up a Hadoop cluster. Unfortunately, this is hard. Today it may take 6-9 months just to set up a cluster on premise. Even if the organization already has an on premise cluster, it may take 2-3 months to get a few more servers for a new big data project.\n<h3>Integrate a zoo of systems</h3>\nOnce a cluster is in place, the next step is building a data pipeline. As shown in Figure 1, a typical data pipeline based on Hadoop includes ETL, (interactive) data exploration, building dashboards and reports, advanced analytics, and data products, such as a recommendation system.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"margin-top: 30px; margin-bottom: 5px; max-width: 100%;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure1.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 1: Typical big data pipeline.</em></p>\nUnfortunately, implementing such a data pipeline is very hard, as it requires stitching together a hodgepodge of disparate, complex systems, including batch processing systems (e.g., Hadoop MR), query engines (e.g., Hive, Impala, Apache Drill), business intelligence tools (e.g., Qlik, Tableau), and systems providing support for machine learning and graph-based algorithms (e.g., Giraph, GraphLab, Mahout, R).\n<h3>Hard to use systems</h3>\nUnfortunately, even after setting up the pipeline, the systems themselves remain hard to use. Different systems expose different APIs and programming languages (e.g., Java, Clojure, Scala, Python, R), and, at best, they provide a shell interface. Furthermore, performing advanced analytics is hard, and building data applications is even harder.\n<h2>Databricks</h2>\nOur goal at Databricks is to address all these challenges. To achieve this goal we have introduced Databricks (Figure 2) an end-to-end platform for data analysis and processing that we believe will make big data easier to use than ever before.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"display: block; max-width: 60%; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure2.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 2: Databricks Cloud</em></p>\nDatabricks is built around Apache Spark and consists of two additional components: a hosted platform (Databricks Platform) and a workspace (Databricks Workspace). Next, let’s see how these components address each of the three challenges.\n<h3>Databricks Platform: No need to set-up and maintain clusters</h3>\nDatabricks Platform is a hosted platform that makes it trivially easy to create and manage a cluster. Databricks Platform includes a sophisticated cluster manager which allows users to have a cluster up and running in seconds, while providing everything they need out-of-the-box. In particular, Databricks Platform provides security, resource isolation, it instantiates a fully configured and up-to-date Spark cluster, it allows dynamic scaling, and it provides seamless data import. This way, Databricks Platform obviates the need to set up and maintain an on premise cluster.\n<h3>Apache Spark: Unifying existing big data systems</h3>\nDatabricks is built around Apache Spark, which unifies many of the functionalities provided by today’s big data systems. In particular, Apache Spark provides support for batch processing, interactive query processing, streaming, machine learning, and graph based computations, all with a single API. This enables developers, data scientists, and data engineers to implement their entire pipeline in one system.\n<h3>Databricks Workspace: Making the platform easy to use</h3>\nDatabricks Workspace makes it dramatically easier to use big data frameworks, in general, and Spark, in particular, by providing three powerful web-based applications: notebooks, dashboards, and a job launcher.\n\n<strong>Notebooks</strong> allow users to interactively query and visualize data. Notebooks also provide support for online collaboration, thus allowing multiple users to cooperate on data exploration in real-time. Currently, notebooks allows users to query and analyze data using Python, SQL, and Scala.\n\nOnce users create one or more notebooks, they can take the most interesting results from these notebooks and create sophisticated <strong>dashboards</strong>. They can do so through a powerful yet intuitive dashboard builder, and then publish a dashboard just with the click of a button to other employees in the organization, or to their customers. Dashboards are interactive, as every plot can depend on one or more variables. When these variables are updated, the query behind each plot is automatically re-executed, and the plot is regenerated.\n\nFinally, Databricks Workspace includes a <strong>job launcher</strong> that enables users to programmatically run arbitrary Spark jobs. For instance, users can schedule jobs to run periodically, or run when their inputs change.\n<h3>Building big data pipelines with Databricks</h3>\nFigure 3 shows how Databricks can dramatically simplify the big data pipeline shown in Figure 1.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"margin-top: 30px; margin-bottom: 5px; max-width: 100%;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure3.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 3: Databricks Cloud, big data pipeline</em></p>\nLet’s assume for simplicity that most customer’s data is in S3. Then, Databricks can operate on S3 data in place (no need to copy it) and implement the entire data pipeline:\n<ul>\n \t<li>One can do ETL using Spark jobs or notebooks, which can run periodically or when their inputs change</li>\n \t<li>One can do interactive exploration and advanced analytics using notebooks</li>\n \t<li>One can create dashboards in reports using the Dashboard builder</li>\n \t<li>Finally, one can create, build and run data products as Spark jobs or notebooks.</li>\n</ul>\nIn addition, Databricks can read inputs from other storage systems and databases available in AWS, and lets users use their favorite business intelligence tools through an ODBC connector.\n\nThis way Databricks allows users to focus on finding answers and building great data products rather than wrestling with setting up clusters, and stitching together disparate systems that are difficult to use.\n<h2>Third Party Applications</h2>\nWhile Databricks Workspace makes Databricks instantly useful out-of-the-box and it allows users to build non-trivial data pipelines, we are planning to add support for third party applications, beyond our own applications.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"display: block; max-width: 60%; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Figure4.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 4: Supporting 3rd party apps</em></p>\nWe are already working with some of the certified Spark application developers to run their applications on top of Databricks. We are looking forward to extending Databricks with a vibrant application ecosystem.\n<h2>Databricks and Spark Community</h2>\nDatabricks will have a beneficial impact on the Apache Spark project, and it reaffirms our commitment to making Spark the best big data framework. Databricks will dramatically accelerate Spark’s adoption, as it will make it much easier to learn and use Apache Spark. In addition, Databricks runs 100% Apache Spark. This means that there is no lock in. Users can take their jobs and applications they develop on Databricks and run them on any certified Spark distribution be it on premise or in the cloud.\n<h2>Summary</h2>\nIt is our belief that Databricks will dramatically simplify big data analysis, and it will become the best place to develop, test, and run data products. This will further fuel the growth of the Spark community by making big data easier to use than ever before. For more information about availability and deployment scenarios, please check the <a href=\"http://www.databricks.com/cloud#cloud-faq\" target=\"_blank\">following FAQ</a>.</td></tr><tr><td>List(Xiangrui Meng)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-07-16, 2014-07-16, UTC)</td><td>MLlib is an Apache Spark component focusing on machine learning. It became a standard component of Spark in version 0.8 (Sep 2013). The initial contribution was from Berkeley AMPLab. Since then, 50+ developers from the open source community have contributed to its codebase. With the release of Apache Spark 1.0, I’m glad to share some of the new features in MLlib. Among the most important ones are:\n<ul>\n \t<li>sparse data support</li>\n \t<li>regression and classification trees</li>\n \t<li>distributed matrices</li>\n \t<li>PCA and SVD</li>\n \t<li>L-BFGS optimization algorithm</li>\n \t<li>new user guide and code examples</li>\n</ul>\nThis is the first in a series of blog posts about features and optimizations in MLlib. We will focus on one feature new in 1.0 — sparse data support.\n<h2>Large-scale ≈ Sparse</h2>\nWhen I was in graduate school, I wrote “large-scale sparse least squares” in a paper draft. My advisor crossed out the word “sparse” and left a comment: “Large-scale already implies sparsity, so you don’t need to mention it twice.” It is a good argument. Sparse datasets are indeed very common in the big data world, where the sparsity may come from many sources, e.g.,\n<ul>\n \t<li>feature transformation: one-hot encoding, interaction, and binarization,</li>\n \t<li>large feature space: n-grams,</li>\n \t<li>missing data: rating matrix.</li>\n</ul>\nTake the Netflix Prize qualifying dataset as an example. It contains around 100 million ratings generated by 480,189 users on 17,770 movies. Therefore, the rating matrix contains only 1% nonzeros. If an algorithm can utilize this sparsity, it can see significant improvements.\n<h2>Exploiting Sparsity</h2>\nIn Apache Spark 1.0, MLlib adds full support for sparse data in Scala, Java, and Python (previous versions only supported it in specific algorithms like alternating least squares). It takes advantage of sparsity in both storage and computation in methods including SVM, logistic regression, Lasso, naive Bayes, k-means, and summary statistics.\n\nTo give a concrete example, we ran k-means clustering on a dataset that contains more than 12 million examples with 500 feature dimensions. There are about 600 million nonzeros and hence the density is about 10%. The result is listed in the following table:\n<table style=\"width: 350px; height: 120px;\" border=\"0\" cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: black;\">\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">sparse</th>\n<th style=\"text-align: center;\">dense</th>\n</tr>\n<tr>\n<td style=\"text-align: center;\">storage</td>\n<td style=\"text-align: center;\">7GB</td>\n<td style=\"text-align: center;\">47GB</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">time</td>\n<td style=\"text-align: center;\">58s</td>\n<td style=\"text-align: center;\">240s</td>\n</tr>\n</thead>\n</table>\n&nbsp;\n\nSo, not only did we save 40GB of storage by switching to the sparse format, but we also received a 4x speedup. If your dataset is sparse, we strongly recommend you to try this feature.\n<h2>Getting Started</h2>\nBoth sparse and dense feature vectors are supported via the Vector interface. A sparse vector is represented by two parallel arrays: indices and values. Zero entries are not stored. A dense vector is backed by a double array representing its entries. For example, a vector [1., 0., 0., 0., 0., 0., 0., 3.] can be represented in the sparse format as (7, [0, 6], [1., 3.]), where 7 is the size of the vector, as illustrated below:\n<p align=\"center\"><img class=\"alignnone wp-image-939\" src=\"https://databricks.com/wp-content/uploads/2014/07/xAJI2gUHspdUO4rO3JxsGPlHCsUvIZc9xI08QJA_14sebOFMDFMTSvbYi1c4AaPS-rh7Ly-FBJukdGxOo7mjKj9q4wb1ehZXfFFZHOLdn2MQGhLjtooF7Cm053PbwvXHUg.png\" alt=\"mllib-sparse-vector\" width=\"249\" height=\"112\" /></p>\nTake the Python API as an example. MLlib recognizes the following types as dense vectors:\n<ul>\n \t<li>NumPy’s <code><a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\">array</a>,</code></li>\n \t<li>Python’s list, e.g., <code>[1, 2, 3].</code></li>\n</ul>\nand the following as sparse vectors:\n<ul>\n \t<li>MLlib’s <code><a href=\"http://spark.apache.org/docs/1.0.1/api/python/pyspark.mllib.linalg.SparseVector-class.html\">SparseVector</a>,</code></li>\n \t<li>SciPy’s <code><a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix\">csc_matrix</a> </code>with a single column.</li>\n</ul>\nWe recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented in <a href=\"http://spark.apache.org/docs/1.0.0/api/python/pyspark.mllib.linalg.Vectors-class.html\"><code>Vectors</code></a> to create sparse vectors.\n\n[python]\nimport numpy as np\nimport scipy.sparse as sps\nfrom pyspark.mllib.linalg import Vectors\n\n# Use a NumPy array as a dense vector.\ndv1 = np.array([1.0, 0.0, 3.0])\n# Use a Python list as a dense vector.\ndv2 = [1.0, 0.0, 3.0]\n# Create a SparseVector.\nsv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n# Use a single-column SciPy csc_matrix as a sparse vector.\nsv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), \n    np.array([0, 2])), shape = (3, 1))\n[/python]\n\nK-means takes an RDD of vectors as input. For supervised learning, the training dataset is represented by an RDD of labeled points. A LabeledPoint contains a label and a vector, either sparse or dense. Creating a labeled point is straightforward.\n\n[python]\nfrom pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.regression import LabeledPoint\n\n# Create a labeled point with a positive label and a dense vector.\npos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n\n# Create a labeled point with a negative label and a sparse vector.\nneg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))\n[/python]\n\nMLlib also supports reading and saving labeled data in LIBSVM format. For more information on the usage, please visit the <a href=\"https://spark.apache.org/docs/1.0.1/mllib-guide.html\">MLlib guide</a> and <a href=\"https://github.com/apache/spark/tree/v1.0.1/examples/src/main/scala/org/apache/spark/examples/mllib\">code examples</a>.\n<h2>When to Exploit Sparsity</h2>\nFor many large-scale datasets, it is not feasible to store the data in a dense format. Nevertheless, for medium-sized data, it is natural to ask when we should switch from a dense format to sparse. In MLlib, a sparse vector requires 12nnz+4 bytes of storage, where nnz is the number of nonzeros, while a dense vector needs 8n bytes, where n is the vector size. So storage-wise, the sparse format is better than the dense format when more than 1/3 of the elements are zero. However, assuming that the data can be fit into memory in both formats, we usually need sparser data to observe a speedup, because the sparse format is not as efficient as the dense format in computation. Our experience suggests a sparsity of around 10%, while the exact switching point for the running time is indeed problem-dependent.\n\nSparse data support is part of Apache Spark 1.0, which is available for download right now at <a href=\"http://spark.apache.org/\">http://spark.apache.org/</a>. We will cover more new features in MLlib in a series of posts. So stay tuned.</td></tr><tr><td>List(Matei Zaharia)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-07-19, 2014-07-19, UTC)</td><td><div class=\"post-meta\">This post originally appeared in <a href=\"http://inside-bigdata.com/2014/07/15/theres-spark-theres-fire-state-apache-spark-2014/\" target=\"_blank\">insideBIGDATA</a> and is reposted here with permission.</div>\n\n<hr />\n\nWith the second <a href=\"http://spark-summit.org/2014\">Spark Summit</a> behind us, we wanted to take a look back at our journey since 2009 when Apache Spark, the fast and general engine for large-scale data processing, was initially developed. It has been exciting and extremely gratifying to watch Spark mature over the years, thanks in large part to the vibrant, open source community that latched onto it and busily began contributing to make Spark what it is today.\n\nThe idea for Spark first emerged in the AMPLab (AMP stands for Algorithms, Machines, and People) at the University of California, Berkeley. With its significant industry funding and exposure, the AMPlab had a unique perspective on what is important and what issues exist among early adopters of big data. We had worked with most of the early users of Hadoop and consistently saw the same issues arise. Spark itself started as the solution to one such problem—speeding machine learning applications on clusters, which machine learning researchers in the lab were having trouble doing using Hadoop. However, we soon realized that we could easily cover a much broader set of applications.\n<h2>The Vision</h2>\nWhen we worked with early Hadoop users, we saw that they were all excited about the scalability of MapReduce. However, as soon as these users began using MapReduce, they needed more than the system could offer. First, users wanted faster data analysis—instead of waiting tens of minutes to run a query, as was required with MapReduce’s batch model, they wanted to query data interactively, or even continuously in real-time. Second, users wanted more sophisticated processing, such as iterative machine learning algorithms, which were not supported by the rigid, one-pass model of MapReduce.\n\nAt this point, several systems had started to emerge as point solutions to these problems, e.g., systems that ran only interactive queries, or only machine learning applications. However, these systems were difficult to use with Hadoop, as they would require users to learn and stitch together a zoo of different frameworks to build pipelines. Instead, we decided to try to generalize the MapReduce model to support more types of computation in a single framework.\n\nWe achieved this using only two simple extensions to the model. First, we added support for storing and operating on data in memory—a key optimization for the more complex, iterative algorithms required in applications like machine learning, and one that proved shrewd with the continued drop in memory prices. Second, we modeled execution as general directed acyclic graphs (DAGs) instead of the rigid model of map-and-reduce, which led to significant speedups even on disk. With these additions we were able to cover a wide range of emerging workloads, matching and sometimes exceeding the performance of specialized systems while keeping a single, simple unified programming model.\n\nThis decision allowed, over time, new functionality such as Shark (SQL over Spark), Spark Streaming (stream processing), MLlib (efficient implementations of machine learning algorithms), and GraphX (graph computation over Spark) to be built. These modules in Spark are not separate systems, but libraries that users can combine together into a program in powerful ways. Combined with the more than 80 basic data manipulation operators in Spark, they make it dramatically simpler to build big data applications compared to previous, multi-system pipelines. And we have sought to make them available in a variety of programming languages, including Java, Scala, and Python (available today) and soon R.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Databricks_stack.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 1: The Spark Stack</em></p>\nAs interest in Spark increased, we received a lot of questions about how Spark related to Hadoop and whether Spark was its replacement. The reality is that Hadoop consists of three parts: a file system (HDFS), resource management (YARN/Mesos), and a processing layer (MapReduce). Spark is only a processing engine and thus is an alternative for that last layer. Having it operate over HDFS data was a natural starting point because the volume of data in HDFS was growing rapidly. However, Spark’s architecture has also allowed it to support a host of storage systems beyond Hadoop. Spark is now being used as a processing layer for other data stores (e.g., Cassandra, MongoDB) or even to seamlessly join data from multiple data stores (e.g., HDFS and an operational data store).\n<h2>Success Comes from the Community</h2>\nPerhaps the one decision that has made Spark so robust is our continuing commitment to keep it 100 percent open source and work with a large set of contributors from around the world. That commitment continues to pay dividends in Spark’s future and effectiveness.\n\nIn a relatively short time, enthusiasm rose among the open source community and is still growing. Indeed, in just the last 12 months, Spark has had more than 200 people from more than 50 organizations contribute code to the project, making it the most active open source project in the Hadoop ecosystem. Even after reaching this point, Spark is still enjoying steady growth, moving us toward the inflection point of the hockey stick adoption curve.\n\nNot only has the community invested countless hours in development, but it has also gotten people excited about Spark and brought users together to share ideas. One of the more exciting events was the first Spark Summit held in December 2013 in San Francisco, which drew nearly 500 attendees. At this year’s Summit, held June 30, we had double the amount of participants. The 2014 Summit included more than 50 community talks on applications, data science, and research using Spark. In addition, Spark community meetups have sprouted all over the United States and internationally, and we anticipate that number to grow.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/07/Databricks_contrib.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 2: Spark Contributors By Release</em></p>\n\n<h2>Spark in the Enterprise</h2>\nWe have seen a real rise in excitement among enterprises as well. After going through the initial proof-of-concept process, Spark has found its place in the enterprise ecosystem and every major Hadoop distributor has made Spark part of their distribution. For many of these distributions, support came from the bottom up: we heard from vendors that customers were downloading and using Spark on their own and then contacting vendors to ask them to support it.\n\nSpark is being used across many verticals including large Internet companies, government agencies, financial service companies, and some major players such as Yahoo, eBay, Alibaba, and NASA. These enterprises are deploying Spark for a variety of use cases including ETL, machine learning, data product creation, and complex event processing with streaming data. Vertical-specific cases include churn analysis, fraud detection, risk analytics, and 360-degree customer views. And many companies are conducting advanced analytics using Spark’s scalable machine learning library (MLlib), which contains high-quality algorithms that leverage iteration to yield better results.\n\nFinally, in addition to being used directly by customers, Spark is increasingly the backend for a growing number of higher-level business applications. Major business intelligence vendors such as Microstrategy, Pentaho, and Qlik have all certified their applications on Spark, while a number of innovative startups such as Adatao, Tresata, and Alpine are basing products on it. These applications bring the capabilities of Spark to a much broader set of users throughout the enterprise.\n<h2>Spark’s Future</h2>\nWe recently released version 1.0 of Apache Spark – a major milestone for the project. This version includes a number of added capabilities such as:\n<ul>\n \t<li>A stable application programming interface to provide compatibility across all 1.x releases.</li>\n \t<li>Spark SQL to provide schema-aware data modeling and SQL language support.</li>\n \t<li>Support for Java 8 lambda syntax to simplify writing applications in Java.</li>\n \t<li>Enhanced MLlib with several new algorithms; MLlib continues to be extremely active on its own with more than 40 contributors since it was introduced in September 2013.</li>\n \t<li>Major updates to Spark’s streaming and graph libraries.</li>\n</ul>\nThese wouldn’t have happened without the support of the community. Many of these features were requested directly by users, while others were contributed by the dozens of developers who worked on this release. One of our top priorities is to continue to make Spark more robust and focus on key enterprise features, such as security, monitoring, and seamless ecosystem integration.\n\nAdditionally, the continued success of Spark is dependent on a vibrant ecosystem. For us, it is exciting to see the community innovate and enhance above, below, and around Spark. Maintaining compatibility across the various Spark distributions will be critical, as we’ve seen how destructive forking and fragmentation can be to open source efforts. We would like to define and center compatibility around the Apache version of Spark, where we continue to make all our contributions. We are excited to see the community rally around this vision.\n\nWhile Spark has come far in the past five years, we realize that there is still a lot to do. We are working hard on new features and improvements in both the core engine and the libraries built on top. We look forward to future Spark releases, an expanded ecosystem, and future Summits and meetups where people are generating ideas that go far beyond what we imagined years ago at UC Berkeley.</td></tr><tr><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-07-23, 2014-07-23, UTC)</td><td>Recommendation systems are among the most popular applications of machine learning. The idea is to predict whether a customer would like a certain item: a product, a movie, or a song. Scale is a key concern for recommendation systems, since computational complexity increases with the size of a company's customer base. In this blog post, we discuss how Apache Spark MLlib enables building recommendation models from billions of records in just a few lines of Python (<a href=\"http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\">Scala/Java APIs also available</a>).<!--more-->\n\n[python]\nfrom pyspark.mllib.recommendation import ALS\n\n# load training and test data into (user, product, rating) tuples\ndef parseRating(line):\n  fields = line.split()\n  return (int(fields[0]), int(fields[1]), float(fields[2]))   \ntraining = sc.textFile(&quot;...&quot;).map(parseRating).cache()\ntest = sc.textFile(&quot;...&quot;).map(parseRating)\n\n# train a recommendation model\nmodel = ALS.train(training, rank = 10, iterations = 5)\n\n# make predictions on (user, product) pairs from the test data\npredictions = model.predictAll(test.map(lambda x: (x[0], x[1])))\n[/python]\n\n<h2>What’s Happening under the Hood?</h2>\nRecommendation algorithms are usually divided into:\n\n(1) <strong>Content-based filtering</strong>: recommending items similar to what users already like. An example would be to play a Megadeth song after a Metallica song.\n\n(2) <strong>Collaborative filtering</strong>: recommending items based on what similar users like, e.g., recommending video games after someone purchased a game console because other people who bought game consoles also bought video games.\n<p align=\"center\"><img class=\"aligncenter wp-image-984\" src=\"https://databricks.com/wp-content/uploads/2014/07/als-illustration.png\" alt=\"\" width=\"350px\" /></p>\nSpark MLlib implements a collaborative filtering algorithm called <strong>Alternating Least Squares (ALS)</strong>, which has been implemented in many machine learning libraries and widely studied and used in both academia and industry. ALS models the rating matrix (R) as the multiplication of low-rank user (U) and product (V) factors, and learns these factors by minimizing the reconstruction error of the observed ratings. The unknown ratings can subsequently be computed by multiplying these factors. In this way, companies can recommend products based on the predicted ratings and increase sales and customer satisfaction.\n\nALS is an iterative algorithm. In each iteration, the algorithm alternatively fixes one factor matrix and solves for the other, and this process continues until it converges. MLlib features a blocked implementation of the ALS algorithm that leverages Spark’s efficient support for distributed, iterative computation. It uses native LAPACK to achieve high performance and scales to billions of ratings on commodity clusters.\n<h2>Scalability, Performance, and Stability</h2>\nRecently we did an experiment to benchmark ALS implementations in Spark MLlib at scale. The benchmark was conducted on EC2 using m3.2xlarge instances set up by the Spark EC2 script. We ran Spark using out-of-the-box configurations. To help understand state-of-the-art, we also built Mahout from GitHub and tested it. This benchmark is reproducible on EC2 using the scripts at <a href=\"https://github.com/databricks/als-benchmark-scripts\">https://github.com/databricks/als-benchmark-scripts</a>.\n\nWe ran 5 iterations of ALS on scaled copies of the <a href=\"https://snap.stanford.edu/data/web-Amazon.html\" target=\"_blank\">Amazon Reviews dataset</a>, which contains 35 million ratings collected from 6.6 million users on 2.4 million products. For each user, we create pseudo-users that have the same ratings. That is, for every rating as (userId, productId, rating), we generate (userId+i, productId, rating) where 0 &lt;= i &lt; s and s is the scaling factor.\n<p align=\"center\"><img class=\"alignnone size-full wp-image-1015\" src=\"https://databricks.com/wp-content/uploads/2014/07/als-perf.png\" alt=\"ALS performance\" width=\"600\" /></p>\nThe current version of Mahout runs on Hadoop MapReduce, whose scheduling overhead and lack of support for iterative computation substantially slows down ALS. Mahout recently announced switching to Spark as the execution engine, which will hopefully address the performance concerns.\n\nSpark MLlib demonstrated excellent performance and scalability, as demonstrated in the chart above. MLlib can also scale to much larger datasets and to larger number of nodes, thanks to its fault-tolerance design. With 50 nodes, we ran 10 iterations of MLlib's ALS on 100 copies of the Amazon Reviews dataset in only 40 minutes. And with EC2 spot instances the total cost was less than $2. Users can use Spark MLlib to reduce the model training time and the cost for ALS, which is historically very expensive to run because the algorithm is very communication intensive and computation intensive.\n<table style=\"width: 500px; height: 70px;\" cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: black;\">\n<td style=\"text-align: center;\"># ratings</td>\n<td style=\"text-align: center;\"># users</td>\n<td style=\"text-align: center;\"># products</td>\n<td style=\"text-align: center;\"> time</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">3.5 billion</td>\n<td style=\"text-align: center;\">660 million</td>\n<td style=\"text-align: center;\">2.4 million</td>\n<td style=\"text-align: center;\">40 mins</td>\n</tr>\n</thead>\n</table>\n&nbsp;\n\nIt is our belief at Databricks and the broader Spark community that machine learning frameworks need to be performant, scalable, and be able to cover a wide range of workloads including data exploration and feature extraction. MLlib integrates seamlessly with other Spark components, delivers best-in-class performance, and substantially simplifies operational complexity by running on top of a fault-tolerant engine. That said, our work is not done and we are working on making machine learning easier. Stay tuned for more exciting features.\n\n&nbsp;\n\nNote: The blog post was updated on July 24, 2014 to reflect a new performance optimization that will be included in Spark MLlib 1.1. The runtime for 3.5B ratings went down from 90 mins in MLlib 1.0 to 40 mins in MLlib 1.1.</td></tr><tr><td>List(Li Pu, Reza Zadeh)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-07-22, 2014-07-22, UTC)</td><td><div class=\"post-meta\">Guest post by Li Pu from Twitter and Reza Zadeh from Databricks on their recent contribution to Apache Spark's machine learning library.</div>\n\n<hr />\n\nThe <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular Value Decomposition (SVD)</a> is one of the cornerstones of linear algebra and has widespread application in many real-world modeling situations. Problems such as recommender systems, linear systems, least squares, and many others can be solved using the SVD. It is frequently used in statistics where it is related to principal component analysis (PCA) and to correspondence analysis, and in signal processing and pattern recognition. Another usage is latent semantic indexing in natural language processing.\n\nDecades ago, before the rise of distributed computing, computer scientists developed the single-core <a href=\"http://www.caam.rice.edu/software/ARPACK/\">ARPACK package</a> for computing the eigenvalue decomposition of a matrix. Since then, the package has matured and is in widespread use by the numerical linear algebra community, in tools such as <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html\">SciPy</a>, <a href=\"http://www.gnu.org/software/octave/doc/interpreter/External-Packages.html\">GNU Octave</a>, and <a href=\"http://www.mathworks.com/products/matlab/index.html\">MATLAB</a>.\n\nAn important feature of ARPACK is its ability to allow for arbitrary matrix storage formats. This is possible because it doesn't operate on the matrices directly, but instead acts on the matrix via prespecified operations, such as matrix-vector multiplies. When a matrix operation is required, ARPACK gives control to the calling program with a request for a matrix-vector multiply. The calling program must then perform the multiply and return the result to ARPACK.\n\nBy using the distributed-computing power of Spark, we can distribute the matrix-vector multiplies, and thus exploit the years of numerical computing expertise that have gone into building ARPACK, and the years of distributing computing expertise that have gone into Spark.\n\nSince ARPACK is written in Fortran77, it cannot immediately be used on the Java Virtual Machine. However, through the <a href=\"https://github.com/fommil/netlib-java\">netlib-java</a> and <a href=\"https://github.com/scalanlp/breeze\">breeze</a> interfaces, we can use ARPACK on the JVM. This also means that low-level hardware optimizations can be exploited for any local linear algebraic operations. As with all linear algebraic operations within MLlib, we use hardware acceleration whenever possible.\n\nWe are building the linear algebra capabilities of <a href=\"http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html\">MLlib</a>. Currently in Apache Spark 1.0 there is support for Tall and Skinny <a href=\"https://github.com/apache/spark/pull/88\">SVD and PCA,</a> and as of Apache Spark 1.1, we will have support for SVD and PCA <a href=\"https://github.com/apache/spark/pull/964\">via ARPACK</a>.\n<h2>Example Runtime</h2>\nA very popular matrix in the recommender systems community is the Netflix Prize Matrix. The matrix has 17,770 rows, 480,189 columns, and 100,480,507 non-zeros. Below we report results on several larger matrices (up to 16x larger) we experimented with at Twitter.\n\nWith the Spark implementation of SVD using ARPACK, calculating wall-clock time with 68 executors and 8GB memory in each, looking for the top 5 singular vectors, we can factorize larger matrices distributed in RAM across a cluster, in a few seconds.\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Matrix size</b></td>\n<td><b>Number of nonzeros</b></td>\n<td><b>Time per iteration (s)</b></td>\n<td><b>Total time (s)</b></td>\n</tr>\n<tr>\n<td>23,000,000 x 38,000</td>\n<td>51,000,000</td>\n<td>0.2</td>\n<td>10</td>\n</tr>\n<tr>\n<td>63,000,000 x 49,000</td>\n<td>440,000,000</td>\n<td>1</td>\n<td>50</td>\n</tr>\n<tr>\n<td>94,000,000 x 4,000</td>\n<td>1,600,000,000</td>\n<td>0.5</td>\n<td>50</td>\n</tr>\n</tbody>\n</table>\nApart from being fast, SVD is also easy to run. Here is a <a href=\"https://gist.github.com/vrilleup/9e0613175fab101ac7cd\">code snippet</a> showing how to run it on sparse data loaded from a text file.</td></tr><tr><td>List(Scott Walent)</td><td>List(Company Blog, Events)</td><td>List(2014-07-23, 2014-07-23, UTC)</td><td>From June 30 to July 2, 2014 we held the <a href=\"http://spark-summit.org/2014\">second Spark Summit</a>, a conference focused on promoting the adoption and growth of <a href=\"http://spark.apache.org\">Apache Spark</a>. This was an exciting year for the Spark community and we are proud to share some highlights.\n<ul>\n\t<li>1,164 participants from over 453 companies attended</li>\n\t<li>Spark Training sold out at 300 participants</li>\n\t<li>31 organizations sponsored the event</li>\n\t<li>12 keynotes and 52 community presentations were given</li>\n</ul>\n&nbsp;\n\nVideos and slides from all presentations are now available on the <a href=\"http://spark-summit.org/2014/agenda\">Summit 2014 agenda</a> page. Some highlights include:\n<ul>\n\t<li>Spark Summit <a href=\"https://www.youtube.com/watch?v=lO7LhVZrNwA&amp;index=2&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr\">keynote from Databricks CEO Ion Stoica</a> introducing <a href=\"http://www.databricks.com/cloud\">Databricks Cloud</a></li>\n\t<li>Open source community updates by <a href=\"https://www.youtube.com/watch?v=e-Ys-2uVxM0&amp;index=2&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr\">Matei Zaharia</a> and <a href=\"https://www.youtube.com/watch?v=2iXQnTVgHuw&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV&amp;index=3\">Patrick Wendell</a></li>\n\t<li>Keynotes from <a href=\"https://www.youtube.com/watch?v=KuFaBJiFzmI&amp;index=2&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV\">SAP</a>, <a href=\"https://www.youtube.com/watch?v=8kcdwnbHnJo&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV&amp;index=2\">Cloudera</a>, <a href=\"https://www.youtube.com/watch?v=tznIN_mUcR8&amp;index=12&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV\">MapR</a> and <a href=\"https://www.youtube.com/watch?v=3qrFAcNQGHY&amp;index=4&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr\">Datastax</a></li>\n\t<li>Application talks including how Spark is used in <a href=\"https://www.youtube.com/watch?v=Gg_5fWllfgA&amp;index=5&amp;list=PL-x35fyliRwjCR-gDhk1ekG4jh2ltgKSV\">neuroscience</a>, <a href=\"https://www.youtube.com/watch?v=I6qmEcGNgDo&amp;list=PL-x35fyliRwgNYIV1P9prgtKyiMrTKp5k&amp;index=2\">image processing</a> and <a href=\"https://www.youtube.com/watch?v=RwyEEMw-NR8&amp;list=PL-x35fyliRwiST9gF7Z8Nu3LgJDFRuwfr&amp;index=6\">genomics</a></li>\n\t<li>Talks from the core developers on components including <a href=\"https://www.youtube.com/watch?v=7WwwLkRs3-Y&amp;list=PL-x35fyliRwiDhtOvRSNgMdw05xFMzvhU&amp;index=2\">Spark SQL</a>, <a href=\"https://www.youtube.com/watch?v=SF4Xv2bvZW0&amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x&amp;index=7\">MLlib</a>, <a href=\"https://www.youtube.com/watch?v=MFSUAkDBSdQ&amp;index=7&amp;list=PL-x35fyliRwiDhtOvRSNgMdw05xFMzvhU\">JSON support</a> and <a href=\"https://www.youtube.com/watch?v=CUX1SG9zTkU&amp;index=2&amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x\">SparkR</a></li>\n</ul>\nAdditionally, we have posted the training videos, slides and all hands-on exercises for free on the <a href=\"http://spark-summit.org/2014/training\">Databricks Spark Training page</a>.\n<h3>Spark Summit comes to NYC in 2015</h3>\n<a href=\"http://spark-summit.org/east/2015\">Spark Summit East</a> will debut in New York City in early 2015. If you would like to be notified when tickets are on sale, please <a href=\"https://docs.google.com/forms/d/1xrVXJfzalGzol0eGONd2zr9zG7jSCJwUBPquviB-SkI/viewform?usp=send_form\">pre-register here</a>. While we are excited to expand into another city, we are actively planning for our third Spark Summit in San Francisco. Make sure to <a href=\"https://docs.google.com/forms/d/1OgZx4uCwElR0Y9UkPlowgxtS4MRLzzehgQ7pWDJAhZo/viewform?usp=send_form\">pre-register here</a> to get updates about the event.\n<h3 id=\"other-summit-resources\">Other Summit Resources</h3>\n<ul>\n\t<li>Follow the <a href=\"http://twitter.com/spark_summit\">@spark_summit</a> twitter handle</li>\n\t<li>Like the <a href=\"http://facebook.com/ApacheSparkSummit\">Summit Facebook page</a></li>\n</ul></td></tr><tr><td>List(Oscar Mendez (CEO of Stratio))</td><td>List(Company Blog, Partners)</td><td>List(2014-08-08, 2014-08-08, UTC)</td><td><div class=\"post-meta\">This is a guest post from our friends at <a href=\"http://www.stratio.com\" target=\"_blank\">Stratio</a> announcing that their platform is now a \"Certified Apache Spark Distribution\".</div>\n\n<hr />\n\n<h2>Certified distribution</h2>\nStratio is delighted to announce that it is officially a Certified Apache Spark Distribution. The certification is very important for us because we deeply believe that the certification program provides many benefits to the Spark community: It facilitates collaboration and integration, offers broad evolution and support for the rich Spark ecosystem, simplifies adoption of critical security updates and allows development of applications valid for any certified distribution - a key ingredient for a successful ecosystem.\n<!--more-->\nThis post is a brief history of how we started with big data technologies until we made the shift to Spark.\n<h2>When Stratio met Spark: A true love story</h2>\nWe started using Big Data technologies more than 7 years ago, when Hadoop was still in Beta. We did not start using Big Data technologies because we were so smart as to predict it was going to be the future of data, it was just chance and necessity.\n\nWe had a product for ORM (online reputation management). This product was collecting Internet comments about more than 400 hundred companies. After collecting all these comments we were processing them with our semantic engine, to make reports and to send alerts to our clients. With Web 2.0 the information in the Internet started to grow exponentially; all the blogs were generating millions of posts and comments. Thus, our semantic engine was taking longer to process each day, until we were not able to send the reports before 9am. We discussed ways of solving the problem, e.g., to increase the number of servers as well as other solutions. As you have already imagined, one of the solutions was to use a very incipient technology called Hadoop to optimize our semantic engine processing. We were young and brave (we still are) and we followed that path. It was really hard; Hadoop Beta had a lot of bugs. After several months, we were able to implement a MapReduce style program for our semantic engine and make our first test. The results were impressive; we shortened the time needed to process all the comments from 12 hours to less than 30 minutes. It was awesome; of course we knew that the semantic processing was pretty “map reducible”, but it blew our minds anyway. That was the moment we fell in love with Big Data technology.\n\nBy 2013 we had been developing Big Data projects for more than 6 years. Those were the times when we were implementing Nathan Marz Lambda architectures combining Hadoop and Storm. We achieved fantastic goals, and our clients were impressed with the results. But we also found some limitations: there were no interactive queries nor real time data streaming analysis. And the projects were becoming more complex to develop, deploy and to support. So we were looking for a better way and a technology to serve our clients, and hence we found Spark.\n<h2>Spark</h2>\nWe started using Spark during 2013. The in-memory processing and the elegance of the architecture were able to provide incredible power and possibilities with maximum simplicity - it was just awesome. We decided to incorporate Spark in our platform before it became an Apache Incubator project, because we saw that the concepts behind it, and the improvements and possibilities were so huge that we did not have any doubt that it was the new “Hadoop Map Reduce” engine. During 2013, Spark streaming was added to Spark, and in 2014 Spark became an Apache Top Level project, so time has proven us right. In fact, we did not incorporate Spark keeping Hadoop Map Reduce; we completely replaced Hadoop Map Reduce, creating a Pure Spark Platform.\n<h2>Stratio: A pure Spark enterprise platform</h2>\nTo include spark in our platform so early was a bit risky, but to become such early adopters has had a big return for us. We have been able to create a pure spark enterprise platform in record time and our first version was launched at the end of March.\n\nThanks to Spark and our Pure Spark approach, our Enterprise Platform was leaner and simpler than former platforms:\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/STRATIO_presentacion2_admin.png\" alt=\"\" align=\"middle\" />\n\nWe have integrated some of the Hadoop tools such as Flume, and also created some modules that were needed in order to have a real Enterprise ready Spark platform.\n<h2>Admin</h2>\nThe first one was our admin module. “Admin” is responsible for:\n<ul>\n \t<li>Installation, Full deployment on-premise and cloud</li>\n \t<li>Platform management and monitoring</li>\n \t<li>Security</li>\n \t<li>System Dashboards and Reporting</li>\n \t<li>System alerts</li>\n</ul>\n<h2>CrossData</h2>\nAfter 7 years doing Big Data projects, we have seen once again how difficult it was for the clients to use Big Data technology, so we tried to simplify the use of Big data technology with three main objectives in mind:\n<ul>\n \t<li>Allow the clients to use the system just using SQL. Nothing else needed.</li>\n \t<li>Combine the best processing engine with the best NoSQL Databases, to leverage the benefits of both worlds.</li>\n \t<li>Allow combination: Different storage systems ( HDFS, MongoDB, Elastic Search…) or data stored with data streaming entering in real time into the system (past data with current data)</li>\n</ul>\nIn order to achieve the goals described above we created “CrossData”:\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/Spark-hub-06-06.png\" alt=\"\" align=\"middle\" />\n\n“CrossData” is not only an easy SQL interface; it combines data, and also uses Spark to complement the NoSQL Databases with features not implemented in their APIs, e.g., make SQL joint in MongoDB or to allow any other before “impossible” sentences.\n<h2>Spark Streaming and Stratio Streaming</h2>\nWe have been using Spark streaming from the very beginning. In fact we were replacing Storm in some of the projects we developed before Spark Streaming was launched.\n\nWe at Stratio think that Spark Streaming is the perfect tool to build a powerful solution for interactive complex event processing. Therefore we have combined Spark Streaming with complex engine processing (CEP) and Kafka creating Stratio Streaming:\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/Imagen5.png\" alt=\"\" align=\"middle\" />\n<h2>Use cases</h2>\nWe only use Spark for processing, and not just in POCs but for real projects for big companies. Comparing the projects we were doing with former technologies with the ones we are doing now with Spark, we can point out several benefits:\n<ul>\n \t<li><strong>Developers</strong>: Easier and faster development</li>\n \t<li><strong>System Engineers</strong>: Easier deployment and cost reduction support</li>\n \t<li><strong>Clients and Business</strong>: More value with less complexity</li>\n</ul>\nWe have measured the improvement of using Spark against the previous Big Data platforms for some of our Banking Clients, and Spark has been up to 20X faster, reducing hours of processing down to minutes. Here are some examples of real use cases of Spark and the Stratio platform:\n<ul>\n \t<li><strong>NH Hotels</strong>: They wanted to aggregate customer satisfaction data and guests’ reviews from social networks and combine them with financial data. Using the Stratio platform they were able to manage about 200K reviews per year. The Quality Focus Online tool is used by more than 400 hotel directors world-wide and the 15% of the variable income of employees depends on its measurements. Additionally, they have considerably reduced their negative reviews by using this tool.</li>\n \t<li><strong>Telefonica</strong>: The cyber security group has the necessity to analyze all their logs in order to detect or even prevent possible hacking attacks. After checking many of the existing technologies they decide to use the Stratio platform for such a task. Currently they can take profit of all the available information coming from different sources (access logs, DNS, email, reviews, etc) by detecting and resolving possible security weaknesses or exploits and generate exhaustive informs.</li>\n</ul>\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 85%; display: block; margin: 30px auto 20px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/08/Logos.png\" alt=\"\" align=\"middle\" />\n<h2>The future</h2>\nWe see Spark as the evolution of Hadoop Map Reduce, extended and with none of the previous limitations, so we are just at the beginning of a revolution for Big Data processing. And it is only improving with new modules and possibilities every year that will make products and projects which were once impossible with former technologies a reality. So stay connected because this adventure has just started.</td></tr><tr><td>List(Andy Huang (Alibaba Taobao Data Mining Team), Wei Wu (Alibaba Taobao Data Mining Team))</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-08-15, 2014-08-15, UTC)</td><td><div class=\"post-meta\">This is a guest blog post from our friends at Alibaba Taobao.</div>\n\n<hr />\n\nAlibaba Taobao operates one of the world’s largest e-commerce platforms. We collect hundreds of petabytes of data on this platform and use Apache Spark to analyze these enormous amounts of data. Alibaba Taobao probably runs some of the largest Spark jobs in the world. For example, some Spark jobs run for weeks to perform feature extraction on petabytes of image data. In this blog post, we share our experience with Spark and GraphX from prototype to production at the Alibaba Taobao Data Mining Team.\n<!--more-->\n\nEvery day, hundreds of millions of users and merchants interact on Alibaba Taobao’s marketplace. These interactions can be expressed as complicated, large scale graphs. Mining data requires a distributed data processing engine that can support fast interactive queries as well as sophisticated algorithms.\n\nSpark and GraphX embed a standard set of graph mining algorithms, including PageRank, triangle counting, connected components, shortest path. The implementation of these algorithms focuses on reusability. Users can implement variants of these algorithms in order to exploit performance optimization opportunities for specific workloads. In our experience, the best way to learn GraphX is to read and understand the source code of these algorithms.\n\nAlibaba Taobao started prototyping with GraphX in Apache Spark 0.9 and went into production in May 2014 around the time that Apache Spark 1.0 was released.\n\nOne thing to note is that GraphX is still evolving quickly. Although the user-facing APIs are relatively stable, the internals have seen fairly large refactoring and improvements from 0.8 to 1.0. Based on our experience, each minor version upgrade provided 10 - 20% performance improvements even without modifying our application code.\n<h2>Graph Inspection Platform</h2>\nGraph-based structures model the many relationships between our users and items in our store. Our business and product teams constantly need to make decisions based on the value and health of each relationship. Before Spark, they used their intuition to estimate such properties, resulting in decisions which were not a good fit with reality. To solve this problem, we developed a platform to scientifically quantify all these metrics in order to provide evidence and insights for product decisions.\n\nThis platform requires constantly re-iterating the set of metrics it provides to users, depending on product demand. The interactive nature of both Spark and GraphX proves very valuable in building this platform. Some of the metrics this platform measures are:\n\n<b>Degree Distribution</b>: Degree distribution measures the distribution of vertex degrees (e.g. how many users have 50 friends). It also provides valuable information on the number of high degree vertices (so-called super vertices). Often our downstream product infrastructure needs to accommodate super vertices in a special manner (because they have a high impact on propagation algorithms), and thus it is crucial to understand their distribution among our data. GraphX’s VertexRDD provides built-in support for both in-degrees and out-degrees.\n\n<b>Second Degree Neighbors</b>: Modeling social relationships often requires measuring the second-degree neighbor distribution. For example, in an instant messaging platform we developed, the number of “retweets” correlates with the number of second degree neighbors (e.g. number of friends of friends). While GraphX does not yet provide built-in support for counting second degree neighbors, we implemented it using two rounds of propagations: the first round propagates each vertex’s ID to its neighbors, and the second round propagates all IDs from neighbors to second degree neighbors. After the two rounds of propagations, each vertex calculates the number of second degree neighbors using a hash set.\n\nOne thing to note in this calculation is that we use the aforementioned degree distribution to remove super vertices from the second degree neighbor calculation. Otherwise, these super vertices would create too many messages, leading to high computation skew and high memory usage.\n\n<b>Connected Components</b>: Connected components refer to some set of subgraphs that are “connected”, i.e. there exists a path connecting any pair of vertices in the subgraph. Connected component is very useful in dividing a large graph into multiple, smaller graphs, and then operations that are computationally too expensive to run on the large graph. This algorithm can also be adapted to discover tightly connected networks.\n\nWe are developing more metrics using both built-in functions provided by Spark and GraphX, as well as new ones implemented internally. This platform nurtures a new culture such that our product decisions are no longer based on instinct and intuition, but rather on metrics mined from data.\n<h2>Multi-graph Merging</h2>\nThe Graph Inspection Platform provides us with different properties for modeling relationships. Each relationship structure has its own strengths and weaknesses. For example, some relationship structure provides more valuable information in connected components, while another other structure might work better for interactions. We often make decisions based on multiple different properties and structural representations of the same underlying graph. Based on GraphX, we developed a multi-graph merging framework that creates “intersections” of multiple graphs.\n\n<img class=\"alignnone size-full wp-image-1173\" src=\"https://databricks.com/wp-content/uploads/2014/08/image00.png\" alt=\"image00\" width=\"578\" height=\"196\" />\n\nThe attached figure illustrates the algorithm to merge graph A and graph B to create graph C: edges are created in graph C if any of its vertices exist in graph A or graph B.\n\nThis merging framework was implemented using the outerJoinVertices operator provided by GraphX. In addition to naively merging two graphs, the framework can also assign different weights to the input graphs. In practice, our analysis pipelines often merge multiple graphs in a variety of ways and run them on the Graph Inspection Platform.\n<h2>Belief Propagation</h2>\nWeighted belief propagation is a classic way of modeling graph data, often used to predict a user’s influence or credibility. The intuition is simple: highly credited users often interact with other highly credited users, while lower credited users often interact with other lower credited users. Although the algorithm is simple, historically we did not attempt to run these on our entire graph due to the computation cost to scale this to hundreds of millions of users and billions of interactions. Using GraphX, we are able to scale this analysis to the entire graphs we have.\n\nEach run of the algorithm requires 3 iterations, and each iteration requires 8 iterations in GraphX’s Pregel API. After a total of 30 iterations in Pregel, the AUC (area under the curve) increased from 0.6 to 0.9, which is a very satisfactory prediction rate.\n\nWhile we are still in the early stages of our journey with GraphX, already today we have been able to generate impressive insights with graph modeling and analysis that would have been very hard to accomplish at our scale without GraphX. We plan to enrich and further develop our various platforms and frameworks to include an even wider array of metrics and apply them to tag/topic inference, demographics inference, transaction prediction, which will in turn improve our various recommendation systems’ effectiveness.\n<h2>Authors</h2>\nAndy Huang leads the data mining team at Taobao. He is a very early adopter of Spark, using it production since Spark 0.5.\n\nWei Wu is an engineer at Taobao’s data mining team. His interests span distributed systems, large-scale machine learning and data mining.\n\nThis guest blog post is a translation of part of an article published by <a href=\"http://www.csdn.net/article/2014-08-07/2821097\">CSDN Programmer Magazine</a>.</td></tr><tr><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-08-27, 2014-08-27, UTC)</td><td>One of our philosophies in Apache Spark is to provide rich and friendly built-in libraries so that users can easily assemble data pipelines. With Spark, and MLlib in particular, quickly gaining traction among data scientists and machine learning practitioners, we’re observing a growing demand for data analysis support outside of model fitting. To address this need, we have started to add scalable implementations of common statistical functions to facilitate various components of a data pipeline. <!--more-->We’re pleased to announce Apache Spark 1.1. ships with built-in support for several statistical algorithms common in exploratory data pipelines:\n<ul>\n \t<li><strong>correlations</strong>: data dependence analysis</li>\n \t<li><strong>hypothesis testing</strong>: goodness of fit; independence test</li>\n \t<li><strong>stratified sampling</strong>: scaling training set with controlled label distribution</li>\n \t<li><strong>random data generation</strong>: randomized algorithms; performance tests</li>\n</ul>\nAs ease of use is one of the main missions of Spark, we’ve devoted a great deal of effort to API designing for the statistical functions. Spark's statistics APIs borrow inspirations from widely adopted statistical packages, such as R and SciPy.stats, which are shown in a recent O’Reilly survey to be the most popular tools among data scientists.\n<h2>Correlations</h2>\n<a href=\"http://en.wikipedia.org/wiki/Correlation\" target=\"_blank\">Correlations</a> provide quantitative measurements of the statistical dependence between two random variables. Implementations for correlations are provided under <span style=\"font-family: monospace; font-size: 80%;\">mllib.stat.Statistics</span>.\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">corr(x, y = None, method = \"pearson\" | \"spearman\")</span></li>\n</ul>\n</td>\n</tr>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">R</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">cor(x, y = NULL, method = c(\"pearson\", \"kendall\", \"spearman\"))</span></li>\n</ul>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">SciPy</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">pearsonr(x, y)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">spearmanr(a, b = None)</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nAs shown in the table, R and SciPy.stats present us with two very different directions for correlations API in MLlib. We ultimately converged on the R style of having a single function that takes in the correlation method name as a string argument out of considerations for extensibility as well as conciseness of the API list. The initial set of methods contains \"pearson\" and \"spearman\", the two most commonly used correlations.\n<h2>Hypothesis testing</h2>\n<a href=\"http://en.wikipedia.org/wiki/Hypothesis_testing\" target=\"_blank\">Hypothesis testing</a> is essential for data-driven applications. A test result shows the statistical significance of an event unlikely to have occurred by chance. For example, we can test whether there is a significant association between two samples via independence tests. In Apache Spark 1.1, we implemented chi-squared tests for goodness-of-fit and independence:\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chiSqTest(observed: Vector, expected: Vector)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chiSqTest(observed: Matrix)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chiSqTest(data: RDD[LabeledPoint])</span></li>\n</ul>\n</td>\n</tr>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">R</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE)</span></li>\n</ul>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">SciPy</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">chisquare(f_obs, f_exp = None, ddof = 0, axis = 0)</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nWhen designing the chi-squared test APIs, we took the greatest common denominator of the parameters in R’s and SciPy’s APIs and edited out some of the less frequently used parameters for API simplicity. Note that as with R and SciPy, the input data types determine whether the goodness of fit or the independence test is conducted. We added a special case support for input type <span style=\"font-family: monospace; font-size: 80%;\">RDD[LabeledPoint]</span> to enable feature selection via chi-squared independence tests.\n<h2>Stratified sampling</h2>\nIt is common for a large population to consist of various-sized subpopulations (strata), for example, a training set with many more positive instances than negatives. To sample such populations, it is advantageous to sample each stratum independently to reduce the total variance or to represent small but important strata. This sampling design is called <a href=\"http://en.wikipedia.org/wiki/Stratified_sampling\" target=\"_blank\">stratified sampling</a>. Unlike the other statistics functions, which reside in MLlib, we placed the stratified sampling methods in Spark Core, as sampling is widely used in data analysis. We provide two versions of stratified sampling, <span style=\"font-family: monospace; font-size: 80%;\">sampleByKey</span> and <span style=\"font-family: monospace; font-size: 80%;\">sampleByKeyExact</span>. Both apply to an RDD of key-value pairs with key indicating the stratum, and both take a map from users that specifies the sampling probability for each stratum. Neither R nor SciPy provides built-in support for stratified sampling.\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr>\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">sampleByKey(withReplacement, fractions, seed)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%;\">sampleByKeyExact(withReplacement, fractions, seed)</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nSimilar to <span style=\"font-family: monospace; font-size: 80%;\">RDD.sample</span>, <span style=\"font-family: monospace; font-size: 80%;\">sampleByKey</span> applies Bernoulli sampling or Poisson sampling to each item independently, which is cheap but doesn’t guarantee the exact sample size for each stratum (the size of the stratum times the corresponding sampling probability). <span style=\"font-family: monospace; font-size: 80%;\">sampleByKeyExact</span> utilizes scalable sampling algorithms that guarantee the exact sample size for each stratum with high probability, but it requires multiple passes over the data. We have a separate method name to underline the fact that it is significantly more expensive.\n<h2>Random data generation</h2>\nRandom data generation is useful for testing of existing algorithms and implementing randomized algorithms, such as random projection. We provide methods under <span style=\"font-family: monospace; font-size: 80%;\">mllib.random.RandomRDDs</span> for generating RDDs that contains i.i.d. values drawn from a distribution, e.g., uniform, standard normal, or Poisson.\n<table cellspacing=\"5\" cellpadding=\"5\" align=\"center\">\n<thead>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">MLlib</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">normalRDD(sc, size, [numPartitions, seed])</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">normalVectorRDD(sc, numRows, numCols, [numPartitions, seed])</span></li>\n</ul>\n</td>\n</tr>\n<tr style=\"border-bottom-style: solid; border-bottom-width: 1px; border-color: silver;\">\n<td style=\"text-align: center;\">R</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">rnorm(n, mean=0, sd=1)</span></li>\n</ul>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">SciPy</td>\n<td style=\"text-align: left;\">\n<ul>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">randn(d0, d1, ..., dn)</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">normal([loc, scale, size])</span></li>\n \t<li><span style=\"color: #757575; font-family: monospace; font-size: 80%; line-height: 1;\">standard_normal([size])</span></li>\n</ul>\n</td>\n</tr>\n</thead>\n</table>\nThe random data generation APIs exemplify a case where we added Spark-specific customizations to a commonly supported API. We show a side-by-side comparison of MLlib’s normal distribution data generation APIs vs. those of R and SciPy in the table above. We provide 1D (<span style=\"font-family: monospace; font-size: 80%;\">RDD[Double]</span>) and 2D (<span style=\"font-family: monospace; font-size: 80%;\">RDD[Vector]</span>) support, compared to only 1D in R and arbitrary dimension in NumPy, since both are prevalent in MLlib functions. In addition to the Spark specific parameters, such as SparkContext and number of partitions, we also allow users to set the seed for reproducibility. Apart from the built-in distributions, users can plug in their own via <span style=\"font-family: monospace; font-size: 80%;\">RandomDataGenerator</span>.\n<h2>What about SparkR?</h2>\nAt this point you may be asking why we’re providing native support for statistics functions inside Spark given the existence of the SparkR project. As an R package, SparkR is a great lightweight solution for empowering familiar R APIs with distributed computation support. What we’re aiming to accomplish with these built-in Spark statistics APIs is cross language support as well as seamless integration with other components of Spark, such as Spark SQL and Streaming, for a unified data product development platform. We expect these features to be callable from SparkR in the future.\n<h2>Concluding remarks</h2>\nBesides a set of familiar APIs, statistics functionality in Spark also brings R and SciPy users huge benefits including scalability, fault tolerance, and seamless integration with existing big data pipelines. Both R and SciPy run on a single machine, while Spark can easily scale up to hundreds of machines and distribute the computation. We compared the running times of MLlib’s Pearson’s correlation on a 32-node cluster with R’s, not counting the time needed for moving data to the node with R installed. The results shown below demonstrates a clear advantage of Spark over R on performance and scalability.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/08/Spark-vs-R-pearson.png\"><img class=\"aligncenter size-full wp-image-1239\" src=\"https://databricks.com/wp-content/uploads/2014/08/Spark-vs-R-pearson.png\" alt=\"Spark-vs-R-pearson\" width=\"400\" /></a></p>\nAs the Statistics APIs are experimental, we’d love feedback from the community on the usability of these designs. We also welcome contributions from the community to enhance statistics functionality in Spark.</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2014-09-12, 2014-09-12, UTC)</td><td>Today we’re thrilled to announce the release of Apache Spark 1.1! Apache Spark 1.1 introduces many new features along with scale and stability improvements. This post will introduce some key features of Apache Spark 1.1 and provide context on the priorities of Spark for this and the next release.<!--more--> In the next two weeks, we’ll be publishing blog posts with more details on feature additions in each of the major components. Apache Spark 1.1 is already available to Databricks customers and has also been posted today on the <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">Apache Spark website</a>.\n\n<!--more-->\n<h2>Maturity of SparkSQL</h2>\nThe 1.1 released upgrades Spark SQL significantly from the preview delivered in Apache Spark 1.0. At Databricks, we’ve migrated all of our customer workloads from Shark to Spark SQL, with between 2X and 5X <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">performance improvements</a> across the board. Apache Spark 1.1 adds a JDBC server for Spark SQL, a key feature allowing direct upgrade of Shark installations which relied on JDBC. We’ve also opened up the Spark SQL type system with a public types API, allowing for rich integration with third party data sources. This will provide an extension point for many future integrations, such as the Datastax Cassandra driver. Using this types API, we’ve added turn-key support for loading JSON data into Spark's native ShemaRDD format:\n\n[python]\n# Create a JSON RDD in Python\n&gt;&gt;&gt; people = sqlContext.jsonFile(“s3n://path/to/files...”)\n# Visualize the inferred schema\n&gt;&gt;&gt; people.printSchema()\n# root\n#  |-- age: integer (nullable = true)\n#  |-- name: string (nullable = true)\n[/python]\n\n<h2>Expansion of MLlib</h2>\nSpark’s machine learning library adds several new algorithms, including a library for <a href=\"https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html\">standard exploratory statistics</a> such as sampling, correlations, chi-squared tests, and randomized inputs. This allows data scientists to avoid exporting data to single-node systems (R, SciPy, etc) and instead directly operate on large scale datasets in Spark. Optimizations to internal primitives provide a 2-5X performance improvement in most MLlib algorithms out of the box. Decision trees, a popular algorithm, has been ported to Java and Python. Several other algorithms have also been added, including TF-IDF, SVD via Lanczos, and nonnegative matrix factorization. The next release of MLlib will introduce an enhanced API for end-to-end machine learning pipelines.\n<h2>Sources and Libraries for Spark Streaming</h2>\nSpark streaming extends its library of ingestion sources in this release adding two new sources. The first is support for Amazon Kinesis, a hosted stream processing engine. Spark Streaming also adds H/A source for Apache Flume using a new data source which provides transactional hand-off of events from Flume to gracefully tolerate worker failures. Apache Spark 1.1 adds the first of a set of online machine learning algorithms with the introduction of a streaming linear regression. Looking forward, the Spark Streaming roadmap will feature a general recoverability mechanism for all input sources, along with an ever-growing list of connectors. The example below shows training a linear model using incoming data, then using an updated model to make a prediction:\n\n[scala]\n &gt; val stream = KafkaUtils.createStream(...)\n\n // Train a linear model on a data stream\n &gt; val model = new StreamingLinearRegressionWithSGD()\n   .setStepSize(0.5)\n   .setNumIterations(10)\n   .setInitialWeights(Vectors.dense(...))\n   .trainOn(DStream.map(record =&gt; createLabeledPoint(record))\n\n // Predict using the latest updated model\n &gt; model.latestModel().predict(myDataset)\n[/scala]\n\n<h2>Performance in Spark Core</h2>\nThis release adds significant internal changes to Spark focused on improving performance for large scale workloads. Apache Spark 1.1 features a new implementation of the Spark shuffle, a key internal primitive used by almost all data-intensive programs. The new shuffle improves performance by more than 5X for workloads with extremely high degree of parallelism, a key pain point in earlier versions of Spark. Apache Spark 1.1 also adds a variety of other improvements to decrease memory usage and improve performance.\n<h2>Optimizations and Features in PySpark</h2>\nSeveral of the disk-spilling modifications introduced in Apache Spark 1.0 have been ported to Spark’s Python runtime extension. This release also adds support in Python for reading and writing data from SequenceFiles, Avro, and other Hadoop-based input formats. PySpark now supports the entire Spark SQL API, including support for nested types inside of SchemaRDD’s.\n\nThe efforts on improving scale and robustness of Spark and PySpark are based on feedback from the community along with direct interactions with our customer workloads at Databricks. The next release of Spark will continue along this theme, with a focus on improving instrumentation and debugging for users to pinpoint performance bottlenecks.\n\nThis post only scratches the surface of interesting features in Apache Spark 1.1. Head on over to the <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">official release notes</a> to learn more about this release and stay tuned to hear more about Apache Spark 1.1 from Databricks over the coming days!</td></tr><tr><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2014-09-16, 2014-09-16, UTC)</td><td>With Apache Spark 1.1 recently released, we’d like to take this occasion to feature one of the most popular Spark components - Spark Streaming - and highlight who is using Spark Streaming and why.\n\nApache Spark 1.1. adds several new features to Spark Streaming.  In particular, Spark Streaming extends its library of ingestion sources to include Amazon Kinesis, a hosted stream processing engine, as well as to provide high availability for Apache Flume sources.  Moreover, Apache Spark 1.1 adds the first of a set of online machine learning algorithms with the introduction of a streaming linear regression.\n\nMany organizations have evolved from exploratory, discovery use cases of big data to use cases that require reasoning on data as it arrives in order to make decisions in real time.  Spark Streaming enables this category of high-value use cases, providing a system for processing fast and large streams of data in real time.\n\n<b>What is it?</b>\n\nSpark Streaming is an extension of the core Spark API that enables high-throughput, reliable processing of live data streams. Spark Streaming ingests data from any source including Amazon Kinesis, Kafka, Flume, Twitter and file systems such as S3 and HDFS.  Users can express sophisticated algorithms easily using high-level functions to process the data streams.  The core innovation behind Spark Streaming is to treat streaming computations as a series of deterministic micro-batch computations on small time intervals, executed using Spark's distributed data processing framework.  Micro-batching unifies the programming model of streaming with that of batch use cases and enables strong fault recovery guarantees while retaining high performance.  The processed data can then be stored in any file system (including HDFS), database (including Hbase), or live dashboards.\n\n<b>Where is it being used?</b>\n\nSpark Streaming has seen a significant uptake in adoption in the past year as enterprises increasingly use it as part of Spark deployments.  The Databricks team is aware of more than 40 organizations that have deployed Spark Streaming in production.\n\nJust as impressive is the breadth of industries across which Spark Streaming is being used.  For instance:\n<ul>\n \t<li>A leading software vendor leverages Spark Streaming to power its <b>real-time supply chain analytics platform</b>, which provides more than 1,000 supplier performance metrics to users via dashboards and detailed operational reports.</li>\n \t<li>A large <b>hardware vendor</b> is using Spark Streaming for security intelligence operations, notably a first check of known threats.</li>\n \t<li>A leading <b>advertising</b> technology firm is processing click stream data for its real-time ad auction platform, leading to more accurate targeting of display advertising, better consumer engagement and higher conversion.</li>\n \t<li>A global <b>telecommunications</b> provider is collecting metrics from millions of mobile phones and analyzing them to determine where to place new cell phone towers and upgrade aging infrastructure, resulting in improved service quality.</li>\n \t<li>A pre-eminent video analytics provider is using Spark Streaming to help <b>broadcasters and media companies</b> monetize video with personalized, interactive experiences for every screen.</li>\n</ul>\nWe’ve seen Spark Streaming benefit many parts of an organization, as the following examples illustrate:\n<ul>\n \t<li><b>Marketing &amp; Sales</b>:  Analysis of customer engagement and conversion, powering real-time recommendations while customers are still on the site or in the store.</li>\n \t<li><b>Customer Service &amp; Billing</b>:  Analysis of contact center interactions, enabling accurate remote troubleshooting before expensive field technicians are dispatched.</li>\n \t<li><b>Manufacturing</b>: Real-time, adaptive analysis of machine data (e.g., sensors, control parameters, alarms, notifications, maintenance logs, and imaging results) from industrial systems (e.g., equipment, plant, fleet) for visibility into asset health, proactive maintenance planning, and optimized operations.</li>\n \t<li><b>Information Technology</b>:  Log processing to detect unusual events occurring in streams of data, empowering IT to take remedial action before service quality degrades.</li>\n \t<li><b>Risk Management</b>:  Anomaly detection and root cause forensics, which sometimes makes it possible to stop fraud while it happens.</li>\n</ul>\n<b>Why is it being used?</b>\n\nThe reasons enterprises give for adopting (and in many cases transitioning to) Spark Streaming often start with the advantages that Spark itself brings.  All the strengths of Spark’s unified programming model apply to Spark Streaming, which is particularly relevant for real-time analytics that combine historical data with fresh data:\n<ul>\n \t<li><b>Making data science accessible to non-scientists</b>:  Spark’s declarative APIs enable users who have domain expertise but lack data science expertise to express a business problem and its associated processing algorithm and data pipeline using simple high-level operators.</li>\n \t<li><b>Higher productivity for data workers</b>:  Spark’s write-once-run-anywhere approach unifies batch and stream processing.  In fact, Spark ties together the different parts of an analytics pipeline in the same tool, such as discovery, ETL, data engineering, machine learning model training and execution, across all types of structured and unstructured data.</li>\n \t<li><b>Exactly-once semantics: </b> Many business critical use cases have a need for exactly-once stateful processing, not at-most-once (which includes zero) or at-least-once (which includes duplicates).  Exactly-once provides users with certainty on questions such as the exact number of frauds, emergencies or outages occurring today.</li>\n \t<li><b>No compromises on scalability and throughput</b>.  Spark Streaming is designed for hyper-scale environments and combines statefulness and persistence with high throughput.</li>\n \t<li><b>Ease of operations</b>:  Spark provides a unified run time across different processing engines.  One physical cluster and one set of operational processes covers the full spectrum of use cases.</li>\n</ul>\nWe have also learned from the community that the high throughput that Spark Streaming provides is just as important as latency.  In fact, latency of a few hundred milliseconds is sufficient for the vast majority of streaming use cases.  Rare exceptions include algorithmic trading.\n\nOne capability that allows Spark Streaming to be deployed in such a wide variety of situations is that users have a choice of three resource managers:  Full integration with YARN and Mesos as well as the ability to rely on Spark’s easy-to-use stand-alone resource manager.  Moreover, Spark and Spark Streaming are supported already by leading vendors such as Cloudera, MapR and Datastax.  We expect other vendors will include and support Spark in their Hadoop distributions in the near future.\n\nPlease stay tuned for future posts on Spark Streaming technical design patterns and practical use cases.</td></tr><tr><td>List(Burak Yavuz, Xiangrui Meng)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-09-22, 2014-09-22, UTC)</td><td>With an ever-growing community, Apache Spark has had it’s <a href=\"https://databricks.com/blog/2014/09/11/announcing-spark-1-1.html\" target=\"_blank\">1.1 release</a>. MLlib has had its fair share of contributions and now supports many new features. We are excited to share some of the performance improvements observed in MLlib since the 1.0 release, and discuss two key contributing factors: torrent broadcast and tree aggregation.\n<h2>Torrent broadcast</h2>\nThe beauty of Spark as a unified framework is that any improvements made on the core engine come for free in its standard components like MLlib, Spark SQL, Streaming, and GraphX. In Apache Spark 1.1, we changed the default broadcast implementation of Spark from the traditional <code>HttpBroadcast</code> to <code>TorrentBroadcast</code>, a BitTorrent like protocol that evens out the load among the driver and the executors. When an object is broadcasted, the driver divides the serialized object into multiple chunks, and broadcasts the chunks to different executors. Subsequently, executors can fetch chunks individually from other executors that have fetched the chunks previously.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/broadcast.png\"><img class=\"aligncenter size-full wp-image-1399\" src=\"https://databricks.com/wp-content/uploads/2014/09/broadcast.png\" alt=\"broadcast\" width=\"85%\" /></a></p>\nHow does this change in Spark Core affect MLlib’s performance?\n\nA common communication pattern in machine learning algorithms is the one-to-all broadcast of intermediate models at the beginning of each iteration of training. In large-scale machine learning, models are usually huge and broadcasting them via http can make the driver a severe bottleneck because all executors (workers) are fetching the models from the driver. With the new torrent broadcast, this load is shared among executors as well. It leads to significant speedup, and MLlib takes it for free.\n<h2>Tree aggregation</h2>\nSimilar to broadcasting models at the beginning of each iteration, the driver builds new models at the end of each iteration by aggregating partial updates collected from executors. This is the basis of the MapReduce paradigm. One performance issue with the <code>reduce</code> or <code>aggregate</code> functions in Spark (and the original MapReduce) is that the aggregation time scales linearly with respect to the number of partitions of data (due to the CPU cost in merging partial results and the network bandwidth limit).\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/aggregation.png\"><img class=\"aligncenter size-full wp-image-1398\" src=\"https://databricks.com/wp-content/uploads/2014/09/aggregation.png\" alt=\"aggregation\" width=\"85%\" /></a></p>\nIn MLlib 1.1, we introduced a new aggregation communication pattern based on multi-level aggregation trees. In this setup, model updates are combined partially on a small set of executors before they are sent to the driver, which dramatically reduces the load the driver has to deal with. Tests showed that these functions reduce the aggregation time by an order of magnitude, especially on datasets with a large number of partitions.\n<h2>Performance improvements</h2>\nChanging the way models are broadcasted and aggregated has a huge impact on performance. Below, we present empirical results comparing the performance on some of the common machine learning algorithms in MLlib. The x-axis can be thought of the speedup the 1.1 release has over the 1.0 release. Speedups between 1.5-5x can be observed across all algorithms. The tests were performed on an EC2 cluster with 16 slaves, using m3.2xlarge instances. The scripts to run the tests are a part of the “spark-perf” test suite which can be found on <a href=\"https://github.com/databricks/spark-perf\" target=\"_blank\">https://github.com/databricks/spark-perf</a>.\n<p style=\"text-align: center; font-size: 75%;\" align=\"center\"><img class=\"wp-image-1412 size-full\" src=\"https://databricks.com/wp-content/uploads/2014/09/mllib-perf-test.png\" alt=\"mllib-perf-test\" width=\"85%\" />\nFor ridge regression and logistic regression, the Tall identifier corresponds to a tall-skinny matrix (1,000,000 x 10,000) and Fat corresponds to a short-fat matrix (10,000 x 1,000,000).</p>\nPerformance improvements in distributed machine learning typically come from a combination of communication pattern improvements and algorithmic improvements. We focus on the former in this post, and algorithmic improvements will be discussed later. So <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">download</a> Spark 1.1 now, enjoy the performance improvements, and stay tuned for future posts.</td></tr><tr><td>List(Gavin Targonski (Product Management at Talend))</td><td>List(Company Blog, Partners)</td><td>List(2014-09-15, 2014-09-15, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.talend.com\" target=\"_blank\">Talend</a> after having Talend Studio <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\nAs the move to the next generation of integration platforms grows momentum, the need to implement a proven and scalable technology is critical. Databricks and Apache Spark, delivered on the major Hadoop distributions, is one such area where the delivery of massively scalable technology with low risk implementation is really key.\n\nAt Talend we see a wide array of batch processes, moving to an operational and real time perspective, driven by the consumers of the data. In this vein, the uptake in adoption and the growing community of Apache Spark, the powerful open-source processing engine, has been hard to miss. In a relatively short time, it is now a part of every major Hadoop vendor’s offering, is the most active open source project in the Big Data space, and has been deployed in production across a number of verticals.\n\nTraditional ETL and enterprise integration provides limitations, in both timeliness of extract, speed of load and importantly the supportability of the integration infrastructure itself. Spark delivers a new execution topology, allowing your Big Data platform to deliver much more than just ‘traditional’ Hadoop MapReduce tasks.\n\nThe power of integrating Spark and Talend Studio is that Talend users will get to immediately harness the power of Spark, all 80+ operations and sophisticated analytics, directly from the familiar and easy to use Talend Studio interface. With Talend and its unique approach to code generation, the code required to load data and execute a query in Spark is managed for you. The designers simply need to identify the data, Talend can then provide the tools to deliver the data at the right time, in the right format and into the desired Hadoop environment.\n\nAdditionally, we’re thrilled to announce – in conjunction with Databricks - that Talend Studio is now officially “Certified on Spark”. With the certification of Talend 5.5, the interoperability of your integration job created with Talend, and its execution on any Certified Spark Distribution is guaranteed. It also means that as a technology user you can benefit from the power of the platforms without having to maintain your own detailed roadmap of component upgrade, update and continued refactoring of jobs – Talend manages this for you. More broadly speaking, Talend is also supportive of the open and transparent nature of the certification process, which is designed to maintain compatibility within the Spark ecosystem while simultaneously encouraging innovation at the platform and application layers.\n\nBeyond the technical integration, Talend Labs worked closely with the R&amp;D team, based in Paris, to create an end-to-end scenario to showcase the key features and functions of the integrated Spark solution. This means that users have a fully-functional starting point, available from Talend and proven with Spark, to get you started on your journey.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/09/talend_screenshot.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Figure 1: Talend Studio and Spark</em></p>\nTalend is always evolving its certification in line with its key partners and the Big Data Ecosystem, and Spark is no exception. With such a fast moving project, significant features and improvements are being rolled out rapidly. Talend is committed to supporting Spark and will be moving fast to certify and ensure compatibility with future Spark releases.</td></tr><tr><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-09-18, 2014-09-18, UTC)</td><td><div class=\"post-meta\">This is a guest post by Nick Pentreath of <a href=\"http://graphflow.com\">Graphflow</a> and Kan Zhang of <a href=\"http://ibm.com\">IBM</a>, who contributed Python input/output format support to Apache Spark 1.1.</div>\n\n<hr />\n\nTwo powerful features of Apache Spark include its native APIs provided in Scala, Java and Python, and its compatibility with any Hadoop-based input or output source. This language support means that users can quickly become proficient in the use of Spark even without experience in Scala, and furthermore can leverage the extensive set of third-party libraries available (for example, the many data analysis libraries for Python).\n\nBuilt-in Hadoop support means that Spark can work \"out of the box\" with any data storage system or format that implements Hadoop's <code>InputFormat</code> and <code>OutputFormat</code> interfaces, including HDFS, HBase, Cassandra, Elasticsearch, DynamoDB and many others, as well as various data serialization formats such as SequenceFiles, Parquet, Avro, Thrift and Protocol Buffers.\n\nPreviously, Hadoop InputFormat/OutputFormat support was provided only in Scala or Java. To access such data sources in Python, other than simple text files, users would need to first read the data in Scala or Java, and write it out as a text file for reading again in Python. With the release of <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">Apache Spark 1.1</a>, Python users can now read and write their data directly from and to any Hadoop-compatible data source.\n<h2>An Example: Reading SequenceFiles</h2>\n<a href=\"http://wiki.apache.org/hadoop/SequenceFile\">SequenceFile</a> is the standard binary serialization format for Hadoop. It stores records of <code>Writable</code> key-value pairs, and supports splitting and compression. SequenceFiles are a commonly used format in particular for intermediate data storage in Map/Reduce pipelines, since they are more efficient than text files.\n\nSpark has long supported reading SequenceFiles natively using the <code>sequenceFile</code> method available on a <code>SparkContext</code> instance, which also utilizes Scala features to allow specifying the key and value type in the method call parameters. For example, to read a SequenceFile with <code>Text</code> keys and <code>DoubleWritable</code> values in Scala, we would do the following:\n\n<!--more-->\n\n[scala]val rdd = sc.sequenceFile[String, Double](path)[/scala]\n\nSpark takes care of converting <code>Text</code> to <code>String</code> and <code>DoubleWritable</code> to <code>Double</code> for us automatically.\n\nThe new PySpark API functionality exposes a <code>sequenceFile</code> method on a Python <code>SparkContext</code>instance that works in much the same way, with the key and value types being inferred by default. The <code>saveAsSequenceFile</code> method available on a PySpark <code>RDD</code> allows users to save an <code>RDD</code> of key-value pairs as a SequenceFile. For example, we can create an <code>RDD</code> from a Python collection, save it as a SequenceFile, and read it back using the following code snippet:\n\n<!--more-->\n\n[python]rdd = sc.parallelize([('key1', 1.0), ('key2', 2.0), ('key3', 3.0)])\nrdd.saveAsSequenceFile('/tmp/pysequencefile/')\n...\n\nsc.sequenceFile('/tmp/pysequencefile/').collect()\n[(u'key1', 1.0), (u'key2', 2.0), (u'key3', 3.0)]\n[/python]\n\n<h2>Under the Hood</h2>\nThis feature is built on top of the existing Scala/Java API methods. For it to work in Python, there needs to be a bridge that converts Java objects produced by Hadoop<code>InputFormats</code> to something that can be serialized into pickled Python objects usable by PySpark (and vice versa).\n\nFor this purpose, a <code>Converter</code> trait is introduced, along with a pair of default implementations that handle the standard Hadoop <a href=\"http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html\">Writables</a>.\n<h2>Custom Hadoop Converters</h2>\nWhile the default converters handle the most common Writable types, users need to supply custom converters for custom Writables, or for serialization frameworks that do not produce Writables. To see an illustration of this, some additional converters for <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala\">HBase</a> and <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/CassandraConverters.scala\">Cassandra</a>, together with related <a href=\"https://github.com/apache/spark/tree/master/examples/src/main/python\">PySpark scripts</a>, are included in the Apache Spark 1.1 example sub-project.\n<h2>A More Detailed Example: Custom Converters for Avro</h2>\nFor those who want to dive deeper, we will show how to write more complex custom PySpark converters, using the <a href=\"http://avro.apache.org/docs/current/\">Apache Avro</a> serialization format as an example.\n\nOne thing to consider is what input data the converter will be getting. In our case, we intend to use the converter with <code>AvroKeyInputFormat</code> and the input data will be Avro records wrapped in an AvroKey. Since we want to work with all 3 Avro data mappings (Generic, Specific and Reflect), for each Avro schema type, we need to handle all possible data types produced by those mappings. For example, for Avro <code>BYTES</code> type, both Generic and Specific mappings output <code>java.nio.ByteBuffer</code>, while Reflect mapping outputs <code>Array[Byte]</code>. So our <code>unpackBytes</code> method needs to handle both cases.\n\n<!--more-->\n\n[scala]def unpackBytes(obj: Any): Array[Byte] = {\n  val bytes: Array[Byte] = obj match {\n    case buf: java.nio.ByteBuffer =&amp;gt; buf.array()\n    case arr: Array[Byte] =&amp;gt; arr\n    case other =&amp;gt; throw new SparkException(\n      s&quot;Unknown BYTES type ${other.getClass.getName}&quot;)\n  }\n  val bytearray = new Array[Byte](bytes.length)\n  System.arraycopy(bytes, 0, bytearray, 0, bytes.length)\n  bytearray\n}\n[/scala]\n\nAnother thing to consider is what data types the converter will output, or equivalently, what data types PySpark will see. For example, for the Avro <code>ARRAY</code> type, the Reflect mapping may produce primitive arrays, Object arrays or <code>java.util.Collection</code> depending on its input. We convert all of these to<code>java.util.Collection</code>, which are in turn serialized into instances of a Python <code>List</code>.\n\n<!--more-->\n\n[scala]def unpackArray(obj: Any, schema: Schema): java.util.Collection[Any] = obj match {\n  case c: JCollection[_] =&amp;gt;\n    c.map(fromAvro(_, schema.getElementType))\n  case arr: Array[_] if arr.getClass.getComponentType.isPrimitive =&amp;gt;\n    arr.toSeq\n  case arr: Array[_] =&amp;gt;\n    arr.map(fromAvro(_, schema.getElementType)).toSeq\n  case other =&amp;gt; throw new SparkException(\n    s&quot;Unknown ARRAY type ${other.getClass.getName}&quot;)\n}\n[/scala]\n\nFinally, we need to handle nested data structures. This is done by recursively calling between individual <code>unpack*</code> methods and the central switch <code>fromAvro</code>, which handles the dispatching for all Avro schema types.\n\n<!--more-->\n\n[scala]def fromAvro(obj: Any, schema: Schema): Any = {\n  if (obj == null) {\n    return null\n  }\n  schema.getType match {\n    case UNION   =&gt; unpackUnion(obj, schema)\n    case ARRAY   =&gt; unpackArray(obj, schema)\n    case FIXED   =&gt; unpackFixed(obj, schema)\n    case MAP     =&gt; unpackMap(obj, schema)\n    case BYTES   =&gt; unpackBytes(obj)\n    case RECORD  =&gt; unpackRecord(obj)\n    case STRING  =&gt; obj.toString\n    case ENUM    =&gt; obj.toString\n    case NULL    =&gt; obj\n    case BOOLEAN =&gt; obj\n    case DOUBLE  =&gt; obj\n    case FLOAT   =&gt; obj\n    case INT     =&gt; obj\n    case LONG    =&gt; obj\n    case other   =&gt; throw new SparkException(\n      s&quot;Unknown Avro schema type ${other.getName}&quot;)\n  }\n}\n[/scala]\n\nThe complete source code for <code>AvroWrapperToJavaConverter</code> can be found in the Spark examples, in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\">AvroConverters.scala</a>, while the related PySpark script for using the converter can be found <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/python/avro_inputformat.py\">here</a>.\n<h2>Conclusion and Future Work</h2>\nWe're excited to bring this new feature to PySpark in the <a href=\"http://spark.apache.org/releases/spark-release-1-1-0.html\">1.1 release</a>, and look forward to seeing how users make use of both the built-in functionality, and custom converters.\n\nOne limitation of the current <code>Converter</code> interface is that there is no way to set custom configuration options. A future improvement could be to allow converters to take a Hadoop <a href=\"https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html\">Configuration</a> that would allow configuration at runtime.</td></tr><tr><td>List(Vida Ha)</td><td>List(Company Blog, Product)</td><td>List(2014-09-24, 2014-09-24, UTC)</td><td>At Databricks, we are often asked how to go beyond the basic Apache Spark tutorials and start building real applications with Spark.  As a result, we are developing reference applications <a href=\"http://github.com/databricks/reference-apps\" target=\"_blank\">on github</a> to demonstrate that.  We believe this is a great way to learn Spark, and we plan on incorporating more features of Spark into the applications over time.  We also hope to highlight any technologies that are compatible with Spark and include best practices.\n<h3>Log Analyzer Application</h3>\nOur first reference application is log analysis with Spark.  Logs are a large and common data set that contain a rich set of information. Log data can be used for monitoring web servers, improving business and customer intelligence, building recommendation systems, preventing fraud, and much more.  Spark is a wonderful tool to use on logs - Spark can process logs faster than Hadoop MapReduce, it is easy to code so we can compute many statistics with ease, and many of Spark’s libraries can be used on log data.  Also, Spark SQL can be used for querying logs using familiar SQL syntax and Spark Streaming can be used to provide real-time logs analysis.\n\nThe log analysis reference application is broken down into sections with detailed explanations and small code examples to demonstrate various features of Spark.  We start off with a gentle example of processing historical log data using standalone Spark, then cover how to do the same analysis with Spark SQL, before covering Spark Streaming.  Along the way, we highlight any caveats or recommended best practices for using Spark - such as how to refactor your code for reuse between the batch and streaming libraries.  The examples are self-contained and emphasize one aspect of Spark at a time, so you can experiment and modify the examples to deepen your understanding of that topic.\n\n<img src=\"https://databricks.com/wp-content/uploads/2014/09/app_diagram.png\" alt=\"\" width=\"100%\" />\n\nCode from the examples is combined to form a sample end-to-end application as shown in the diagram above.  For now, the final application is an MVP log analysis streaming application.  The application monitors a directory for new log input files, and when it receives one - the file is processed.   Spark Streaming is used to compute statistics for the last N time as well as all of time, and refreshed.  Each time interval, an html file is written which reflects the latest log data in the folder.\n<h3>Twitter Streaming Language Classifier</h3>\nThe second reference application is a popular demo that the Databricks team has given, and was video taped here:\n\n<a href=\"https://www.youtube.com/watch?v=FjhRkfAuU7I#t=2035\"><img src=\"https://databricks.com/wp-content/uploads/2014/09/aaron-yaaay.png\" alt=\"\" width=\"100%\" /></a>\n\nThis application demonstrates the following:\n<ol>\n \t<li>Collecting Twitter data using Spark Streaming.</li>\n \t<li>Examine the tweets using Spark SQL and training a Kmeans clustering model for classifying the language of a tweet using Spark MLLib.</li>\n \t<li>Applying the model in realtime using Spark MLLib and Spark Streaming.</li>\n</ol>\nNow, we are releasing the code so you can run the demo yourself and walk through how it works.\n<h3>Get Started with the Databricks Reference Applications</h3>\nThese applications are just a start though - we will add more and improve our reference applications over time. Please follow these steps to get started:\n<ol>\n \t<li>Read <a href=\"http://databricks.gitbooks.io/databricks-spark-reference-applications/\" target=\"_blank\">the documentation online</a> in book format.</li>\n \t<li>Go to our <a href=\"http://github.com/databricks/reference-apps\" target=\"_blank\">github repo</a> to view the code for the reference applications.</li>\n \t<li>Please open an <a href=\"https://github.com/databricks/reference-apps/issues\" target=\"_blank\">issue on github</a> with any useful feedback about our reference applications.</li>\n</ol></td></tr><tr><td>List(John Tripier, Paco Nathan)</td><td>List(Announcements, Company Blog)</td><td>List(2014-09-19, 2014-09-19, UTC)</td><td>When Databricks was initially founded a little more than a year ago, there was tremendous excitement around Apache Spark, but it was still early days.  The project had ~60 contributors over the previous 12 months, and was not yet available commercially.  One of our main focus areas since then has been continuing to grow Spark and the community and making it easily accessible for enterprises and users alike.\n\nTaking a step back, it’s terrific to see the progress that Spark has made since then.  Spark is today the most active open source project in the Big Data ecosystem with over 300 contributors in the last 12 months alone, and is available through several platform vendors, including all of the major Hadoop distributors.  The <a href=\"http://www.spark-summit.org\" target=\"_blank\">Spark Summit</a>, dedicated to bringing together the Spark community, more than doubled in size a short 6 months after the inaugural version, and Spark meetups continue to grow in size, frequency, and cities spanned. \n \nThis momentum has also led to a significant surge in enterprise usage of Spark as organizations move along the adoption curve.  Not only does the number of enterprises leveraging Spark in a PoC or development environment continue to grow rapidly, but a significant number have moved on to large-scale production workloads.  Production uses span a number of verticals, including telecom, financial services, media and entertainment, retail, and the public sector.  Many great examples of these are publicly available, whether in the archives of the most recent Spark Summit, or in blog posts such as the recent one by <a href=\"https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html\">Alibaba Taobao</a>.  At the same time, a growing number of applications - BI, ETL, and domain-specific - are being ported over to Spark or just being built on top of it from the start.\n\nThe next phase in this evolution, as enterprises increasingly focus on converting data into value, is to build enterprise applications that capture domain-specific business logic on top of Spark.  The challenge is that the pace of Spark adoption has outstripped the pool of Spark experts, creating high demand for developers with Apache Spark expertise.  Essentially, enterprises need people who have demonstrated expertise in how to implement best practices and expand depth of insights through Spark’s combination of sophisticated analytics and blazing speed. \n\nAt Databricks, as the company founded by the creators of Spark and the continued driving force behind the project, we get daily requests soliciting our help from companies who are looking to build increasingly complex and sophisticated solutions on top of their Spark deployments. Unfortunately, we're not a professional services company and we're simply not built for those type of engagements. The typical follow-on question is then around how to identify Spark experts that could help the enterprise with these engagements. \n\nAs a result, we're establishing the <a href=\"http://www.oreilly.com/data/sparkcert.html?cmp=ex-strata-na-lp-na_apache_spark_certification\" target=\"_blank\">Certified Spark Developer program</a>, which provides the industry standard for demonstrating Spark expertise.\n\nThe O’Reilly team was seeing the same demand and so Databricks’ Spark experts and the O’Reilly Media editorial team have partnered to address those needs. This new program  – consisting of a formal exam and subsequent certification – establishes the industry standard for measuring and validating technical expertise in Spark. \n\nFor developers, this program allows them to validate their knowledge of Spark and it publicly recognizes their expertise.  For Enterprises, this program provides clear guidelines and the confidence that their teams have the requisite expertise. For professional services companies, this program certifies their capabilities to deliver solutions based on Spark and provides an additional form of validation when communicating with customers.\n\nThe developer certification program by Databricks+O’Reilly augments our efforts to grow the Spark community and enable the ecosystem as a critical component of all successful platforms.  Recent Databricks initiatives have included the <a href=\"https://databricks.com/spark/certification/certified-on-spark\">‘Certified on Spark’</a> and <a href=\"https://databricks.com/spark/certification/certified-spark-distribution\">‘Certified Spark Distribution’</a> programs designed to ensure compatibility between Spark applications and distributions. There is also a long-running and rapidly expanding training program, and events such as Spark Summit which bring together thousands of members of the Spark community from across the globe.\n\nSpark Developer Certification will be launched formally the upcoming O’Reilly Media conference Strata NY + Hadoop World in New York City, October 15-17. Initial exams will be conducted during the conference. For more information:  <a href=\"http://www.oreilly.com/data/sparkcert.html?cmp=ex-strata-na-lp-na_apache_spark_certification\" target=\"_blank\">http://oreilly.com/go/sparkcert</a></td></tr><tr><td>List(Christopher Burdorf (Senior Software Engineer at NBC Universal))</td><td>List(Company Blog, Customers)</td><td>List(2014-09-24, 2014-09-24, UTC)</td><td><div class=\"post-meta\">This is a guest blog post from our friends at NBC Universal outlining their Apache Spark use case.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nNBC Universal is one of the world’s largest media and entertainment companies with revenues of US$ 26 billion. It operates television networks, cable channels, motion picture and television production companies as well as branded theme parks worldwide. Popular brands include NBC, Universal Pictures, Universal Parks &amp; Resorts, Telemundo, E!, Bravo and MSNBC.\n\nDigital video media clips for NBC Universal’s cable TV programs and commercials are produced and broadcast from its Los Angeles office to cable TV channels in Asia Pacific, Europe, Latin America and the United States. Moreover, viewers increasingly consume NBC Universal’s vast content library online and on-demand.\n\nTherefore, NBC Universal’s IT Infrastructure team needs to make decisions on how best to serve that content, which involves a trade-off between storage and bandwidth cost versus consumer convenience. NBC Universal can keep all content available online and cached at the edge of the network to minimize latency. This way, all of the content could be delivered instantly to consumers across all countries where NBC Universal has a presence. But this would also be the most costly option.\n\nTherefore, the business challenge is to determine the optimal mix between storing the most popular content locally close to its viewers, and serving less popular content only on demand, which incurs higher network costs, or taking it offline altogether.\n<h2>Solution</h2>\nNBC Universal turned to Spark to analyze all the content meta-data for its international content distribution. Metadata associated with the media clips is stored in an Oracle database and in broadcast automation playlists. Apache Spark is used to query the Oracle database and distribute the metadata from the broadcast automation playlists into multiple large in-memory resilient distributed datasets (RDDs). One RDD stores Scala objects containing media IDs, time codes, schedule dates and times, channels for airing etc. It then creates multiple RDDs containing broadcast frequency counts by week, month, and year and uses Spark’s map/reduceByKey to generate the counts. The resulting data is bulk loaded into HBase where it is queried from a Java/Spring web application. The application converts the queried results into graphs illustrating media broadcast frequency counts by week, month, and year on an aggregate and a per channel basis.\n\nA secondary procedure then queries filepath information from Oracle and builds another RDD that contains that information along with the date the file was written. It then computes a Spark join of that RDD and the previous frequency count data RDD to produce a new RDD that contains the usage frequency based on the file age sorted in ascending order. This resulting data is used to generate histograms to help determine if the offlining mechanism is working optimally.\n\nNBC Universal runs Apache Spark in production in conjunction with Mesos, HBase and HDFS and uses Scala as the programming language. The rollout in production happened in Q1 2014 and was smooth.\n\nSpark provides both extremely fast processing times, leveraging its distributed in-memory approach, as well as much better IT staff productivity. In my Spark Summit 2014 talk, I highlighted two aspects of our Spark deployment:\n<ul>\n \t<li><strong>Developer productivity:</strong> The combination of Spark and Scala provides an “ideal programming environment”</li>\n \t<li><strong>Operational stability:</strong> Mesos for cluster management.</li>\n</ul>\nAlternative approaches not based on Spark would have required a much more complicated data processing pipeline and workflow. For example, since the main memory usage requires more than what is available on a single server, a much slower procedure would have to be used that would have involved processing the data in chunks, and writing intermediate results to HDFS and then loading the chunks of data back into main memory and processing it in a manner similar to traditional Hadoop map/reduce jobs which has been shown to be much slower than Spark, through various examples on the Apache Spark website.\n<h2>Value Realized</h2>\nSpark provides a fast and easy way to assemble a data pipeline and conduct analyses that drive decisions on which content to keep online versus take off-line. Moreover, infrastructure administrators gain valuable insights into network utilization. They can detect patterns that help them understand wastage of bandwidth in the multi-system operator (MSO) network. The initial results are promising, which prompted NBC to expand its use of Spark to machine learning.\n<h2>To Learn More:</h2>\n<ol>\n \t<li><a href=\"https://www.youtube.com/watch?v=bhunR-Wb7KY&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS&amp;index=6\" target=\"_blank\">https://www.youtube.com/watch?v=bhunR-Wb7KY&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS&amp;index=6</a></li>\n \t<li><a href=\"http://spark-summit.org/wp-content/uploads/2014/06/Using-Spark-to-Generate-Analytics-for-International-Cable-TV-Video-Distribution-Christopher-Burdorf.pdf\" target=\"_blank\">http://spark-summit.org/wp-content/uploads/2014/06/Using-Spark-to-Generate-Analytics-for-International-Cable-TV-Video-Distribution-Christopher-Burdorf.pdf</a></li>\n</ol></td></tr><tr><td>List(Manish Amde (Origami Logic), Joseph Bradley (Databricks))</td><td>List(Engineering Blog, Machine Learning)</td><td>List(2014-09-30, 2014-09-30, UTC)</td><td><div class=\"post-meta\">This is a post written together with one of our friends at <a href=\"http://www.origamilogic.com/\">Origami Logic</a>. Origami Logic provides a Marketing Intelligence Platform that uses Apache Spark for heavy lifting analytics work on the backend.</div>\n\n<hr />\n\nDecision trees and their ensembles are industry workhorses for the machine learning tasks of classification and regression. Decision trees are easy to interpret, handle categorical and continuous features, extend to multi-class classification, do not require feature scaling and are able to capture non-linearities and feature interactions.\n\nDue to their popularity, almost every machine learning library provides an implementation of the decision tree algorithm. However, most are designed for single-machine computation and seldom scale elegantly to a distributed setting. Apache Spark is an ideal platform for a scalable distributed decision tree implementation since Spark's in-memory computing allows us to efficiently perform multiple passes over the training dataset.\n\nAbout a year ago, open-source developers joined forces to come up with a fast distributed decision tree implementation that has been a part of the Spark MLlib library since release 1.0. The Spark community has actively improved the decision tree code since then. This blog post describes the implementation, highlighting some of the important optimizations and presenting test results demonstrating scalability.\n\n<b>New in Spark 1.1</b>: MLlib decision trees now support multiclass classification and include several performance optimizations. There are now APIs for Python, in addition to Scala and Java.\n<h2>Algorithm Background</h2>\nAt a high level, a decision tree model can be thought of as hierarchical if-else statements that test feature values in order to predict a label. An example model for a binary classification task is shown below. It is based upon car mileage data from the 1970s! It predicts the mileage of the vehicle (high/low) based upon the weight (heavy/light) and the horsepower.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/decision-tree-example.png\"><img class=\"alignnone size-full wp-image-1597\" src=\"https://databricks.com/wp-content/uploads/2014/09/decision-tree-example.png\" alt=\"decision-tree-example\" width=\"503\" height=\"297.5\" /></a></p>\nA model is learned from a training dataset by building a tree top-down. The if-else statements, also known as splitting criteria, are chosen to maximize a notion of information gain --- it reduces the variability of the labels in the underlying (two) child nodes compared the parent node. The learned decision tree model can later be used to predict the labels for new instances.\n\nThese models are interpretable, and they often work well in practice. Trees may also be combined to build even more powerful models, using ensemble tree algorithms. Ensembles of trees such as random forests and boosted trees are often top performers in industry for both classification and regression tasks.\n<h2>Simple API</h2>\nThe example below shows how a decision tree in MLlib can be easily trained using a few lines of code using the new Python API in Spark 1.1. It reads a dataset, trains a decision tree model and then measures the training error of the model. Java and Scala examples can be found in <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">the Spark documentation on DecisionTree</a>.\n\n[python]\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import DecisionTree\nfrom pyspark.mllib.util import MLUtils\n\n# Load and parse the data file into an RDD of LabeledPoint.\n# Cache data since we will use it again to compute training error.\ndata = MLUtils.loadLibSVMFile(sc, 'mydata.txt').cache()\n\n# Train a DecisionTree model.\nmodel = DecisionTree.trainClassifier(\n    data,\n    numClasses=2,\n    categoricalFeaturesInfo={}, # all features are continuous\n    impurity='gini',\n    maxDepth=5,\n    maxBins=32)\n\n# Evaluate model on training instances and compute training error\npredictions = model.predict(data.map(lambda x: x.features))\nlabelsPredictions = data.map(lambda lp: lp.label).zip(predictions)\ntrainErr = labelsPredictions.filter(lambda (v, p): v != p).count() \\\n    / float(data.count())\nprint('Training Error = ' + str(trainErr))\nprint('Learned classification tree model:')\nprint(model)\n[/python]\n\n<h2>Optimized Implementation</h2>\nSpark is an ideal compute platform for a scalable distributed decision tree implementation due to its sophisticated DAG execution engine and in-memory caching for iterative computation. We mention a few key optimizations.\n\n<b>Level-wise training</b>: We select the splits for all nodes at the same level of the tree simultaneously. This level-wise optimization reduces the number of passes over the dataset exponentially: we make one pass for each level, rather than one pass for each node in the tree. It leads to significant savings in I/O, computation and communication.\n\n<b>Approximate quantiles</b>: Single machine implementations typically use sorted unique feature values for continuous features as split candidates for the best split calculation. However, finding sorted unique values is an expensive operation over a distributed dataset. The MLlib decision tree uses quantiles for each feature as split candidates. It's a standard tradeoff for improving decision tree performance without significant loss of accuracy.\n\n<b>Avoiding the map operation</b>: The early prototype implementations of the decision tree used both map and reduce operations when selecting best splits for tree nodes. The current code uses significantly less computation and communication by exploiting the known structure of the pre-computed split candidates to avoid the map step.\n\n<b>Bin-wise computation</b>: The best split computation discretizes features into bins, and those bins are used for computing sufficient statistics for splitting. We precompute the binned representations of each instance, saving computation on each iteration.\n<h2>Scalability</h2>\nWe demonstrate the scalability of MLlib decision trees with empirical results on various datasets and cluster sizes.\n<h4>Scaling with dataset size</h4>\nThe two figures below show the training times of decision trees as we scale the number of instances and features in the dataset. The training times increased linearly, highlighting the scalability of the implementation.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-instances.png\"><img class=\"alignnone size-full wp-image-1519\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-instances.png\" alt=\"DT-scaling-instances\" width=\"394.5\" height=\"251\" /></a></p>\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-features.png\"><img class=\"alignnone size-full wp-image-1518\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-scaling-features.png\" alt=\"DT-scaling-features\" width=\"386\" height=\"248.5\" /></a></p>\nThese tests were run on an EC2 cluster with a master node and 15 worker nodes, using r3.2xlarge instances (8 virtual CPUs, 61 GB memory). The trees were built out to 6 levels, and the datasets were generated by the <a href=\"https://github.com/databricks/spark-perf\">spark-perf library</a>.\n<h4>Spark 1.1 speedups</h4>\nThe next two figures show improvements in Apache Spark 1.1, relative to the original Apache Spark 1.0 implementation. On the same datasets and cluster, the new implementation is 4-5X faster on many datasets!\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-instances.png\"><img class=\"alignnone size-full wp-image-1606\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-instances.png\" alt=\"DT-speedups-instances\" width=\"358\" height=\"248.5\" /></a></p>\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-features.png\"><img class=\"alignnone size-full wp-image-1607\" src=\"https://databricks.com/wp-content/uploads/2014/09/DT-speedups-features.png\" alt=\"DT-speedups-features\" width=\"356\" height=\"248.5\" /></a></p>\n\n<h2>What’s Next?</h2>\nThe tree-based algorithm development beyond release 1.1 will focus primarily on ensemble algorithms such as random forests and boosting. We will also keep optimizing the decision tree code for performance and plan to add support for more options in the upcoming releases.\n\nTo get started using decision trees yourself, <a href=\"http://spark.apache.org/\">download Spark 1.1 today</a>!\n<h2>Further Reading</h2>\n<ul>\n \t<li>See examples and the API in <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">the MLlib decision tree documentation</a>.</li>\n \t<li>Watch <a href=\"http://spark-summit.org/2014/talk/scalable-distributed-decision-trees-in-spark-mllib\">the decision tree presentation</a> from the 2014 Spark Summit.</li>\n \t<li>Check out <a href=\"http://functional.tv/post/98342564544/sfscala-sfbaml-joseph-bradley-decision-trees-on-spark\">video</a> and <a href=\"https://speakerdeck.com/jkbradley/mllib-decision-trees-at-sf-scala-baml-meetup\">slides</a> from another talk on decision trees at a Sept. 2014 SF Scala/Bay Area Machine Learning meetup.</li>\n</ul>\n&nbsp;\n<h3>Acknowledgements</h3>\nThe Spark MLlib decision tree work was initially performed jointly with Hirakendu Das (Yahoo Labs), Evan Sparks (UC Berkeley AMPLab), and Ameet Talwalkar and Xiangrui Meng (Databricks). More contributors have joined since then, and we welcome your input too!</td></tr><tr><td>List(Eric Carr (VP Core Systems Group at Guavus))</td><td>List(Company Blog, Partners)</td><td>List(2014-09-25, 2014-09-25, UTC)</td><td><div class=\"post-meta\">This is a guest blog post from our friends at <a href=\"http://www.guavus.com\" target=\"_blank\">Guavus</a> - now a Certified Apache Spark Distribution - outlining how they leverage Spark to deliver value to telecom companies.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nGuavus is a leading provider of big data analytics solutions for the Communications Service Provider (CSP) industry. The company counts 4 of the top 5 mobile network operators, 3 of the top 5 Internet backbone providers, as well as 80% of cable MSOs in North America as customers. The Guavus Reflex platform provides operational intelligence to these service providers. Reflex currently analyzes more than 50% of all US mobile data traffic and processes more than 2.5 petabytes of data per day.\n\nYet that data grows at an exponential rate. Ever increasing data volume and velocity makes it harder to generate timely insights. For instance, one operational issue can quickly cascade into multiple issues down-stream in the network, which makes it critical to go from insight to action in a very short time frame.\n\nMany service providers have deployed data lakes to explore more of their information. These hold data across all time periods and are well suited to analytic jobs that take several minutes to complete at best. Yet a much more urgent and important challenge is to enable decision making on incoming streams of data in real time. Therefore, Guavus needed an ability to produce meaningful, real-time insights on just the last seconds of data, correlating data sources across network equipment, end customer devices, subscriber and billing systems. A particular challenge for the first generation of the Reflex Platform was to filter specific events from large amounts of incoming data, which is necessary in order to achieve the goal of generating real-time operational intelligence.\n<h2>Solution</h2>\nSince the company’s launch in February 2006, Guavus has been developing big data analytics solutions based on Hadoop and Map Reduce technologies. As customers’ requirements have become more time sensitive and the technologies have matured, Guavus looked to evolve its solutions from batch-processing (\n\nGuavus’ second product generation Reflex 2.0™ was unveiled at Spark Summit and is built on Apache Spark and Hadoop YARN. Thanks to the capabilities of Apache Spark, the latest version of Reflex expands beyond batch processing to enable truly real-time continuous analysis at scale from multiple sources including data from billing systems, CRM, OSS, networks, applications, devices and clouds. Reflex 2.0 continuously correlates, fuses and analyzes these data streams with data at rest to provide communications service providers with a 360-degree view of what’s going on in their network.\n\nSpark’s capabilities for iterative processing, filtering and enrichment of event stream data made it ideal for use with existing event filtering algorithms. Developers and end users embraced the new platform because of its ease of use and high-level abstractions. These included Spark machine learning libraries but also a distributed SQL query engine for Hadoop data.\n\nAs part of the company’s commitment to the open source community and strong belief in Apache Spark’s capabilities for streaming analytics, the Guauvs Reflex 2.0 platform is now also a Certified Spark Distribution. Certification is significant, as it will allow Guavus to innovate even faster and enhance the Reflex platform with greater flexibility, while ensuring compatibility with the latest standards and support for the growing ecosystem around Spark.\n\nMoreover, Guavus contributes back to the open source community in the area of real-time event processing. When conducting operational analyses on network data, it is important to be able to distinguish between the actual time the event happened, and the time stamp when the event was received by the system. Guavus’ team has evolved the D-streams (discretized streams) feature to a concept it calls bin-streams, which better addresses this common service provider use case. This represents Guavus’ first contribution back into the Spark community.\n<h2>Value Realized</h2>\nSpark enables an “analyze first” approach that allows service providers to reason on the data as it arrives, versus the traditional “store first ask questions later” approach of Hadoop. This real-time analytic capability is disproportionately valuable, since it allows the service provider to take action while the shopper is in the store, while the customer is on the line with the call center agent, or while fraudulent transactions are in progress.\n\nThe metrics Guavus shared at Spark Summit are certainly impressive. Reflex 2.0 processes over 2.5 petabytes of data per day, which equals 250 billion records per day, and 2.5 million transactions per second. Guavus has observed a 3 to 5x performance improvement for its Reflex 2.0 product versus its 1.x product generation, while drastically reducing the hardware footprint required.\n\nBy analyzing data as it arrives within milliseconds of when it hits the network, Guavus customers can trigger immediate actions and improve decision-making.\n\nGuavus has built a data layer that sits on top of the Reflex platform that transforms, aggregates and correlates data from multiple sources, including streaming and stored data, and applies machine learning algorithms to then feed the data into an analytic application. The data layer works in conjunction with the application to deliver rapid time to value, in some cases reducing development time by as much as 12 months. The data layer can also feed into third party applications and into data lakes for maximum extensibility. And by creating an abstraction layer that manages the data complexity and optimizes the processing for Extract Transform Load (ETL) and Enterprise Data Warehouse (EDW) at the edge vs. in a central repository, Guavus fundamentally changes the economics of analyzing the data.\n\nFor example, a Tier 1 US Multiple System Operator (MSO) leverages the Guavus Reflex platform to correlate call center events and network streaming events to detect anomalies and identify the root cause for timely resolution. Based on these insights, the MSO was able to make adjustments in the moment to improve the customer experience. The CareReflex application allowed the MSO to discriminate between device and network related issues using one-click root-cause analysis. From there, the Interactive Voice Response (IVR) could be deflected and the customer service agent script modified accordingly. In addition, field operations were dispatched to repair network faulty equipment vs. customer premise equipment saving the MSO millions in unnecessary truck rolls and improving mean time to resolution (MTTR) for customer call agents. The MSO estimates that this single application will result in $50 million in savings annually.\n\nOther examples of how CSPs can use Guavus Reflex 2.0 operational intelligence platform include:\n<ul>\n \t<li>Develop solutions that can be embedded into workflows and business processes to improve CAPEX/OPEX efficiencies</li>\n \t<li>Identify and prevent fraudulent activity in the network as it happens</li>\n \t<li>Create highly targeted, personalized marketing mobile ad campaigns based on subscriber activities</li>\n</ul>\n<h2>To Learn More:</h2>\n<ol>\n \t<li><a href=\"http://spark-summit.org/wp-content/uploads/2014/07/Building-Big-Data-Operational-Intelligence-Platform-with-Apache-Spark-Eric-Carr.pdf\" target=\"_blank\">http://spark-summit.org/wp-content/uploads/2014/07/Building-Big-Data-Operational-Intelligence-Platform-with-Apache-Spark-Eric-Carr.pdf</a></li>\n \t<li><a href=\"https://www.youtube.com/watch?v=qMD4XIOtgh0&amp;index=3&amp;list=PL-x35fyliRwhD4eFBjkHpYyZjTWW9qZDu\" target=\"_blank\">https://www.youtube.com/watch?v=qMD4XIOtgh0&amp;index=3&amp;list=PL-x35fyliRwhD4eFBjkHpYyZjTWW9qZDu</a></li>\n</ol></td></tr><tr><td>List(Jeremy Freeman (Freeman Lab))</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2014-10-01, 2014-10-01, UTC)</td><td>The brain is the most complicated organ of the body, and probably one of the most complicated structures in the universe. It’s millions of neurons somehow work together to endow organisms with the extraordinary ability to interact with the world around them. Things our brains control effortlessly -- kicking a ball, or reading and understanding this sentence -- have proven extremely hard to implement in a machine.\n\nFor a long time, our efforts were limited by experimental technology. Despite the brain having many neurons, most technologies could only monitor the activity of one, or a handful, at once. That these approaches taught us so much -- for example, that there are neurons that respond only when you look at a particular object -- is a testament to experimental ingenuity.\n\nIn the next era, however, we will be limited not by our recordings, but our ability to make sense of the data. New technologies make it possible to monitor the activity of many thousands of neurons at once -- from a small region of the mouse brain, or from the entire brain of the larval zebrafish. These advances in recording come with dramatic increases in data size. Several years ago, a large neural data set might have been a few GB, amassed across months or years. Today, monitoring the entire zebrafish brain can yield several TBs in an hour (see <a href=\"http://thefreemanlab.com/pdf/vladimirov-2014-nature-methods.pdf\" target=\"_blank\">this paper</a> for a description of recent experimental technology).\n\n<iframe src=\"//www.youtube.com/embed/YLVdRPVj-XM\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Whole-brain activity measured in a larval zebrafish while simultaneously presenting a moving visual stimulus (upper left) and monitoring intended swimming behavior (size of circle)</em></p>\nThere are many challenges to analyzing neural data. The measurements are indirect, and useful signals must be extracted and transformed, in a manner tailored to each experimental technique -- our version of ETL. At a higher-level, our analyses must find patterns of biological interest from the sea of data. An analysis is only as good as the experiment it motivates; the faster we can explore data, the sooner we can generate a hypothesis and move research forward.\n\nIn the past, neuroscience analysis has largely relied on single-workstation solutions. In the future, it will increasingly rely on some form of distributed computing, and we think Spark is the ideal platform. This post explains why.\n<h2>Why Apache Spark?</h2>\nThe first challenge in bringing distributed computing to a community not currently using it is deployment. Apache Spark can run out-of-the-box on Amazon’s EC2, which immediately opens it up to a wide community. Although the cloud has many advantages, several universities and research institutes have existing high-performance computing clusters. We were pleased and surprised by how straightforward it was to integrate Spark into our own cluster, which runs the Univa Grid Engine. We used the Spark standalone scripts, alongside the existing UGE scheduler, to enable our users to launch their own private Spark cluster with a pre-specified number of nodes. We did not need to setup Hadoop, because Spark can natively load data from our networked file system. Our infrastructure is common to many academic and research institutions, and we hope it will be easy to replicate our approach elsewhere.\n\nOne of the reasons neural data analysis is so challenging -- and so fascinating -- is that little of what we do is standardized. Unlike, say, trying to maximize accuracy of user recommendations, or performance of a classifier, we are trying to maximize our understanding. To be sure, there are families of workflows, analyses, and algorithms that we use regularly, but it’s just a toolbox, and a constantly evolving one. To understand data, we must try many analyses, look at the results, modify at many levels -- whether adjusting preprocessing parameters, or developing an entirely new algorithm -- and inspect the results again.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/databricks-blog-post-graphic-01.png\" alt=\"\" align=\"middle\" />\n<p style=\"margin-bottom: 30px;\" align=\"center\"><em>Examples of analyses performed using Spark, including basic processing of activity patterns (left), matrix factorization to characterize functionally similar regions (as depicted by different colors) (middle), and embedding dynamics of whole-brain activity into lower-dimensional trajectories (right)</em></p>\nThe ability to cache a large data set in RAM and repeatedly query it with multiple analyses is critical for this exploratory process, and is a key advantage of Spark compared to conventional MapReduce systems. Any data scientist knows there is a “working memory for analysis”: if you need to wait more than a few minutes for a result, you forget what you were doing. If you need to wait overnight, you’re lost. With Spark, especially once data is cached, we can get answers to new queries in seconds or minutes, instead of hours or days. For exploratory analysis, this is a game changer.\n<h2>Why PySpark?</h2>\nSpark offers elegant and powerful APIs in Scala, Java, and Python. We are developing a library for neural data analysis, called <a href=\"http://thunder-project.org/\" target=\"_blank\">Thunder</a>, largely in the Python API (PySpark), which for us offers several unique advantages. (A paper describing this library and its applications, in collaboration with the lab of Misha Ahrens, and featuring Spark developer Josh Rosen as a co-author, was recently published in Nature Methods, and is available <a href=\"http://thefreemanlab.com/pdf/freeman-2014-nature-methods.pdf\" target=\"_blank\">here</a>.)\n\n1) <strong>Ease of use</strong>. Although Scala has many advantages (and I personally prefer it), for most users Python is an easier language to learn and use for developing analyses. Due to its scientific computing libraries (see below), Python adoption is increasing in neuroscience and other scientific fields. And in some cases, users with existing analyses can bring them into Spark straightaway. To consider a simple example, a common workflow is to fit individual models to thousands or millions of neurons or neural signals independently. We can easily express this embarrassingly-parallel problem in PySpark (see an <a href=\"http://research.janelia.org/zebrafish/tuning.html\" target=\"_blank\">example</a>). If a new user wants to do the same analysis with their own, existing model-fitting routine, already written in Python and vetted on smaller scale data, it would be plug-and-play. More exciting still, working in Spark means they can use the exact same platform to try more complex distributed operations: for example, take the parameters from those independently fitted models and perform clustering or dimensionality reduction -- all in Python.\n\n2) <strong>Powerful libraries</strong>. With libraries like NumPy, SciPy, and scikit-learn, Python has become a powerful platform for scientific computing. When using PySpark, we can easily leverage these libraries for components of our analyses, including signal processing (e.g. fourier transforms), linear algebra, optimization, statistical computations, and more. Spark itself offers, through its MLlib library, many high-performance distributed implementations of machine learning algorithms. These implementations nicely complement the analyses we are developing. In so far as there is overlap, and our analyses are sufficiently general-purpose, we are either using -- or are in the process of contributing to -- analyses in MLLib. But much of what we do, and how we implement it, is specific to our problems and data types (e.g. images and time series). With Thunder, we hope to provide an example of how an external library can thrive on top of Spark (see also the <a href=\"http://bdgenomics.org/\" target=\"_blank\">ADAM</a> library for genomic analysis from the AmpLab).\n\n3) <strong>Visualization</strong>. Especially for exploratory data analysis, the ability to to visualize intermediate results is critical, and often these visualizations must be tailored and tweaked depending on the data. Again, the combination of Spark and Python offers many advantages. Python has a core plotting library, matplotlib, and new libraries are improving its aesthetics and capabilities (e.g. <a href=\"http://mpld3.github.io/\" target=\"_blank\">mpld3</a>, <a href=\"https://github.com/mwaskom/seaborn\" target=\"_blank\">seaborn</a>). In an iPython notebook, we can perform analyses with Spark and visually inspect results (see an <a href=\"http://nbviewer.ipython.org/url/research.janelia.org/zebrafish/notebooks/optomotor-response-PCA.ipynb\" target=\"_blank\">example</a>). We are developing workflows in which custom visualizations are tightly integrated into each of our analyses, and consider this crucial as our analyses become increasingly complex.\n<h2>The future: Spark Streaming</h2>\nSpark has already massively sped up our post-hoc data processing and analysis. But what if we want an answer during an experiment? We are beginning to use Spark Streaming for real-time analysis and visualization of neural data. Because Spark Streaming is built in to the Spark ecosystem, rather than an independent platform, we can leverage a common code-base, and also the same deployment and installation. Streaming analyses will let us adapt our experiments on the fly. And as technology marches ever forward, we may soon collect data so large and so fast that we couldn’t store the complete data sets even if we wanted to. By analyzing more and more online, and storing less, Spark Streaming may give us a solution.\n<h2>Conclusion</h2>\nWe are at the beginning of an exciting moment in large-scale neuroscience. We think Spark will be core to our analytics, but significant challenges lie ahead. Given the scale and complexity of our problems, different research groups must work together to unify analysis efforts, vet alternative approaches, and share data and code. We believe that any such effort must be open-source through-and-through, and we are fully committed to building open-source solutions. We also need to work alongside the broader data science and machine learning community to develop new analytical approaches, which could in turn benefit communities far beyond neuroscience. Understanding the brain will require all of our biological and analytical creativity -- and we just might help revolutionize data science in the process.</td></tr><tr><td>List(Russell Cardullo (Sharethrough))</td><td>List(Company Blog, Customers)</td><td>List(2014-10-07, 2014-10-07, UTC)</td><td><div class=\"post-meta\">This is a guest blog post from our friends at <a href=\"http://www.sharethrough.com\" target=\"_blank\">Sharethrough</a> providing an update on how their use of Apache Spark has continued to expand.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nSharethrough is an advertising technology company that provides native, in-feed advertising software to publishers and advertisers. Native, in-feed ads are designed to match the form and function of the sites they live on, which is particularly important on mobile devices where interruptive advertising is less effective. For publishers, in-feed monetization has become a major revenue stream for their mobile sites and applications. For advertisers, in-feed ads have been proven to drive more brand lift than interruptive banner advertisements.\n\nSharethrough’s publisher and advertiser technology suite is capable of optimizing the format of an advertisement for seamless placement on content publishers websites and apps. This involves four major steps: Targeting the right users, optimizing the creative content for the target, scoring the content quality, and allowing the advertiser to bid on the actual ad placement in a real-time auction.\n\nAll four steps are necessary to optimize an advertiser’s return on marketing investment. But this requires real-time capabilities in the following three areas:\n<ul>\n \t<li><strong>Creative optimization:</strong> Choosing the best performing content variant from a seemingly infinite number of variations in thumbnails, headlines, descriptions etc.</li>\n \t<li><strong>Spend tracking:</strong> Advertisers expect automatic adjustment (“programmatic” in advertising parlance) to real-time bidding algorithms to achieve their campaign goals given their parameters and budget. A required feature of Sharethrough’s platform is to provide real-time adjustments into how content engagement consumes an advertising budget.</li>\n \t<li><strong>Operational monitoring:</strong> When expected behaviour falls outside of positive or negative norms (e.g. traffic spikes during the Oscars or lowered spend), these need to be understood and addressed in a timely manner to answer the question - “Is this event expected and/or acceptable?”</li>\n</ul>\nBetter creative content and optimal placement translate into better consumer engagement and higher conversion, but Sharethrough needs to measure the business impact of these optimizations in real time.\n<h2>Technology Challenge</h2>\nThe technology that Sharethrough was using prior to Spark was not able to accommodate the short feedback cycles required to meet these three objectives.\n\nAfter migrating from Amazon Elastic MapReduce in 2010, we deployed the Cloudera Distribution of Hadoop on Amazon Web Services, primarily for batch use cases such as Extract, Transform and Load (ETL). These batch runs are used for intermittent performance reporting and billing throughout the day, with delays on the order of hours, not minutes. After the launch of our new platform in 2013, it became apparent that Hadoop was not well suited to serve Sharethrough’s increasingly real-time needs.\n\nSharethrough’s data processing pipeline relies on Apache Flume to write web server log data into HDFS in 64MB increments based on the default HDFS block size. Sharethrough runs a set of MapReduce jobs at periodic intervals with the resulting output written to a data warehouse using Sqoop.\n\nThis setup generated insights with a delay of more than one hour. Sharethrough was unable to update the models sooner than these existing batch workflows allowed. This meant that advertisers could not be sure that they had optimized the return on their content investment because any decisions were taken on data that was a few hours old.\n<h2>Solution</h2>\nIn the middle of 2013, we turned to Apache Spark and in particular Spark Streaming because we needed a system to process click stream data in real time.\n\nIn my Spark Summit 2014 <a href=\"https://www.youtube.com/watch?v=0QXzKqMPgWQ&amp;index=2&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS\" target=\"_blank\">talk</a>, I highlighted the reasons for choosing Spark:\n\n“We found Spark Streaming to be a perfect fit for us because of its easy integration into the Hadoop ecosystem, powerful functional programming API, low-friction interoperability with our existing batch workflows and broad ecosystem endorsement.”\n\nSpark is compatible with our existing investments in Hadoop. This means that existing HDFS files can be used as an input for Spark computations, and Spark can use HDFS for persistent storage.\n\nAt the same time, Spark makes it easy for developers lacking an understanding of the various elements of Hadoop to become productive. While Spark integrates with Hadoop, it does not require knowledge of HDFS, MapReduce or the various Hadoop processing engines. At Sharethrough, much of the backend code is written using Scala, and therefore blends into Spark very naturally since Spark supports the Scala APIs.\n\nThis allows our developers to work at the level of the actual business logic and data pipeline that specify what has to happen. Spark then figures out how this has to happen, coordinating lower-level tasks such as data movement and recovery. The resulting code is quite concise due to the Spark API.\n\nWe’re using Spark for streaming now but the opportunity to also use Spark for batch processing is really appealing to us. Spark provides a unified development environment and runtime across both batch and real-time workloads, allowing reusability between batch and streaming jobs. It also makes it much easier to combine arriving real-time data with historical data in one analysis.\n\nFinally, the community support available with Spark is quite helpful, from mailing lists and an ecosystem of code contributors all the way to companies like Cloudera, MapR and Datastax that offer professional support.\n<h2>Deployment in Detail</h2>\nSharethrough runs Spark on 10 AWS m1.xlarge nodes, ingests 200 GB per day and is using Mesos for cluster management.\n\nFollowing the principles of the Lambda architecture, Sharethrough uses Hadoop for the batch layer of its architecture, what we call the “cold path”, and Spark for the “hot path” real-time layer.\n\nIn the hot path, Flume writes out all the clickstream data to RabbitMQ. Next, Spark reads from RabbitMQ at a (configurable) batch size of five seconds. The resulting output updates the predictive models that run our business. The end-to-end process completes within a few seconds, including the Spark processing time and the time taken by Flume to transmit the data to RabbitMQ.\n\nBecause of API consistency, our engineers design and test locally in a simple batch mode and then run the same job in production using streaming mode. This enables the system to achieve the desired optimization required for real-time bidding.\n\nGoing forward, we aim to simplify the upstream components of our data pipeline using Amazon Kinesis. Kinesis would supplant existing queueing systems like RabbitMQ by connecting to all sources such as web servers, machine logs or mobile devices. Kinesis would then form the central hub from which all applications including Spark can pull data. Spark support for Kinesis was added as part of the recent Spark 1.1 release in September 2014.\n<h2>Value Realized</h2>\nSpark delivers on our business objectives of improving creative optimization, spend tracking and operational monitoring. Spark makes it easier to deliver ads on budget, which is particularly critical for campaigns that may only run for a few days. Spend can be tracked and operational issues adjusted in real-time. For instance, if Sharethrough releases code that does not render well on some third-party sites, this can now be detected and fixed immediately.\n\nBut Spark also creates value for our technical team. Engineers can conduct and learn from real-time experiments much more quickly than before. Code re-use and testing is another significant benefit. Because of the higher abstraction level and unified programming model of Spark, Sharethrough can much more easily reuse the code from one job to create another job in a modular fashion just by replacing a few lines of code. This results in much cleaner looking code, which is easier to debug, test, reuse and maintain. Furthermore we can use a single analytics cluster to provide both real-time stream processing as well as batch analytic workflows without the operational and resource overhead of supporting two different clusters with different latency requirements.\n<h2>To Learn More:</h2>\n<ol>\n \t<li><a href=\"https://www.youtube.com/watch?v=0QXzKqMPgWQ&amp;index=2&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS\" target=\"_blank\">https://www.youtube.com/watch?v=0QXzKqMPgWQ&amp;index=2&amp;list=PL-x35fyliRwimkfjeg9CEFUSDIE1F7qLS</a></li>\n \t<li><a href=\"http://spark-summit.org/wp-content/uploads/2014/07/Spark-Streaming-for-Realtime-Auctions-Russell-Cardullo.pdf\" target=\"_blank\">http://spark-summit.org/wp-content/uploads/2014/07/Spark-Streaming-for-Realtime-Auctions-Russell-Cardullo.pdf</a></li>\n</ol></td></tr><tr><td>List(Sean Kandel (CTO at Trifacta))</td><td>List(Company Blog, Partners)</td><td>List(2014-10-09, 2014-10-09, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.trifacta.com\" target=\"_blank\">Trifacta</a> after having their data transformation platform <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Spark.”</a></div>\n\n<hr>\n\nToday we announced v2 of the Trifacta Data Transformation Platform, a release that emphasizes the important role that Hadoop plays in the new big data enterprise architecture. With Trifacta v2 we now support transforming data of all shapes and sizes in Hadoop. This means supporting Hadoop-specific data formats as both inputs and outputs in Trifacta v2 - data formats such as Avro, ORC and Parquet. It also means intelligently executing data transformation scripts through not only MapReduce, which was available in Trifacta v1, but also Spark. Trifacta v2 has been officially Certified on Spark by Databricks.\n\nOur partnership with Databricks brings the performance and flexibility of the Spark data processing engine to the world of data wrangling. It has been a pleasure to work with the original creators of Spark and to introduce a new category of applications to the Spark community. Our inspiration to integrate with Spark was in part the sheer power of the technology. But it was also prompted by the tremendous momentum of the open source Apache Spark community and project.  We’ve seen a growing number of technology and Fortune 500 companies select Spark as a critical component of their investments in Hadoop implementations.\n\nWith support now from all of the major Hadoop distributions, including Cloudera, Hortonworks, MapR and Pivotal, Spark is certainly here to stay as a foundational component of the Hadoop ecosystem. And having tested Spark against many different data transformation use cases, we now know why. With Spark under the hood of Trifacta, we can now execute large-scale data transformations at interactive response rates. This capability complements the execution frameworks that we introduced in Trifacta v1, where we supported instant and batch execution. In Trifacta v1, we could either execute over small data instantaneously in the browser or we could operate over large volumes in batch mode by compiling transformation scripts to execute in MapReduce. Now in Trifacta v2, we can intelligently select between in-browser, Spark and MapReduce execution for the user, so that our customers can focus on analysis instead of technical details.\n\nWe leverage Spark’s flexible execution model to drive low-latency processing for a variety of data transformation workloads. For instance, Spark is suitable for interactive data structuring and cleaning transformations, iterative machine learning routines that power Trifacta’s Predictive InteractionTM technology and efficient analytic queries for Visual Data Profiling.\n\nWhat made it relatively easy to plug native Spark execution into our Data Transformation Platform was Trifacta’s declarative transformation language, Wrangle. Wrangle is a data transformation language that is designed to translate visual interactions that users have in Trifacta into natively executable code that can run on variety of different processing frameworks including MapReduce and Spark.\n\nWith the immense technical talent of both organizations, I am looking forward to seeing what we’re able to build together and the impact it will have on big data. If you’re interested in learning more about Wrangle, Trifacta’s Domain-Specific Language or how our architecture makes it easy to plug into multiple data processing frameworks, I’ll be speaking next week with Joe Hellerstein at Strata New York on the <a href=\"http://strataconf.com/stratany2014/public/schedule/detail/36612\" target=\"_blank\">topic</a> or stay tuned to the <a href=\"http://www.trifacta.com/product/platform/\" target=\"_blank\">product page</a> on our website.</td></tr><tr><td>List(Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-10-10, 2014-10-10, UTC)</td><td><strong>Update November 5, 2014</strong>: Our benchmark entry has been reviewed by the benchmark committee and Apache Spark has won the <a href=\"http://sortbenchmark.org/\">Daytona GraySort contest</a> for 2014! Please see this <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">new blog post for update</a>.\n\nApache Spark has seen phenomenal adoption, being widely slated as the successor to Hadoop MapReduce, and being deployed in clusters from a handful to thousands of nodes. While it was clear to everybody that Spark is more efficient than MapReduce for data that fits in memory, we heard that some organizations were having trouble pushing it to large scale datasets that could not fit in memory. Therefore, since the inception of Databricks, we have devoted much effort, together with the Spark community, to improve the stability, scalability, and performance of Spark. Spark works well for gigabytes or terabytes of data, and it should also work well for petabytes.\n\nTo evaluate these improvements, we decided to participate in the <a href=\"http://sortbenchmark.org/\">Sort Benchmark</a>. With help from Amazon Web Services, we participated in the Daytona Gray category, an industry benchmark on how fast a system can sort 100 TB of data (1 trillion records). Although our entry is still under review, we are eager to share with you our submission. The previous world record was 72 minutes, set by Yahoo using a Hadoop MapReduce cluster of 2100 nodes. Using Spark on 206 EC2 nodes, we completed the benchmark in 23 minutes. This means that Spark sorted the same data <b>3X faster</b> using <b>10X fewer machines</b>. All the sorting took place on disk (HDFS), without using Spark's in-memory cache.\n\nAdditionally, while no official petabyte (PB) sort competition exists, we pushed Spark further to also sort 1 PB of data (10 trillion records) on 190 machines in under 4 hours. This PB time beats previously reported results based on Hadoop MapReduce (16 hours on 3800 machines). To the best of our knowledge, this is the first petabyte-scale sort ever done in a public cloud.\n<table class=\"table\">\n<thead>\n<tr>\n<th width=\"25%\"></th>\n<th width=\"25%\"><a href=\"http://sortbenchmark.org/Yahoo2013Sort.pdf\"><b>Hadoop\nWorld Record</b></a></th>\n<th width=\"25%\"><b>Spark\n100 TB *</b></th>\n<th width=\"25%\"><b>Spark\n1 PB</b></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Data Size</td>\n<td>102.5 TB</td>\n<td>100 TB</td>\n<td>1000 TB</td>\n</tr>\n<tr>\n<td>Elapsed Time</td>\n<td>72 mins</td>\n<td>23 mins</td>\n<td>234 mins</td>\n</tr>\n<tr>\n<td># Nodes</td>\n<td>2100</td>\n<td>206</td>\n<td>190</td>\n</tr>\n<tr>\n<td># Cores</td>\n<td>50400</td>\n<td>6592</td>\n<td>6080</td>\n</tr>\n<tr>\n<td># Reducers</td>\n<td>10,000</td>\n<td>29,000</td>\n<td>250,000</td>\n</tr>\n<tr>\n<td>Rate</td>\n<td>1.42 TB/min</td>\n<td>4.27 TB/min</td>\n<td>4.27 TB/min</td>\n</tr>\n<tr>\n<td>Rate/node</td>\n<td>0.67 GB/min</td>\n<td>20.7 GB/min</td>\n<td>22.5 GB/min</td>\n</tr>\n<tr>\n<td>Sort Benchmark Daytona Rules</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Environment</td>\n<td>dedicated data center</td>\n<td>EC2 (i2.8xlarge)</td>\n<td>EC2 (i2.8xlarge)</td>\n</tr>\n</tbody>\n</table>\n* not an official sort benchmark record\n\n<strong><strong> </strong></strong>\n<h2>Why sorting?</h2>\nAt the core of sorting is the <em>shuffle</em> operation, which moves data across all machines. Shuffle underpins almost all distributed data processing workloads. For example, a SQL query joining two disparate data sources uses shuffle to move tuples that should be joined together onto the same machine, and collaborative filtering algorithms such as <a href=\"https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html\">ALS</a> rely on shuffle to send user/product ratings and weights across the network.\n\nMost data pipelines start with a large amount of raw data, but as the pipeline progresses, the amount of data is reduced due to filtering out irrelevant data or more compact representation of intermediate data. A SQL query on 100 TB of raw input data most likely only shuffles a tiny fraction of the 100 TB across the network. This pattern is also reflected in the naming of MapReduce itself.\n\nSorting, however, is one of the most challenging because there is no reduction of data along the pipeline. Sorting 100 TB of input data requires shuffling 100 TB of data across the network. As a matter of fact, the Daytona competition requires us to replicate both input and output data for fault-tolerance, and thus sorting 100 TB of data effectively generates 500 TB of disk I/O and 200 TB of network I/O.\n\nFor the above reasons, when we were looking for metrics to measure and improve Spark, sorting, one of the most demanding workloads, became a natural choice to focus on.\n\n<strong><strong> </strong></strong>\n<h2>Tell me the technical work that went behind making this possible</h2>\nA lot of development has gone into improving Spark for very large scale workloads. In particular, there are three major pieces of work that are highly relevant to this benchmark.\n\nFirst and foremost, in Apache Spark 1.1 we introduced a new shuffle implementation called <b>sort-based shuffle</b> (<a href=\"https://issues.apache.org/jira/browse/SPARK-2045\">SPARK-2045</a>). The previous Spark shuffle implementation was hash-based that required maintaining P (the number of reduce partitions) concurrent buffers in memory. In sort-based shuffle, at any given point only a single buffer is required. This has led to substantial memory overhead reduction during shuffle and can support workloads with hundreds of thousands of tasks in a single stage (our PB sort used 250,000 tasks).\n\nSecond, we revamped the <b>network module</b> in Spark based on Netty’s Epoll native socket transport via JNI (<a href=\"https://issues.apache.org/jira/browse/SPARK-2468\">SPARK-2468</a>). The new module also maintains its own pool of memory, thus bypassing JVM’s memory allocator, reducing the impact of garbage collection.\n\nLast but not least, we created a new <b>external shuffle service</b> (<a href=\"https://issues.apache.org/jira/browse/SPARK-3796\">SPARK-3796</a>) that is decoupled from the Spark executor itself. This new service builds on the aforementioned network module and ensures that Spark can still serve shuffle files even when the executors are in GC pauses.\n<img class=\"alignnone size-full wp-image-1732\" src=\"https://databricks.com/wp-content/uploads/2014/10/ganglia-network-daytona.png\" alt=\"Network activity during sort\" width=\"550\" />\nWith these three changes, our Spark cluster was able to sustain 3GB/s/node I/O activity during the map phase, and 1.1 GB/s/node network activity during the reduce phase, saturating the 10Gbps link available on these machines.<strong><strong>\n</strong></strong>\n\n<strong><strong> </strong></strong>\n<h2>What other nitty-gritty details have you not told me yet?</h2>\n<b>TimSort</b>: In Apache Spark 1.1, we switched our default sorting algorithm from quicksort to <a href=\"https://en.wikipedia.org/wiki/Timsort\">TimSort</a>, a derivation of merge sort and insertion sort. It performs better than quicksort in most real-world datasets, especially for datasets that are partially ordered. We use TimSort in both the map and reduce phases.\n\n<b>Exploiting Cache Locality</b>: In the sort benchmark, each record is 100 bytes, where the sort key is the first 10 bytes. As we were profiling our sort program, we noticed the cache miss rate was high, because each comparison required an object pointer lookup that was random. We redesigned our record in-memory layout to represent each record as one 16-byte record (two longs in the JVM), where the first 10 bytes represent the sort key, and the last 4 bytes represent the position of the record (in reality it is slightly more complicated than this due to endianness and signedness). This way, each comparison only required a cache lookup that was mostly sequential, rather than a random memory lookup. Originally proposed by Chris Nyberg et al. in AlphaSort, this is a common technique used in high-performance systems.\n\nSpark's nice programming abstraction and architecture allow us to implement these improvements in the user space (without modifying Spark) in a few lines of code. Combining TimSort with our new layout to exploit cache locality, the CPU time for sorting was reduced by a factor of 5.\n\n<b>Fault-tolerance at Scale</b>: At scale a lot of things can break. In the course of this experiment, we have seen nodes going away due to network connectivity issues, the Linux kernel spinning in a loop, or nodes pausing due to memory defrag. Fortunately, Spark is fault-tolerant and recovered from these failures.\n\n<b>Power of the Cloud (AWS)</b>: As mentioned previously, we leveraged 206 i2.8xlarge instances to run this I/O intensive experiment. These instances deliver high I/O throughput via SSDs. We put these instances in a placement group in a VPC to enable enhanced networking via single root I/O virtualization (SR-IOV). Enabling enhanced networking results in higher performance (10Gbps), lower latency, and lower jitter. We would like to thank everyone involved at AWS for their help making this happen including: the AWS EC2 services team, AWS EC2 Business Development team, AWS product marketing and AWS solutions architecture team. Without them this experiment would not have been possible.\n<h2>Isn’t Spark in-memory only?</h2>\nThis has always been one of the most common misconceptions about Spark, especially for people new to the community. Spark is well known for its in-memory performance, but from its inception Spark was designed to be a general execution engine that works both in-memory and on-disk. Almost all Spark operators perform external operations when data does not fit in memory. More generally, Spark’s operators are a strict superset of MapReduce.\n\nAs demonstrated by this experiment, Spark is capable of processing datasets many times larger than the aggregate memory in a cluster.\n<h2>Summary</h2>\nDatabricks, with the help of the Spark community, has contributed many improvements to Apache Spark to improve its performance, stability, and scalability. This enabled Databricks to use Apache Spark to sort 100 TB of data on 206 machines in 23 minutes, which is 3X faster than the previous Hadoop 100TB result on 2100 machines. Similarly, Databricks sorted 1 PB of data on 190 machines in less than 4 hours, which is over 4X faster than the previous Hadoop 1PB result on 3800 machines.\n\nOutperforming large Hadoop MapReduce clusters on sorting not only validates the work we have done, but also demonstrates that Spark is fulfilling its promise to serve as a faster and more scalable engine for data processing of all sizes. We hope that Spark enables equally dramatic improvements in time and cost for all our users.</td></tr><tr><td>List(Reza Zadeh)</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2014-10-20, 2014-10-20, UTC)</td><td><div class=\"post-meta\">Our friends at Twitter have contributed to MLlib, and this post uses material from Twitter’s description of its <a href=\"https://blog.twitter.com/2014/all-pairs-similarity-via-dimsum\" target=\"_blank\">open-source contribution</a>, with permission. The associated <a href=\"https://github.com/apache/spark/pull/1778\" target=\"_blank\">pull request</a> is slated for release in Apache Spark 1.2.</div>\n\n<hr />\n\n<h2>Introduction</h2>\nWe are often interested in finding users, hashtags and ads that are very similar to one another, so they may be recommended and shown to users and advertisers. To do this, we must consider many pairs of items, and evaluate how “similar” they are to one another.\n\nWe call this the “all-pairs similarity” problem, sometimes known as a “similarity join.” We have developed a new efficient algorithm to solve the similarity join called “Dimension Independent Matrix Square using MapReduce,” or <a href=\"http://arxiv.org/abs/1304.1467\" target=\"_blank\">DIMSUM</a> for short, which made one of Twitter’s most expensive batch computations 40% more efficient.\n\nTo describe the problem we’re trying to solve more formally, when given a dataset of sparse vector data, the all-pairs similarity problem is to find all similar vector pairs according to a similarity function such as <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" target=\"_blank\">cosine similarity</a>, and a given similarity score threshold.\n\nNot all pairs of items are similar to one another, and yet a naive algorithm will spend computational effort to consider even those pairs of items that are not very similar. The brute force approach of considering all pairs of items quickly breaks, since its computational effort scales quadratically.\n\nFor example, for a million vectors, it is not feasible to check all roughly trillion pairs to see if they’re above the similarity threshold. Having said that, there exist clever sampling techniques to focus the computational effort on only those pairs that are above the similarity threshold, thereby making the problem feasible. We’ve developed the DIMSUM sampling scheme to focus the computational effort on only those pairs that are highly similar, thus making the problem feasible.\n<h2>Intuition</h2>\nThe main insight that allows gains in efficiency is sampling columns that have many non-zeros with lower probability. On the flip side, columns that have fewer non-zeros are sampled with higher probability. This sampling scheme can be shown to provably accurately estimate cosine similarities, because those columns that have many non-zeros have more trials to be included in the sample, and thus can be sampled with lower probability.\n\nThere is an in-depth description of the algorithm on the <a href=\"https://blog.twitter.com/2014/all-pairs-similarity-via-dimsum\" target=\"_blank\">Twitter Engineering blog post</a>.\n<h2>Experiments</h2>\nWe run DIMSUM on a production-scale ads dataset. Upon replacing the traditional cosine similarity computation in late June, we observed 40% improvement in several performance measures, plotted below.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/Dimsum-first.png\" alt=\"\" align=\"middle\" />\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 35%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/Dimsum-second.png\" alt=\"\" align=\"middle\" />\n<h2>Usage from Spark</h2>\nThe algorithm is available in Apache Spark MLlib as a method in <a href=\"http://spark.apache.org/docs/1.1.0/mllib-data-types.html#rowmatrix\" target=\"_blank\">RowMatrix</a>. This makes it easy to use and access:\n\n[scala]\n// Arguments for input and threshold\nval filename = args(0)\nval threshold = args(1).toDouble\n\n// Load and parse the data file.\nval rows = sc.textFile(filename).map { line =&gt;\n  val values = line.split(' ').map(_.toDouble)\n  Vectors.dense(values)\n}\nval mat = new RowMatrix(rows)\n\n// Compute similar columns perfectly, with brute force.\nval simsPerfect = mat.columnSimilarities()\n\n// Compute similar columns with estimation using DIMSUM\nval simsEstimate = mat.columnSimilarities(threshold)\n[/scala]\n\nHere is an <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\" target=\"_blank\">example invocation of DIMSUM</a>. This functionality will be available as of Spark 1.2.\n\nAdditional information can be found in the <a href=\"https://gigaom.com/2014/09/24/twitter-open-sourced-a-recommendation-algorithm-for-massive-datasets/\" target=\"_blank\">GigaOM article</a> covering the DIMSUM algorithm.</td></tr><tr><td>List(Jeff Feng (Product Manager at Tableau Software))</td><td>List(Company Blog, Partners)</td><td>List(2014-10-15, 2014-10-15, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.tableausoftware.com\" target=\"_blank\">Tableau Software</a>, whose visual analytics software is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/10/Tableau-SparkSQL.png\" alt=\"\" align=\"middle\" />\n<h2>Apache Spark - The Next Big Innovation</h2>\nOnce every few years or so, the big data open source community experiences a major innovation that advances the capabilities of data processing frameworks. For many years, MapReduce and the Hadoop open-source platform served as an effective foundation for the distributed processing of large data sets. Then last year, the introduction of YARN provided the resource manager needed to enable interactive workloads, bringing data processing performance to another level. However, as organizations entrust big data platforms to handle more of their critical business information, the volume and variety of data will continue to grow rapidly as will the need for speed to insight and action on that data. As most of the community would agree, we believe that Apache Spark is the next big innovation and platform to help take on the data challenges of tomorrow.\n\nThe decision to support Spark was easy – it was largely driven by our customers. Spark’s usefulness as a powerful all-around big data platform for interactive queries and data processing has made it <a href=\"http://community.tableausoftware.com/ideas/3445\" target=\"_blank\">one of the most frequently requested data sources</a> in the last couple months. As a company, we strive to make the data sources that are important to our customers universally accessible. It was also prompted by the strong momentum of the Apache Spark project and the broad uptake in community support. Within the last 8 months, 10 of the Hadoop distributors including Cloudera, Hortonworks and MapR have committed to ship Spark as a part of their distribution as well as accelerate the development of the project. Lastly, Tableau was inspired to integrate with Spark because it is a technology that was architected intelligently from the very beginning as demonstrated by some of the <a href=\"https://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html\" target=\"_blank\">early performance results</a>. In addition, our co-founders like to say that we are just getting started at Tableau, and we believe the same is true of Spark.\n<h2>Tableau Software is “Certified on Spark”</h2>\nToday, we are delighted to announce that Tableau Software is now “Certified on Spark.” Tableau sought qualification in the program so that our customers feel confident that the integration of the technologies works seamlessly and delightfully. We also want to help maintain the compatibility of Spark SQL across different distributions as it helps to facilitate a vibrant open source community - one of collaboration and integration. Tableau is committed to supporting Spark and ensuring compatibility with future releases.\n\nIn conjunction with our certification and our mission to “help people see and understand their data,” Tableau is launching a new native Spark SQL connector for both Windows and Mac (currently in beta). We are excited to work with Databricks to bring the performance and versatility of the Spark data processing engine to the masses through visual analysis.\n<h2>Tableau + Spark = Better Together</h2>\nTableau’s integration with Spark brings tremendous value to the Spark community - users can visually analyze their data without writing a single line of Spark SQL code. That’s a big deal because creating a visual interface to your data expands the Spark technology beyond data scientists and data engineers to all business users. The Spark connector takes advantage of Tableau’s flexible connection architecture that gives customers the option to connect live and issue interactive queries, or use Tableau’s fast in-memory database engine. Tableau also provides users the capability to blend Spark data with data from any of our other 40+ direct connectors, empowering users to leverage their existing data assets wherever they are.\n\nNow to see Tableau and Spark SQL in action, we have created a short video demonstrating how users can connect to a Spark cluster and interact with data in Tableau.\n\n<iframe src=\"//www.youtube.com/embed/OKcIf6UdK7c\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n<h2>To Learn More:</h2>\n<strong>Read:</strong> To read more about Tableau’s integration with Spark SQL, please check out our post on the <a href=\"http://www.tableausoftware.com/about/blog/2014/10/tableau-spark-sql-big-data-just-got-even-more-supercharged-33799\" target=\"_blank\">Tableau blog</a>.\n\n<strong>Join the beta:</strong> In order to use Tableau directly against Spark, you’ll need to be a part of the beta program. If you’re interested in joining, <a href=\"mailto:jfeng@tableausoftware.com\" target=\"_blank\">please send us an email</a>.</td></tr><tr><td>List(Scott Walent)</td><td>List(Announcements, Company Blog, Events)</td><td>List(2014-10-23, 2014-10-23, UTC)</td><td>The call for presentations for the inaugural <a href=\"http://spark-summit.org/east\">Spark Summit East</a> is now open. Please join us in New York City on March 18-19, 2015 to share your experience with Apache Spark and celebrate its growing community.\n\nSpark Summit East is looking for presenters who would like to showcase how Spark and its related technologies are used in applications, development, data science and research. Please visit our <a href=\"http://www.spark-summit.org/east/2015/CFP\">submission page</a> for additional details. The Deadline for submissions is December 5, 2014 at 11:59pm PST.\n\nSpark Summit East is the leading event for <a href=\"http://spark.apache.org\">Apache Spark </a>users, developers and vendors. It is an exciting opportunity to meet analysts, researchers, developers and executives interested in utilizing Spark technology to answer big data questions.\n\nIf you missed <a href=\"http://spark-summit.org/2014\">Spark Summit 2014</a>, all the content is available online for free.</td></tr><tr><td>List(Ari Himmel (CEO at Faimdata), Nan Zhu (Chief Architect at Faimdata))</td><td>List(Company Blog, Partners)</td><td>List(2014-10-27, 2014-10-27, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.faimdata.com\" target=\"_blank\">Faimdata</a>, whose Consumer Data Intelligence Service is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\n<h2>Forecasting, Analytics, Intelligence, Machine Learning</h2>\nFaimdata’s Consumer Data Intelligence Service is a turnkey Big Data solution that provides comprehensive infrastructure and applications to retailers. We help our clients form close connections with their customers and make timely business decisions, using their existing data sources. The unified data processing pipeline deployed by Faimdata has three core focuses. They are (i) our Personalization Service that identifies the personal preferences and buying behaviors of each individual consumer using recommendation/machine learning algorithms; (ii) our Data Analytic Workbench where clients execute high performance multi-dimensional analytics across all distributed data sources using pre-defined or ad-hoc SQL-like languages; and (iii) our Social Intelligence Engine where clients can monitor social media related to their brands, products and competitors and link customer profiles to existing CRM data for more effective sales.\n\nBy using Faimdata, our retail clients have integrated their major data sources into a high performance, unified data pipeline for the very first time. Faimdata’s infrastructure reduces overhead related to managing and maintaining data, and our applications empower our clients to make data-driven business decisions that increase customer satisfaction, strengthen marketing and product innovation, and most importantly increase revenue.\n<h2>Transforming Technology</h2>\nData driven decision-making is now mandatory for retailers to compete in the marketplace. Businesspeople want to use their data sources to realize value and understand their customers. They want to design, market and sell the right products to the right people at the right time. By listening to data with advanced analytical tools, businesses can offer customers highly relevant products and brand experiences.\n\nYet, scalability, reliability and extracting value beyond retroactive reporting is costly and difficult for businesses, even though most enterprise technologies already collect a tremendous amount of information. Many retailers simply do not have the technology infrastructure to maximize the usefulness of their data. We strive to resolve the tension between business needs, existing technologies and IT department limitations by empowering technology managers to satisfy client demands, simply and affordably.\n\nThe IT department's prevailing responsibilities of managing and maintaining technologies are now moving to 3rd Platform technology services. Faimdata is in the middle of this shift, providing a turnkey solution that matches the customer’s rich source data to our customizable, real-time and predictive Big Data application. We thereby enable a broad range of retail activities, including: product innovation, data intelligence, CRM, supply chain, merchandising, marketing and ecommerce.\n<h2>The Apache Spark Advantage: Simplicity + Power</h2>\nAs such, it is an honor and pleasure to announce that the Faimdata Consumer Data Intelligence Service is now “Certified on Apache Spark”. Faimdata has been using Spark to enable our Services since the very beginning of our development and it’s a decision that has served our clients well. Often, the biggest challenge in providing a Big Data solution is the complexity of the underlying infrastructure. Thanks to Spark’s brilliant architecture of integrated solutions for different types of workloads, Faimdata is able to develop and seamlessly deploy turnkey applications that satisfy our customer’s requirements within a unified framework. Faimdata’s core systems all leverage Spark, including our ETL System (Spark Core), our Personalization Service (Mllib, GraphX), our Analytics Workbench (Spark Streaming, Spark Core, and Spark SQL), and our Social Intelligence (Spark Core and Spark SQL).\n\nSpark also has unbeatable speed that is critical for retailers. For example, in a scenario of iterative algorithms, Spark is able to perform 100x faster than traditional Hadoop. Spark SQL is also one of the fastest SQL query engines available. This is exactly what our client’s demand, and what Faimdata needs, in order to provide real-time actionable insights and predictive analytics.\n\nThe recent performance of Databricks at Graysort is further testimony to Spark’s speed and scalability and we congratulate Databricks on this notable achievement.\n<h2>Continued Collaboration</h2>\nIt is very exciting for us to join leading innovators in Big Data and be a part of the exemplary Spark open source community. We look forward to the future with Databricks and Spark.\n\nTo learn more about Faimdata please visit our website <a href=\"http://www.faimdata.com\" target=\"_blank\">faimdata.com</a> and send us an email at <a href=\"mailto:ari@faimdata.com\" target=\"_blank\">ari@faimdata.com</a> or on twitter @arihimmel</td></tr><tr><td>List(John Kreisa (VP of Strategic Marketing at Hortonworks))</td><td>List(Company Blog, Partners)</td><td>List(2014-10-31, 2014-10-31, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.hortonworks.com\" target=\"_blank\">Hortonworks</a> announcing a broader partnership with Databricks around Apache Spark.</div>\n\n<hr>\n\nAt Hortonworks we are very excited by the emerging use cases and potential of Apache Spark and Apache Hadoop.   Spark is representative of just one of the shifts underway in the data landscape towards memory optimized processing, that when combined with Hadoop, can enable a new generation of applications.\n\nWe are excited to announce that Hortonworks and Databricks have extended our partnership focus from providing a <a href=\"https://databricks.com/spark/certification/certified-spark-distribution\" target=\"_blank\">Certified Spark Distribution</a> to include a shared vision to further Apache Spark as an enterprise ready component of the Hortonworks Data Platform. We are closely aligned on a strategy and vision of bringing 100% open source software to market for the enterprise and supporting the customer use cases.\n\nHaving two leaders in our respective communities come together makes sense for the community and for customers. Together with Databricks’ expertise in Apache Spark combined with Hortonworks expertise in building a complete enterprise Hadoop data platform, we are better able to engineer solutions that meet the enterprise requirements for big data processing.\n\nFrom the Hortonworks perspective, our view has been very consistent: enabling a wide range of batch, interactive, real-time data processing applications to run simultaneously within a single <a href=\"http://www.hortonworks.com/hdp\" target=\"_blank\">enterprise Hadoop data platform</a> against shared datasets.  We believe applications leveraging Spark can benefit greatly from enabling it as a natively integrated engine within the Hortonworks Data Platform: integrated with YARN and supported by a common set of services for Security, Operations and Governance.\n\nIn June of 2014 we endorsed the standard set of open APIs for application development for Spark on the Hortonworks Data Platform making it a Certified Spark Distribution. This allows developers to build applications on this new engine while enabling operators to leverage a common data platform (Hadoop).\n\nWe are extending our partnership to include a commitment to invest in the following areas with Databricks:\n\n\n<ul>\n<li><strong>Engineering:</strong>  Spark optimized on YARN enables Spark-based applications to share the resources and operate along side other workloads, whether batch or streaming.  Additionally integrating Spark with the Security, Operations, and Governance components of the Hortonworks Data Platform/Apache Hadoop provides fully tested and enterprise-ready modern data platform.</li>\n<li><strong>Customers:</strong> Hortonworks and Databricks will jointly collaborate to support the usage of Spark and the Hortonworks Data Platform for our customers.</li>\n<li><strong>Open Source Foundation:</strong> We share a common vision for working with the open source community and delivering innovation, which will land into the upstream projects and is then delivered as enterprise ready software.</li>\n</ul>\n\nWe look forward to working with the Databricks team to further enable Spark on Hadoop.</td></tr><tr><td>List(Sachin Chawla (VP of Engineering))</td><td>List(Company Blog, Partners)</td><td>List(2014-11-25, 2014-11-25, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.skytree.net\" target=\"_blank\">Skytree</a>, whose Skytree Infinity platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\n<h2>To Infinity and Beyond - Big Data at the speed of light</h2>\nAstronomers were into Big Data before it was big. In order to learn about the history of the universe, they needed to observe and record billions and billions of astronomical objects and perform heavy-duty analysis on the resulting massive datasets. Available predictive methods were not scalable to the size of data sets they were dealing with so they turned to Skytree to obtain unprecedented performance and accuracy on the largest datasets ever collected. Fast-forward a decade or so and the need to store, access, process and analyze datasets of astronomical sizes is now mainstream in the guise of Big Data analytics.\n\n<a href=\"http://www.skytree.net/products/skytree-infinity/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/11/Skytree.png\" alt=\"\" width=\"\" height=\"300\" /></a>\n\nWe have built our Advanced Analytics platform, Skytree Infinity, from the ground up to provide an ultra-fast platform that makes it possible to use the most advanced and accurate machine learning methods on extremely large datasets to extract actionable insights and predictions. Furthermore, the infrastructure available for big data has matured to provide scalable and reliable data management and processing capabilities. The combination of our state-of-the-art machine learning and scalable infrastructure allows for an unprecedented, easy-to-consume solution for big data analysis.\n<h2>Skytree Infinity integration with Apache Spark</h2>\nWhile Skytree Infinity’s analysis capabilities are remarkable, we are acutely aware of the need for easy, seamless integration to other closely related pieces of the Big Data analytics workflow. The Apache Spark project has garnered broad interest and adoption by the industry. Spark offers a rich platform for iterative processing and data manipulation that is well suited to machine learning. The combination of Skytree and Spark provides a widely usable solution for big data analytics that provides a fast, reliable, scalable, and manageable data cleansing/munging/analysis platform for the most challenging business problems.\n\nOur most recent offering in this direction is the certification of Skytree Infinity, the enterprise Machine Learning platform, on Apache Spark by Databricks.\n\n<a href=\"http://www.skytree.net/products/skytree-infinity/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/11/skytree-2.png\" alt=\"\" width=\"\" height=\"300\" /></a>\n\nUsers can access data from multiple sources including RDBMSs, HDFS and Hive, and use Spark calls through Skytree Infinity to perform the most common ML-centric data pre-processing, munging and featurization tasks, to which they can apply advanced machine learning methods of their choice to obtain accurate, actionable predictions and insights from their data.\n<h2>Continued Collaboration</h2>\nWe are happy to be a member of the Spark community, and we are excited to explore the new vistas that this new integration opens.\n\nTo learn more about Skytree Infinity, please visit our website at <a href=\"http://www.skytree.net\" target=\"_blank\">www.skytree.net</a></td></tr><tr><td>List(Sonal Goyal (CEO))</td><td>List(Company Blog, Partners)</td><td>List(2014-12-02, 2014-12-02, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://nubetech.co/\" target=\"_blank\">Nube Technologies</a>, whose Reifier platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\n<h2>About Nube Technologies</h2>\nNube Technologies builds business applications to better decision making through better data. Nube’s fuzzy matching product Reifier helps companies get a holistic view of enterprise data. By linking and resolving entities across various sources, Reifier helps optimize the sales and marketing funnel, promotes enhanced security and risk management and better consolidation and reporting of business data. We help our customers build better and effective models by ensuring that their underlying master data is accurate.\n<h2>Why Apache Spark</h2>\nData matching within a single source or across sources is a very core problem faced by almost every enterprise and we wanted to create a really smart way to solve this. Solving data matching problems is made even more difficult given that most data suffers from poor data quality with foremost reasons being errors and omissions during data collection, multi-field records and large data sizes.\n\nThe problem is an inherently quadratic problem, and although there are techniques to reduce the number of comparisons and boost up speed, applying them intelligently to unknown data is a challenging problem. While building Reifier, our aim is to be able to deal with various kinds of data in different domains be it customer information, product catalogs, organizations or any other variety of data.\n\nWe also wanted to build a system that was lightening fast as well as massively scalable with respect to the huge volumes of data seen by the modern day enterprise. On the development side, our wishlist included a friendly API, robust and scalable architecture, easy to use and well documented framework and inbuilt job dependency management.\n<h2>How we use Spark</h2>\nWhen we evaluated Spark, we were blown away by its speed, power and functionality. Spark’s support for machine learning helped us create a supervised learning product which can completely learn combined similarity rules across different fields of a record from labeled positive and negative samples. We can hence use the same product across different data types easily.\n\nOur algorithms sit atop the base Spark framework and using the custom partitioning by Spark, many times we compare only less than 0.5% of all possible pairs, which is a big performance boost. Our commitment to Spark was bolstered when Reifier got certified on Spark.\n<h2>Nube and Spark Going Forward</h2>\nUsing Spark has clearly been the best architecture decision we took, and we are very happy to be part of the thriving Spark community. We are now looking forward to exploiting other Spark functionality to provide real time distributed fuzzy matching.\n\nDo visit <a href=\"http://www.nubetech.co\" target=\"_blank\">www.nubetech.co</a> to learn more about Reifier and feel free to contact me directly at <a href=\"mailto:sonal@nubetech.co\" target=\"_blank\">sonal@nubetech.co</a> for questions, trials and demonstrations.</td></tr><tr><td>List( Dibyendu Bhattacharya (Big Data Architect))</td><td>List(Company Blog, Partners)</td><td>List(2014-12-09, 2014-12-09, UTC)</td><td><div class=\"post-meta\">This is a guest blog post from our friends at Pearson outlining their Apache Spark use case.</div>\n\n<hr />\n\n<h2>Introduction of Pearson</h2>\nPearson is a British multinational publishing and education company headquartered in London. It is the largest education company and the largest book publisher in the world. Recently, Pearson announced a new organization structure in order to accelerate their push into digital learning, education services and emerging markets. I am part of Pearson Higher Education group, which provides textbooks and digital technologies to teachers and students across Higher Education. Pearson's higher education brands include eCollege, Mastering/MyLabs and Financial Times Publishing.\n<h2>What we wanted to do</h2>\nWe are building a next generation adaptive learning platform which delivers immersive learning experiences designed for the way today’s students read, think, and learn. This learning platform is a scalable, reliable, cloud-based platform providing services to power the next generation of products for Higher Education. With a common data platform, we analyze student performance across product and institution boundaries and deliver efficacy insights to learners and institutions, which we were not able to deliver before. This platform will help Pearson to build new products faster and offer the world's greatest collection of educational content, while delivering most advanced data, analytics, adaptive and personalized capabilities for education.\n<h2>Why we chose Apache Spark and Spark Streaming</h2>\nNow to get the deep understanding of millions of learners, we needed a big data approach, and we found that a huge opportunity exists for ground-breaking industry-leading innovation in learner analytics. We have various use cases ranging from Near Real Time services, building Learner Graph, developing a common learner model for performing adaptive learning and recommendation, and different search based analytics etc. We found Apache Spark is one product which can bring all such capabilities into one platform. Spark supports both batch and real time mode of data processing along with graph analytics and machine learning libraries.\n\nPearson Near Real Time architecture is designed using Spark Streaming. Spark MLLib will be useful for Pearson Machine Learning use cases and Spark Graph Library will be useful for building learner graph in single common platform. Having common APIs and data processing semantics, Spark will help Pearson to build its skills and capabilities in a single platform rather than learning and managing various disparate tools.\n\nHaving a single platform and common API paradigm is one of the key reason we have moved our real time stack to Spark Streaming from our earlier solution which was designed using Apache Storm.\n<h2>What we did</h2>\nPearson's stream processing architecture is built using Apache Kafka and Spark Streaming.\n\nApache Kafka is a distributed messaging infrastructure and in Pearson's implementation, all students' activity and contextual data comes to Kafka cluster from different learning applications. Spark Streaming collects this data from Kafka in near-real-time and perform necessary transformations and aggregation on the fly to build the common learner data model and persists the data in NoSQL store (presently we are using Cassandra). For search related use cases, Spark Streaming consumes messages from Kafka and index them into Apache Blur, which is a distributed search engine on top of HDFS. For both these use cases, we needed a reliable, fault-tolerant Kafka consumer which can consume messages from Kafka topics without any data loss scenarios. Below diagram shows a high level overview of different components in this data pipeline.\n\n<a href=\"http://home.pearsonhighered.com/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/12/pearson-1.png\" alt=\"\" width=\"\" height=\"400\" /></a>\n\nNext figure highlights the functionality of the Spark Streaming application. All the student/instructor activities and domain events from different learning applications are combined together to continuously update the learning model of each student.\n\n<a href=\"http://home.pearsonhighered.com/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/12/pearson-2.png\" alt=\"\" width=\"\" height=\"200\" /></a>\n\nIn real time streaming use cases, Kafka is becoming a most adopted platform for distributed messaging system and this prompts various streaming layer like Spark Streaming to have a built-in Kafka Consumer, so that streaming layer can seamlessly fit into this architecture. When we started working on Spark Streaming and Kafka, we wanted to achieve better performance and stronger guarantees than those provided by the built-in high-level Kafka receiver of Spark Streaming. Hence, we chose to write our custom Kafka receiver. This custom Kafka Consumer for Spark Streaming uses the Low Level Kafka Consumer APIs, and is the most robust, high performant Kafka consumer available for Spark. This consumer handles Kafka node failures, leader changes, manages committed offset in ZK and can have tunable data rate throughput. It also solves the data loss scenarios on Receiver failures.\n\nPearson runs Spark Streaming in Amazon Cloud with YARN managed cluster. The building of common learner data model architecture using Spark Streaming will be in production by end of 2014. The Search based solution will be in production by Q1 2015. Other solutions like Learner Graph or advanced Machine Learning solution will be developed in 2015.\n<h2>To Learn More:</h2>\nFor more information, please refer to my recent <a href=\"https://www.youtube.com/watch?v=n7lfYhJgtJo&amp;list=PLU6n9Voqu_1FM8nmVwiWWDRtsEjlPqhgP&amp;index=25\" target=\"_blank\">talk</a> <a href=\"http://www.slideshare.net/lucidworks/near-real-time-indexing-kafka-messages-into-apache-blur-presented-by-dibyendu-bhattacharya-pearson-north-america\" target=\"_blank\">(slides)</a> at the Lucene/SolrRevolution conference.</td></tr><tr><td>List(Reynold Xin)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-11-05, 2014-11-05, UTC)</td><td>A month ago, we shared with you our entry to the 2014 Gray Sort competition, a 3rd-party benchmark measuring how fast a system can sort 100 TB of data (1 trillion records). Today, we are happy to announce that our entry has been reviewed by the benchmark committee and we have officially won the <a href=\"http://sortbenchmark.org/\">Daytona GraySort contest</a>!\n\nIn case you missed our <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">earlier blog post</a>, using Spark on 206 EC2 machines, we sorted 100 TB of data on disk in 23 minutes. In comparison, the previous world record set by Hadoop MapReduce used 2100 machines and took 72 minutes. This means that Apache Spark sorted the same data <strong>3X faster</strong> using <strong>10X fewer machines</strong>. All the sorting took place on disk (HDFS), without using Spark’s in-memory cache. This entry tied with a UCSD research team building high performance systems and we jointly set a new world record.\n<table class=\"table\">\n<thead>\n<tr>\n<th width=\"25%\"></th>\n<th width=\"25%\"><b>Hadoop MR\nRecord</b></th>\n<th width=\"25%\"><b>Spark\nRecord</b></th>\n<th width=\"25%\"><b>Spark\n1 PB</b></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Data Size</td>\n<td>102.5 TB</td>\n<td>100 TB</td>\n<td>1000 TB</td>\n</tr>\n<tr>\n<td>Elapsed Time</td>\n<td>72 mins</td>\n<td>23 mins</td>\n<td>234 mins</td>\n</tr>\n<tr>\n<td># Nodes</td>\n<td>2100</td>\n<td>206</td>\n<td>190</td>\n</tr>\n<tr>\n<td># Cores</td>\n<td>50400 physical</td>\n<td>6592 virtualized</td>\n<td>6080 virtualized</td>\n</tr>\n<tr>\n<td>Cluster disk throughput</td>\n<td>3150 GB/s\n(est.)</td>\n<td>618 GB/s</td>\n<td>570 GB/s</td>\n</tr>\n<tr>\n<td>Sort Benchmark Daytona Rules</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Network</td>\n<td>dedicated data center, 10Gbps</td>\n<td>virtualized (EC2) 10Gbps network</td>\n<td>virtualized (EC2) 10Gbps network</td>\n</tr>\n<tr>\n<td><strong>Sort rate</strong></td>\n<td><strong>1.42 TB/min</strong></td>\n<td><strong>4.27 TB/min</strong></td>\n<td><strong>4.27 TB/min</strong></td>\n</tr>\n<tr>\n<td><strong>Sort rate/node</strong></td>\n<td><strong>0.67 GB/min</strong></td>\n<td><strong>20.7 GB/min</strong></td>\n<td><strong>22.5 GB/min</strong></td>\n</tr>\n</tbody>\n</table>\nNamed after Jim Gray, the benchmark workload is resource intensive by any measure: sorting 100 TB of data following the strict rules generates 500 TB of disk I/O and 200 TB of network I/O. Organizations from around the world often build dedicated sort machines (specialized software and sometimes specialized hardware) to compete in this benchmark.\n\nWinning this benchmark as a general, fault-tolerant system marks an important milestone for the Spark project. It demonstrates that Spark is fulfilling its promise to serve as a faster and more scalable engine for data processing of all sizes, from GBs to TBs to PBs. In addition, it validates the work that we and others have been contributing to Spark over the past few years.\n\nSince the inception of Databricks, we have devoted much effort to improve the scalability, stability and performance of Spark. This benchmark builds upon some of our major recent work in Spark, including sort-based shuffle (<a href=\"https://issues.apache.org/jira/browse/SPARK-2045\">SPARK-2045</a>), the new Netty-based transport module (<a href=\"https://issues.apache.org/jira/browse/SPARK-2468\">SPARK-2468</a>), and external shuffle service (<a href=\"https://issues.apache.org/jira/browse/SPARK-3796\">SPARK-3796</a>). The former has been released in Apache Spark 1.1, and the latter two will be part of the upcoming Apache Spark 1.2 release.\n\nYou can read <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">our earlier blog post</a> to learn more about our winning entry to the competition. Also expect future blog posts on these major new Spark features.\n\nFinally, we thank Aaron Davidson, Norman Maurer, Andrew Wang, Min Zhou, the EC2 and EBS teams from Amazon Web Services, and the Spark community for their help along the way. We also thank the benchmark committee members Chris Nyberg, Mehul Shah, and Naga Govindaraju for their support.</td></tr><tr><td>List(Matt MacKinnon (Director of Product Management at Zaloni))</td><td>List(Company Blog, Partners)</td><td>List(2014-11-14, 2014-11-14, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.zaloni.com\" target=\"_blank\">Zaloni</a>, whose Bedrock platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\n<h2>Bedrock’s Managed Data Pipeline now includes Apache Spark</h2>\nIt was evident from the all the buzz at the Strata + Hadoop World conference that Apache Spark has now shifted from the early adopter phase to establishing itself as an integral and permanent part of the Hadoop ecosystem. The rapid pace of adoption is impressive!\n\nGiven the entrance of Spark into the mainstream Hadoop world, we are glad to announce that Bedrock is now officially Certified on Spark.\n<h2>How does Spark enhance Bedrock?</h2>\nBedrock™ defines a Managed Data Pipeline as consisting of Ingest, Organize, and Prepare stages. Bedrock’s strength lies in the integrated nature of the way data is handled through these stages.\n● Ingest: Bring data from various sources into Hadoop\n● Organize: Apply business, technical, and operational metadata to the incoming data\n● Prepare: Orchestrate workflows that perform data quality checks, mask sensitive fields, run change data capture actions and transformations.\n\n<a href=\"http://www.zaloni.com/\" target=\"_blank\"><img class=\"first certified\" src=\"/wp-content/uploads/2014/11/bedrock-archi.png\" alt=\"\" width=\"\" height=\"200\" /></a>\n\nThe Prepare stage of the Managed Data Pipeline is where Bedrock and Spark truly complement each other. Data preparation is all aspects of getting raw data ready for analytics and reporting. Spark is ideally suited to perform the type of processing required for data preparation. Combining high-speed, in-memory execution with a robust set of native actions and transformations makes Spark a natural fit. In Bedrock 3.1, Spark is now part of Bedrock’s workflow design palette that supports 20+ built in workflow actions that you can drag and drop onto the canvas to create your preparation workflow. If you prefer using SQL, you can use the Bedrock SparkSQL action.\n\nLet us look at a typical Bedrock Managed Data Pipeline to see where Spark fits. One very common use-case for Bedrock is moving data from a traditional relational database into HDFS and making the data available in Hive. To create this Managed Data Pipeline in Bedrock, we start with the Bedrock landing zone to reliably ingest the data into HDFS. Business and technical metadata is managed and operational metadata is captured as the data arrives and loads into Hadoop. In the Prepare phase, the built-in Bedrock capabilities for checking data quality, masking sensitive data, or merging incremental changes may be required. We finally round out the Prepare stage of the Managed Data Pipeline through the use of Spark or SparkSQL to implement custom or proprietary transformations, aggregations, and analysis.\n<h2>Looking to the Future</h2>\nThis is only the beginning of the value that Bedrock and Spark together can bring to the Managed Data Pipeline. In the future, Bedrock will be extended to support the full Spark ecosystem and existing Bedrock preparation actions, such as masking, data quality checks, and change data capture, will be available with Spark implementations.\n\nTo learn more about Bedrock and a Spark enabled Managed Data Pipeline, visit us online <a href=\"http://www.zaloni.com\" target=\"_blank\">zaloni.com</a>, or feel free to contact me directly at <a href=\"mailto:mmackinnon@zaloni.com\" target=\"_blank\">mmackinnon@zaloni.com</a></td></tr><tr><td>List(John Tripier, Paco Nathan)</td><td>List(Announcements, Company Blog)</td><td>List(2014-11-15, 2014-11-15, UTC)</td><td>More and more companies are using Apache Spark, and many Spark based pilots are currently deploying in production. In social media, at every big data conference or meetup, people describe new POC, prototypes, and production deployments using Spark.\n\nBehind this momentum, a growing need for Spark developers is developing; people who have demonstrated expertise in how to implement best practices for Spark. People who can help the enterprise building increasingly complex and sophisticated solutions on top of their Spark deployments.\n\nAt Databricks, we get contacted by many enterprises looking for Spark resources to help with their next data-driven initiative. And so beyond our effort to train people on Spark directly or through partners all around the world, we have teamed up with O’Reilly for offering the first industry standard for measuring and validating a developer’s expertise on Spark.\n<h2>Benefits of being a Spark Certified Developer</h2>\nThe Spark Developer Certification is the way for a developer to:\n<ul>\n \t<li>Demonstrate recognized validation for your expertise</li>\n \t<li>Meet the global standards to ensure compatibility between Spark applications and distributions</li>\n \t<li>Stay up to date with the latest advances and training in Spark</li>\n \t<li>Be a part of the Spark developers community</li>\n</ul>\nThe first set of exams have taken place at <a href=\"http://www.oreilly.com/data/sparkcert.html\" target=\"_blank\">Strata Barcelona</a> on November 20th 2014.\n\nShortly, developers will be able to take the exam online <a href=\"http://www.oreilly.com/data/sparkcert.html?cmp=ex-strata-na-lp-na_apache_spark_certification\" target=\"_blank\">here</a>. We also expect to run certification sessions at other conferences.\n<h2>How to prepare for the exam</h2>\nYou will take the test on your own computer, under the monitoring of a proctoring team. The test is about 90 minutes with a series of randomly generated questions covering all aspects of Spark.\n\nThe test will include questions in Scala, Python, Java, and SQL. However, deep proficiency in any of those languages is not required, since the questions focus on Spark and its model of computation.\n\nTo prepare for the Spark certification exam, we recommend that you:\n<ul>\n \t<li>Are comfortable coding the advanced exercises in Spark Camp or related training (<a href=\"http://spark-summit.org/2014/training\" target=\"_blank\">example exercises can be found here</a>).</li>\n \t<li>Have mastered the material released so far in the O'Reilly book, Learning Spark</li>\n \t<li>Have some hands-on experience developing Spark apps in production already</li>\n</ul></td></tr><tr><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>List(Company Blog, Partners)</td><td>List(2014-11-22, 2014-11-22, UTC)</td><td><div class=\"post-meta\">This is a guest blog post from our friends at Samsung SDS outlining their Apache Spark use case.</div>\n\n<hr />\n\n<h2>Business Challenge</h2>\nSamsung SDS is the business and IT solutions arm of Samsung Group. A global ICT service provider with over 17,000 employees worldwide and 6.7 billion USD in revenues, Samsung SDS tackles the challenges of some of the largest global enterprises in such industries as manufacturing, financial services, health care and retail.\n\nIn the different areas Samsung is focused on, the ability to make timely decisions that maximize the value to a business becomes critical. Prescriptive analytics methods have been used effectively to support decision making by leveraging probable future outcomes determined by predictive models and suggesting actions that provide maximal business value.\n\nOne of the main challenges in applying prescriptive analytics in these areas is the need to analyze a combination of structured and unstructured data at large scale, which requires a flexible and comprehensive computation framework.\n\nTo demonstrate the effectiveness of prescriptive analytics algorithms implemented by scalable technologies in realizing decision making use-cases, Samsung SDS Research America (SDSRA) has prototyped a framework that Samsung SDS business units can leverage and incorporate as part of the go-to-market products.\n<h2>Why Apache Spark</h2>\nDeveloping such a solution required three main areas of effort:\n<ul>\n \t<li>high volume data processing for feature extraction as a means of modeling business environment state;</li>\n \t<li>prescriptive model training on historical events;</li>\n \t<li>real-time processing of decision requests and corresponding prescribed actions;</li>\n</ul>\nThere are different technologies that can be used to support these effort threads but integrating these technologies can turn into a significant undertaking, one which is not directly bringing value to the project. SDSRA turned to Apache Spark due to its ability to provide efficient solutions for all three areas of effort through multiple components that are unified in one single distributed computation paradigm, at the same time as providing the level of fault tolerance expected.\n\nOur first direct contact with Spark was at the Strata Conference earlier in 2014 in Santa Clara, attending the Berkeley Data Analytics Stack tutorial, when the power of the framework and simplicity of the API became apparent. Coming back to the lab, the team experimented with the framework by attempting to implement data mining algorithms such as Apriori, for finding frequent item sets. After this initial experience, the decision was made to apply the framework in our prescriptive analytics proof-of-concept project, triggering two parallel efforts: one to implement a prescriptive analytics algorithm at scale with Spark and a second effort thread to develop a real-time framework based on Spark Streaming to get prescriptions as a response to a continuous stream of requests.\n\nWith Spark, the original raw data can be loaded into a resilient distributed dataset (RDD) and transformed into the set of features that define state. The states constitute the input for the prescriptive model training, also performed on the Spark framework through a series of RDD transformations. The resulting transformed data set is then used as input for an MLlib regression model for approximation of a value function, which is the main element of the prescriptive model.\n\n<img class=\"first certified\" src=\"/wp-content/uploads/2014/11/Samsung-SDS.png\" alt=\"\" width=\"\" height=\"500\" />\n\nAfter deriving a policy from the trained model, Spark Streaming is applied for processing the stream of requests, using the model for prescribing actions and maintaining the states as part of the stream.\n\nThere are three main characteristics of the Spark ecosystem that makes it a perfect match for this solution: the ability to cache Spark data sets and Spark Streaming data streams in memory, the distributed architecture (allowing horizontal linear scalability on commodity server clusters), and a single development paradigm across all components.\n<h2>What is next</h2>\nAt SDSRA, we see Spark as a key technology providing high throughput and low latency in processing large volume of data ingested at high speed. We look forward to experimenting with additional components of the ecosystem such as SparkSQL and GraphX, as we evolve our decision making engine into a full solution.\n\nTo learn more about SDSRA and this platform, feel free to contact me directly at <a href=\"mailto:l.quintela@samsung.com\" target=\"_blank\">l.quintela@samsung.com</a></td></tr><tr><td>List(Ameet Talwalkar, Anthony Joseph)</td><td>List(Announcements, Company Blog)</td><td>List(2014-12-02, 2014-12-02, UTC)</td><td>In the age of ‘Big Data,’ with datasets rapidly growing in size and complexity and cloud computing becoming more pervasive, data science techniques are fast becoming core components of large-scale data processing pipelines.\n\nApache Spark offers analysts and engineers a powerful tool for building these pipelines, and learning to build such pipelines will soon be a lot easier. Databricks is excited to be working with professors from University of California Berkeley and University of California Los Angeles to produce two new upcoming Massive Open Online Courses (MOOCs). Both courses will be freely available on the edX MOOC platform in <del>spring</del> summer 2015. edX Verified Certificates are also available for a fee.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/12/MOOC1.png\" alt=\"\" align=\"middle\" />\n\nThe first course, called <a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs100-1x-introduction-big-6181\" target=\"_blank\">Introduction to Big Data with Apache Spark</a>, will teach students about Apache Spark and performing data analysis. Students will learn how to apply data science techniques using parallel programming in Spark to explore big (and small) data. The course will include hands-on programming exercises including Log Mining, Textual Entity Recognition, Collaborative Filtering that teach students how to manipulate data sets using parallel processing with PySpark (part of Apache Spark). The course is also designed to help prepare students for taking the <a href=\"http://go.databricks.com/spark-certified-developer\" target=\"_blank\">Spark Certified Developer</a> exam. The course is being taught by Anthony Joseph, a professor at UC Berkeley and technical advisor at Databricks, and will start on <del>February 23rd</del> June 1st, 2015.\n\n<img class=\"aligncenter size-full wp-image-62\" style=\"max-width: 100%; display: block; margin: 30px auto 5px auto;\" src=\"https://databricks.com/wp-content/uploads/2014/12/MOOC2.png\" alt=\"\" align=\"middle\" />\n\nThe second course, called <a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs190-1x-scalable-machine-6066\" target=\"_blank\">Scalable Machine Learning</a>, introduces the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using PySpark. It presents an integrated view of data processing by highlighting the various components of these pipelines, including exploratory data analysis, feature extraction, supervised learning, and model evaluation. Students will use Spark to implement scalable algorithms for fundamental statistical models while tackling real-world problems from various domains. The course is being taught by Ameet Talwalkar, an assistant professor at UCLA and technical advisor at Databricks, and will start on <del>April 14th</del> June 29th, 2015.\n\nBoth courses are available for free on the edX website. You can sign up for them today:\n<ul>\n\t<li><a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs100-1x-introduction-big-6181\" target=\"_blank\">Introduction to Big Data with Apache Spark</a></li>\n\t<li><a href=\"https://www.edx.org/course/uc-berkeleyx/uc-berkeleyx-cs190-1x-scalable-machine-6066\" target=\"_blank\">Scalable Machine Learning</a></li>\n</ul></td></tr><tr><td>List(Lieven Gesquiere (Virdata Lead Core R&D))</td><td>List(Company Blog, Partners)</td><td>List(2014-12-04, 2014-12-04, UTC)</td><td><div class=\"post-meta\">This post is guest authored by our friends at <a href=\"http://www.technicolor.com/\" target=\"_blank\">Technicolor</a>, whose Virdata platform is now <a href=\"http://www.databricks.com/certification\" target=\"_blank\">“Certified on Apache Spark.”</a></div>\n\n<hr />\n\n<h2>About Virdata</h2>\nVirdata is Technicolor’s cloud-native Internet of Things platform offering real-time monitoring, configuration and management of the unprecedented number of connected devices and applications. Combining its highly-scalable data ingestion and messaging capabilities with real-time and historical analytics, Virdata brings value across multiple data-driven markets.\n\nThe Virdata platform was launched at CES Las Vegas in January, 2014.\nThe Virdata cloud-based platform architecture integrates state-of-the-art open source software components into a homogeneous, high-availability data-processing environment.\n<h2>Virdata and Apache Spark</h2>\nThe Virdata solution architecture comprises 3 areas: Messaging, Data Processing and Applications - all accessed through APIs. Its publish/subscribe based messaging infrastructure contains a high-throughput distributed message broker and distributed complex event processing and bidirectional message routing components.\n\nCompleting Virdata's “full stack” Internet of Things (IoT) platform, the solution provides extensive Apache Spark-driven in-memory and post-processing capability in order to transform, store and analyze the huge stream of messages generated by the world of IoT.\n\nSpark was integrated in the Virdata architecture in early 2013 as the data processing framework to analyze incoming IoT messages published by the millions of devices monitored by the Virdata cloud platform. Spark is used in Virdata for batch processing and real-time processing of device data in order to compute time-series, pre-calculated complex data visualizations and custom monitoring reports.\n\nVirdata’s data processing implementation incorporates a Lambda architecture (http://manning.com/marz/) well-suited to a combination of Spark and Spark Streaming.\n\nVirdata migrated its data processing approach from the initial stream-oriented framework where every message was processed independently to adopting Spark Streaming in order to optimize processing and message storage in a distributed manner. The Spark Streaming micro-batching approach specifically enabled eliminating any occurrences of ‘impedance mismatch’ with the storage of data in the Virdata databases.\n\nThe growing adoption of Spark by the open-source community plays an important role. As just one example, the recently released Spark-Cassandra Driver has allowed Virdata to replace custom code with a specialized component, delivering improved performance characteristics.\n<h2>How Virdata benefits from Spark</h2>\nVirdata benefits from Spark in 3 ways:\n<ul>\n \t<li>Spark offers Virdata a single framework for both batch and real-time processing.</li>\n \t<li>Spark offers the programming languages favored by its own developers and the wider data science community.</li>\n \t<li>Spark supports Virdata's native cloud dev-op and configuration environment.</li>\n</ul>\n<h2>Virdata and Spark Going Forward</h2>\nVirdata is especially excited about the growing richness of libraries in the Spark ecosystem and is already considering the integration of additional functionality such as SparkSQL, the recently announced Spark-ElasticSearch integration and the Spark Machine Learning library (MLlib) and looks forward to the announcement of many others .\n\nTo learn more about Technicolor Virdata, please visit our website at <a href=\"http://www.technicolor.com/\" target=\"_blank\">www.technicolor.com</a> and feel free to send us an email at <a href=\"mailto:Jeremy.DeClercq@technicolor.com\" target=\"_blank\">Jeremy.DeClercq@technicolor.com</a> for questions and demonstrations.</td></tr><tr><td>List(by Databricks Press Office)</td><td>List(Announcements, Company Blog)</td><td>List(2015-01-13, 2015-01-13, UTC)</td><td><strong>Highlights:</strong>\n<ul>\n\t<li>Databricks Expands Bay Area Presence, Moves HQ to San Francisco</li>\n\t<li>Company Names Kavitha Mariappan as Marketing Vice President</li>\n</ul>\nPress Release: <a title=\"http://finance.yahoo.com/news/databricks-expands-bay-area-presence-140000610.html\" href=\"http://finance.yahoo.com/news/databricks-expands-bay-area-presence-140000610.html\">http://finance.yahoo.com/news/databricks-expands-bay-area-presence-140000610.html</a>\n\n<strong>San Francisco, Calif. – January 13, 2015 – </strong><a href=\"http://www.databricks.com\">Databricks</a>, the company founded by the creators of the popular open-source Big Data processing engine Apache Spark with its flagship product, Databricks Cloud, today announced the relocation of their headquarters to San Francisco from Berkeley, California. The expansion is a reflection of Databricks’ growth heading into 2015. The company grew more than 200 percent in headcount over the last year and adds talent to its executive bench with the appointment of Kavitha Mariappan, former Marketing Vice President at Maginatics, an EMC Corporation Company, as Vice President of Marketing.\n\nOriginally founded in Berkeley, Databricks’ relocation is spurred by the startup’s rapid growth over the past year, which has led to numerous new hires, increasing the total headcount from 14 to 46 full-time employees. In anticipation of continued growth, the company has secured a prime new space in the SOMA district of San Francisco with over 15,000 square feet – triple the size of the previous office. The new location will also enable Databricks to host their routinely-attended <a href=\"http://www.meetup.com/spark-users/\">Spark MeetUps</a> and training sessions in-house – an initiative the company organizes for Spark users around the Bay Area.\n\n“San Francisco is a melting pot for raw talent and technology innovation, and gives us excellent access to customers and partners who share our vision of simplifying Big Data processing to generate real value,” said Ion Stoica, CEO of Databricks. “Databricks has deep roots in the Bay Area and we are excited to move to this hub of innovation and opportunity, while still having a view of Berkeley from our windows.”\n\nDatabricks is also announcing today the appointment of Kavitha Mariappan as Vice President of Marketing. With 20 years of industry experience, Mariappan will be instrumental in the development and execution of Databricks' global marketing across sales channels and communication platforms, and will work hand-in-hand with the customer engagement, product and engineering teams to communicate the value, use cases, and unique capabilities of Databricks Cloud to the enterprise and developer communities. Mariappan comes to Databricks from Maginatics, an enterprise storage solutions company recently acquired by EMC, where she built and led the team responsible for all aspects of product, technical, channel and outbound marketing and communications.\n\n“Databricks is completely revolutionizing the concept of big data analytics by significantly simplifying the process and enabling customers to extract tremendous value from their data. They are no strangers to technology innovation, with their unique pedigree and industry leadership as the creators of Apache Spark,” adds Mariappan. “I am truly excited to be joining a company that is fundamentally changing the way businesses are turning their data into economic value.”\n\nDatabricks is currently hiring for various positions within engineering, sales &amp; marketing, field engineering and data science. Visit their careers page for more details: <a title=\"https://databricks.com/careers\" href=\"https://databricks.com/careers\">databricks.com/careers</a></td></tr><tr><td>List(Kavitha Mariappan)</td><td>List(Announcements, Company Blog)</td><td>List(2015-01-16, 2015-01-16, UTC)</td><td>Complementing our on-going direct and partner-led Apache Spark training efforts, Databricks has teamed up with O’Reilly to offer the industry’s first standard for measuring and validating a developer’s expertise with Spark.\n\nDatabricks and O’Reilly are proud to announce the online availability of the Spark Certified Developer exams. You can now sign up and take the exam online<a href=\" http://go.databricks.com/spark-certified-developer\"> here</a>.\n\n<b>What is the Spark Certified Developer program?</b>\n\nApache Spark is the most active project in the Big Data ecosystem and is fast becoming the open source alternative of choice for many enterprises. Spark provides enterprises with the scale and sophistication they require to gain insights from their Big Data by providing a unified framework for building data pipelines. Databricks was founded by the team that created and continues to lead both development and training around Spark, and<a href=\"https://databricks.com/product\"> Databricks Cloud</a>, the cloud platform built around Spark to significantly simplify big data processing.\n\nBehind this momentum, there is a growing need for Spark developers: developers who have both demonstrated expertise in how to implement best practices for Spark, and can help the enterprise build increasingly complex and sophisticated solutions on top of their Spark deployments. To address this growing demand for Spark developers and to complement our on-going direct and partner-led Spark training efforts, Databricks has teamed up with O’Reilly to offer the industry’s first standard for measuring and validating a developer’s expertise with Spark.\n\nThe first set of certification exams took place at the<a href=\"http://www.oreilly.com/data/sparkcert.html\"> Strata Conference in Barcelona</a> on November 20th 2014. In addition to our online certification, we also expect to run certification sessions at other upcoming conferences.\n\n<b>Why become a Spark Certified Developer?</b>\n\nAs a Spark Certified Developer you will be able to:\n<ul>\n \t<li>Demonstrate industry recognized validation for your expertise.</li>\n \t<li>Meet global standards required to ensure compatibility between Spark applications and distributions.</li>\n \t<li>Stay up to date with the latest advances and training in Spark.</li>\n \t<li>Become an integral part of the growing Spark developer community.</li>\n</ul>\n<b>I’m interested! How to prepare for the exam?</b>\n\nYou will take the test on your own computer under the supervision of a proctoring team.\n\nThe test is about 90-minutes in duration and includes a series of randomly generated questions covering all aspects of Spark. It will include some questions in Scala, Python, Java, and SQL, however, deep proficiency in any of those languages is not mandatory. Instead the questions will primarily focus on Spark and its computational model.\n\nTo prepare for the Spark certification exam, we recommend that you:\n<ul>\n \t<li>Are comfortable coding the advanced exercises in Spark Camp or related training (<a href=\"http://spark-summit.org/2014/training\">example exercises can be found here</a>).</li>\n \t<li>Have mastered the material released so far in the O’Reilly book, <a href=\"http://shop.oreilly.com/product/0636920028512.do\">Learning Spark</a>.</li>\n \t<li>Already have some hands-on experience developing Spark applications in production environments.</li>\n</ul>\nDon’t forget to sign up and take the exam online<a href=\" http://go.databricks.com/spark-certified-developer\"> here</a>.</td></tr><tr><td>List(Kavitha Mariappan)</td><td>List(Company Blog, Events)</td><td>List(2015-01-20, 2015-01-20, UTC)</td><td>We are thrilled to announce the availability of the <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W4PZ7v36VwsQzW58WPXZ57MJJH0/5/f18dQhb0Sq5z8YHrDTW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4BZM57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33K-8nW7NMj-x9bTNXYW954KlM4P0Yt6W2d4hSK3bWrh8W2YH1kR47xfHKW2HRyfR6trFPNW47YlYy4bfcHbW47Xx4z3C811XW4-SZvb2KQ2YYW3_VZwP5ThdHgW3s1XjF51G0BJW4Zh8Y-57-WqMW3H_Pty2DzCtRW1zBkSq1sQ3b4W8V-D1g5rcXhJW7JS0c27BQjYmVJB4Mm896Q7XW94B_1g7v78c8W8NqNPC5qWyC0W7JTtyJ2Xm03sW3FBZ5D9lNHw9W6_b40v3vyNkPW6J4Ypk8lBfs0W3bnqM_1C-9rFVL--5_1Pct9JW2mPjk95hqX5PW9lKhck4H6s3gN4m21WR6Q977Vb98_P6s16_2W8Ph58-59BvQ0W7y34GD1FmQY-W7r71Hq2PhWHMW7tprCG95RqNQW2j-Sgt2L5GhqW3G6xft6TMH99W6-cC_w3wXTtZW6Sytzy9fTwQmN3FYx-Q_HpmRf6dY7D511\" target=\"_blank\">agenda</a> for Spark Summit East 2015! This inaugural New York City event on <span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332804\"><span class=\"aQJ\">March 18-19, 2015</span></span> has over thirty jam-packed sessions – offering a combination of longer deep-dive presentations and shorter intensive talks. You will have the opportunity to engage the speakers and your peers in discussion and a cross-pollination of ideas.\n\nWant to guarantee your seat now? Don’t forget to register at <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W4ggqXl1bLCgyW2ZPnv83G16gb0/5/f18dQhb0SfHx9dsQGlW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4L8v57mvC2W1BQYdv5Zh1T-W5LrNKl5rGt24W8YVWJx1bx6QJW24Zwjc6Lj7n4W6TnS3121WTXPW72rCNX1ZYXTRW6_94QZ6zCFTdW6twNJp2Wf0HMW5VbtmT1ksGJhW1R4_lk6991h2W7Wm1v220RS8TW59tT3Q2_n9qCW70fl6p7PJ6bdW2gM1cw1VHbdHW1fwyVV6yBPJMW30q8CN2kGg8MN5J92T6z1rlFW1VJryk5lX8fqW13bD4h6dkxBXW6fvMJz52HdgTW30gwjq2MfbGXW5MGRYh2Q9cfPW8gVS916cFGHNW4zybTB7pzzRxW3mH4TM9fy50qW56jN884Np4Y7W3qt5n883FkFRW4NQWD_31fBBTW9g8zmk6dXHHbW56k4J76VRYz6W7K2qZ28gbdv6W30nKq65p0cz2W4y0QxT1Fn75qVy9d1K4Ts_3BW4Pw1Vk2sbPxnW2fxmCv3C2Mz5W2BmRqF3B_qNxW2nMLdv2sG3RZW5Tb4y25hfQL4W8pwqMd8yzslQN6JrKTyM7W7nW91cJF_4HyTJfW7v9ypm70VxgnW87DwKM4HzlXl0\" target=\"_blank\">Spark Summit East</a>.\n\nWe also have a limited number of rooms at The Sheraton New York Times Square Hotel for our special room rate of $299 before tax. <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W2lv3pg1m0SXqW4fsBfK1dKPp40/5/f18dQhb0S3j46_HXFDW12hNB01vYxXLVpfHHm73jjjdW6MZKRp8kYQKyW6bwr5V2Jd4M1W3h5NFK4xy3gfW933vZR4B_cGsN1KnR8gwLv_-W1DxTjd45S_ygW1pRXdM4dTysDW5ptRNz8bX804W1FqhSS7yr9hMW2Qmqdx3ZSHV4W6dlvKs8-5BS2W6QFrX65V9GJqW14Pw-v594Qm7W1xYXmQ3xbLR-W53SHFb1BfnT3W4kKG2n7V5_1rW4Wv2Qf8B5-KLW27bJHq5k2HT6W1gNYDn3TWG9kW7WBmJP7S7Z5yW8qnJcF4-vRv-VL1njb83HYBBW3nH3Z-2Hnv0gW1Tc36l2Fn5vxV15QbM3wGgM5W95qxHK89gczRW3dZ_zP5yD0HkVM1N875mkvTtW51fN7d5frBc_VzXCkp9gPdlNW8yXZNd6p0vyrW6BcvqD8WJFKZN28lM2btTJTDW2VlSjj18-TvYVBbZ9W20L_wSW4lGlJL1lB3xrW72-43R2lK-02W1YJD248-TkxZW81nB_L2BHhTjW7QkGZm70nCd3V1vGks8X7pkdW6Hn_tg6YLnVwW7QSNVK45829wW2mQr6p1dRrZsW96m-QD7_cYKLW8C0HL_9h7w6PW7j4FS95v2pJxW1KdYFP95c1ZcW7fMghf2hPWpWVgcGJ_9ltkzW103\" target=\"_blank\">Book now</a> to take advantage of this great low rate!\n\nLooking forward to seeing you in New York City in March!\n\n<strong>Quick Links</strong>\n\nEvent registration - <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*N5H7Ql72mslMW2L-RmB1DfTcP0/5/f18dQhb0Sq5x8YHttqW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4FNs57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33KX8CW8Z0ByM7Mzn-vW5mNLNv7bqTzMW7vp5SX8bKv1lVprvP-3GYwqtN3Mdkt7ZThmmW4DqHqz6dgmw3W4yv2r03LmJLKW20Y98h1YfY94W72DxTp5RpkcdW3bqp8H9grnZZW2hBQwB1MqhJKW7ldyjx608MF6W772f_V7ldNWMW7zLHZZ65MGdKW83L8wq2kXv5mW1R-HQ87h6pcvW1vv0l17q6mr1W1sG9Y4602Fx-W1YSSpz88rdPbW88nwfg7WC8gkW1kmcBl6YyRMsW62VQ_x1H6fX9W19thHC1hs0BcW3fhDGL7YzgyfW69VM4M7VRPscW88nBch1D7RCnW8lTbVb3m23cDN2cKpZKVR6H2W4q1Pz560-7fXW1SdD8T8-hl8QN1RQf8zJp0X-W2pSCpp7z2LRRW5hfQL48pwqMdW8yzslQ6JrKTyVM7W7n91cJF_W4HyTJf7v9ypmW70Vxgn2_GCTjf4313mb04\" target=\"_blank\">http://</a><a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W23pWJx8ss1hdW8RwgMl99npjz0/5/f18dQhb0Sq5x8YHttqW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4FNs57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33KX8CW8Z0ByM7Mzn-vW5mNLNv7bqTzMW7vp5SX8bKv1lVprvP-3GYwqtN3Mdkt7ZThmmW4DqHqz6dgmw3W4yv2r03LmJLKW20Y98h1YfY94W72DxTp5RpkcdW3bqp8H9grnZZW2hBQwB1MqhJKW7ldyjx608MDxW2d4JJy6XdSmDW61RmwL7MJZ0QW7p239b2fT730W29RM3l21W3nyW7x3b_675fKF9W692b_h7TG4tqW7CGkhm1xykylW1PqznQ1P_kY3W1Y-3L67GFk2vW7l6Y_y88gLfmW6bPh4S22Yg87W82mwwh1ylJvdW61TWj01jJ0j1W83Dxlz1FtSddW8lTbVb3m23cDN2cKpZKVR6H2W4q1Pz560-7fXW1SdD8T8-hl8QN1RQf8zJp0X-W2pSCpp7z2LRRW5hfQL48pwqMdW8yzslQ6JrKTyVM7W7n91cJF_W4HyTJf7v9ypmW70Vxgn723xGpf4313mb04\" target=\"_blank\">spark-summit.org/east/<wbr />2015/register</a>\n\nHotel Booking - <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W6Y2Wq17n_LkBN4SPl45L56C80/5/f18dQhb0SmhT9dZyDgW80lLdp2qwv27W328Kjm4c9pPpMf5bb0XD6prW39DrXh50CtBwW2ykK417cFHF9W7fD2Fy7b_l6_W51frwm6bprKDW6PZb3m51TTwjW7m1FTD90GnbpW1pyc7w2zqhn2W2J56SM8p_3qgW3TsvCz69_P_QN3WkGFKdpNTKW1wZrc07fZrRSW6HnC4g1qf-l2W4X9S6P61SSZmW7mG7sD51vX4yW6HC4dq50QxN4N31qRN0NL6l8W8rBSk135V7DfVQJJn63Lqnm_W8q5FTl4K45-LW328h7y3_khN2W6cR12t4r1Pm7W1nq9rt2kJDdkW3NnV4F1nJpwvW26pdfj35ybjLW8pT83J95C60gV_QY2T3JZpwvW28cVfq2xqVJWW3M1q1-5H6VqbVdNdB_2zqh3dW5G07VT8nhPm4W3LcghS5qZdnHW8qT0W_4YjHwsVf6gKB97zps7W64JxFY4RDBl7W3HsWnh3b4_jCMVwyQbZvDm-W3NGMyj3PHjmtW6TfzdD6QP4KgW12mv4D1Bh91kW4YxKrk4PxC8jW448lxL4fPG21W3LRdbK3skj8NW3M0DBZ3CBqdlN7ly8-WMc6kxW8j1W00976FWKW26nf0w3XTTJlW7Dk8ps5lH1hkW7PSXQP4dLPdTW2QYgDg2-1mjy102\" target=\"_blank\">https://www.starwoodmeeting.<wbr />com/Book/Spark</a>\n\nConference Agenda - <a href=\"http://go.spark-summit.org/e1t/c/*W6stDzJ6_3DYhW6Y-qp35L8r5j0/*W5fq7JB9gVPcPN8Xsb_zvDv3w0/5/f18dQhb0Sq5z8YHrDTW8HLj0x5VQHw7W6bFhBV6P7FhxW4R4BZM57mvC2W1BQYgg4P0TLvW85Q81T83G7d1W9dtj1h7NQNCqW4zWTRG33K-8nW7NMj-x9bTNXYW954KlM4P0Yt6W2d4hSK3bWrh8W2YH1kR47xfHKW2HRyfR6trFPNW47YlYy4bfcHbW47Xx4z3C811XW4-SZvb2KQ2YYW3_VZwP5ThdHgW3s1XjF51G0BJW4Zh8Y-57-WqMW3H_Pty2DzCtRW1zBkVT4_thNbVth64v2MKKkXW5RQwcK3GBthPN78zTvQJ9typW6zst6C2LLg9DW5SqQzd9dSsrYW3y994M2XrNDyW18TYqG4FJR9JW54R4Yw5dxn7bW6CH7QT1TnnNTW3nF65-3PcqzMW5m_k0n7pHv4mW3P4Qnj5y7KV8W880J-m5gnb8LM72-Yn6Q977Vb98_P6s16_2W8Ph58-59BvQ0W7y34GD1FmQY-W7r71Hq2PhWHMW7tprCG95RqNQW2j-Sgt2L5GhqW3G6xft6TMH99W6-cC_w3wXTtZW6Sytzy9fTwQmN3FYx-Q_Hpp4f3SbMCL11\" target=\"_blank\">http://spark-summit.org/<wbr />east/2015/agenda</a>\n\n<strong>Important Dates: </strong>\n\n<span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332805\"><span class=\"aQJ\">February 16, 2015</span></span>: Last day for hotel discount\n\n<span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332806\"><span class=\"aQJ\">March 18, 2015</span></span>: Keynotes and Sessions\n\n<span class=\"aBn\" tabindex=\"0\" data-term=\"goog_929332807\"><span class=\"aQJ\">March 19, 2015</span></span>: Workshops\n\nWe look forward to seeing you all there! Let us know you are coming and stay connected! <strong>#SparkSummitEast</strong></td></tr><tr><td>List(Yin Huai (Databricks))</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-02-02, 2015-02-02, UTC)</td><td>[sidenote]Note: Starting Spark 1.3, SchemaRDD will be renamed to DataFrame.[/sidenote]\n\n<hr />\n\nIn this blog post, we introduce Spark SQL’s JSON support, a feature we have been working on at Databricks to make it dramatically easier to query and create JSON data in Spark. With the prevalence of web and mobile applications, JSON has become the de-facto interchange format for web service API’s as well as long-term storage. With existing tools, users often engineer complex pipelines to read and write JSON data sets within analytical systems. Spark SQL’s JSON support, released in Apache Spark 1.1 and enhanced in Apache Spark 1.2, vastly simplifies the end-to-end-experience of working with JSON data.<!--more-->\n<h2>Existing practices</h2>\nIn practice, users often face difficulty in manipulating JSON data with modern analytical systems. To write a dataset to JSON format, users first need to write logic to convert their data to JSON. To read and query JSON datasets, a common practice is to use an ETL pipeline to transform JSON records to a pre-defined structure. In this case, users have to wait for this process to finish before they can consume their data. For both writing and reading, defining and maintaining schema definitions often make the ETL task more onerous, and eliminate many of the benefits of the semi-structured JSON format. If users want to consume fresh data, they either have to laboriously define the schema when they create external tables and then use a custom JSON serialization/deserialization library, or use a combination of JSON UDFs to query the data.\n\nAs an example, consider a dataset with following JSON schema:\n<pre>{\"name\":\"Yin\", \"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\n{\"name\":\"Michael\", \"address\":{\"city\":null, \"state\":\"California\"}}</pre>\nIn a system like Hive, the JSON objects are typically stored as values of a single column. To access this data, fields in JSON objects are extracted and flattened using a UDF. In the SQL query shown below, the outer fields (name and address) are extracted and then the nested address field is further extracted.\n\n<em>In the following example it is assumed that the JSON dataset shown above is stored in a table called people and JSON objects are stored in the column called jsonObject.</em>\n<pre>SELECT\n  v1.name, v2.city, v2.state \nFROM people\n  LATERAL VIEW json_tuple(people.jsonObject, 'name', 'address') v1 \n     as name, address\n  LATERAL VIEW json_tuple(v1.address, 'city', 'state') v2\n     as city, state;</pre>\n<h2>JSON support in Spark SQL</h2>\nSpark SQL provides a natural syntax for querying JSON data along with automatic inference of JSON schemas for both reading and writing data. Spark SQL understands the nested fields in JSON data and allows users to directly access these fields without any explicit transformations. The above query in Spark SQL is written as follows:\n<pre>SELECT name, age, address.city, address.state FROM people</pre>\n<h3>Loading and saving JSON datasets in Spark SQL</h3>\nTo query a JSON dataset in Spark SQL, one only needs to point Spark SQL to the location of the data. The schema of the dataset is inferred and natively available without any user specification. In the programmatic APIs, it can be done through jsonFile and jsonRDD methods provided by SQLContext. With these two methods, you can create a SchemaRDD for a given JSON dataset and then you can register the SchemaRDD as a table. Here is an example:\n<pre>// Create a SQLContext (sc is an existing SparkContext)\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\n// Suppose that you have a text file called people with the following content:\n// {\"name\":\"Yin\", \"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\n// {\"name\":\"Michael\", \"address\":{\"city\":null, \"state\":\"California\"}}\n// Create a SchemaRDD for the JSON dataset.\nval people = sqlContext.jsonFile(\"[the path to file people]\")\n// Register the created SchemaRDD as a temporary table.\npeople.registerTempTable(\"people\")</pre>\nIt is also possible to create a JSON dataset using a purely SQL API. For instance, for those connecting to Spark SQL via a JDBC server, they can use:\n<pre>CREATE TEMPORARY TABLE people\n    USING org.apache.spark.sql.json\n    OPTIONS (path '[the path to the JSON dataset]')</pre>\nIn the above examples, because a schema is not provided, Spark SQL will automatically infer the schema by scanning the JSON dataset. When a field is JSON object or array, Spark SQL will use STRUCT type and ARRAY type to represent the type of this field. Since JSON is semi-structured and different elements might have different schemas, Spark SQL will also resolve conflicts on data types of a field. To understand what is the schema of the JSON dataset, users can visualize the schema by using the method of printSchema() provided by the returned SchemaRDD in the programmatic APIs or by using DESCRIBE [table name] in SQL. For example, the schema of people visualized through people.printSchema() will be:\n<pre>root\n |-- address: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- name: string (nullable = true)</pre>\nOptionally, a user can apply a schema to a JSON dataset when creating the table using jsonFile and jsonRDD. In this case, Spark SQL will bind the provided schema to the JSON dataset and will not infer the schema. Users are not required to know all fields appearing in the JSON dataset. The specified schema can either be a subset of the fields appearing in the dataset or can have field that does not exist.\n\nAfter creating the table representing a JSON dataset, users can easily write SQL queries on the JSON dataset just as they would on regular tables. As with all queries in Spark SQL, the result of a query is represented by another SchemaRDD. For example:\n<pre>val nameAndAddress = sqlContext.sql(\"SELECT name, address.city, address.state FROM people\")\nnameAndAddress.collect.foreach(println)</pre>\nThe result of a SQL query can be used directly and immediately by other data analytic tasks, for example a machine learning pipeline. Also, JSON datasets can be easily cached in Spark SQL’s built in in-memory columnar store and be save in other formats such as Parquet or Avro.\n<h3>Saving SchemaRDDs as JSON files</h3>\nIn Spark SQL, SchemaRDDs can be output in JSON format through the toJSON method. Because a SchemaRDD always contains a schema (including support for nested and complex types), Spark SQL can automatically convert the dataset to JSON without any need for user-defined formatting. SchemaRDDs can themselves be created from many types of data sources, including Apache Hive tables, Parquet files, JDBC, Avro file, or as the result of queries on existing SchemaRDDs. This combination means users can migrate data into JSON format with minimal effort, regardless of the origin of the data source.\n<h2>What's next?</h2>\nThere are also several features in the pipeline that with further improve Spark SQL's support for semi-structured JSON data.\n<h3>Improved SQL API support to read/write JSON datasets</h3>\nIn Apache Spark 1.3, we will introduce improved JSON support based on the new data source API for reading and writing various format using SQL. Users can create a table from a JSON dataset with an optional defined schema like what they can do with jsonFile and jsonRDD. Also, users can create a table and ask Spark SQL to store its rows in JSON objects. Data can inserted into this table through SQL. Finally, a CREATE TABLE AS SELECT statement can be used to create such a table and populate its data.\n<h3>Handling JSON datasets with a large number of fields</h3>\nJSON data is often semi-structured, not always following a fixed schema. In the future, we will expand Spark SQL’s JSON support to handle the case where each object in the dataset might have considerably different schema. For example, consider a dataset where JSON fields are used to hold key/value pairs representing HTTP headers. Each record might introduce new types of headers and using a distinct column for each one would produce a very wide schema. We plan to support auto-detecting this case and instead use a Map type. Thus, each row may contain a Map, enabling querying its key/value pairs. This way, Spark SQL will handle JSON datasets that have much less structure, pushing the boundary for the kind of queries SQL-based systems can handle.\n\n<em>To try out these new Spark features, <a href=\"https://databricks.com/try-databricks\">get a free trial of Databricks or use the Community Edition</a>.</em></td></tr><tr><td>List(Jeremy Freeman (Howard Hughes Medical Institute))</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2015-01-28, 2015-01-28, UTC)</td><td>Many real world data are acquired sequentially over time, whether messages from social media users, time series from wearable sensors, or — in a case we are particularly excited about — the firing of large populations of neurons. In these settings, rather than wait for all the data to be acquired before performing our analyses, we can use streaming algorithms to identify patterns over time, and make more targeted predictions and decisions.\n\nOne simple strategy is to build machine learning models on static data, and then use the learned model to make predictions on an incoming data stream. But what if the patterns in the data are themselves dynamic? That's where streaming algorithms come in.\n\nA key advantage of Apache Spark is that its machine learning library (MLlib) and its library for stream processing (Spark Streaming) are built on the same core architecture for distributed analytics. This facilitates adding extensions that leverage and combine components in novel ways without reinventing the wheel. We have been developing a family of streaming machine learning algorithms in Spark within MLlib. In this post we describe streaming k-means clustering, included in the recently released Apache Spark 1.2.\n<h2>Algorithm</h2>\nThe goal of k-means is to partition a set of data points into k clusters. The now classic k-means algorithm — developed by Stephen Lloyd in the 1950s for efficient digital quantization of analog signals — iterates between two steps. First, given an initial set of k cluster centers, we find which cluster each data point is closest to. Then, we compute the average of each of the new clusters and use the result to update our cluster centers. At each of these steps — re-assigning and updating — we are making the points within each cluster more and more similar to one another (more formally, we are in both steps shrinking the within-cluster-sum-of-squares). By iterating between these two steps repeatedly, we can usually converge to a good solution.\n[breadcrumb slug=\"image-1\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image04.gif\" alt=\"\" width=\"85%\" /></p>\nIn the streaming setting, our data arrive in batches, with potentially many data points per batch. The simplest extension of the standard k-means algorithm would be to begin with cluster centers — usually random locations, because we haven't yet seen any data — and for each new batch of data points, perform the same two-step operation described above. Then, we use the new centers to repeat the procedure on the next batch. Above is a movie showing the behavior of this algorithm for two-dimensional data streaming from three clusters that are slowly drifting over time. The centers track the true clusters and adapt to the changes over time.\n<h2>Forgetfulness</h2>\nIf the source of the data is constant — the same three clusters forever — the above streaming algorithm will converge to a similar solution as if k-means was run offline on the entire accumulated data set. In fact, in this case the streaming algorithm is identical to a well-known offline k-means algorithm, “mini-batch” k-means, which repeatedly trains on random subsets of the data to avoid loading the entire data set into memory.\n\nHowever, what if the sources of data are changing over time? How can we make our model reflect those changes?\n\nFor this setting, we have extended the algorithm to support <b>forgetfulness</b>, allowing the model to adapt to changes over time. The key trick is to add a new parameter that balances the relative importance of new data versus past history. One setting of this parameter will be equivalent to the scenario described above, where all data from the beginning of time are treated equally. At the other extreme, only the most recent data will be used. Settings in between will combine the present with a partial reflection of the past. Here is an animation showing two settings of this forgetfulness parameter, in streams where the centers change half-way through. Watch how the cluster centers quickly adjust to the new locations in the second case, but take a while to shift in the first.\n[breadcrumb slug=\"image-2\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image03.gif\" alt=\"\" width=\"85%\" />\n<img src=\"https://databricks.com/wp-content/uploads/2015/01/image00.gif\" alt=\"\" width=\"85%\" /></p>\nWith the appropriate setting of the parameter, we can have cluster centers that smoothly adapt to dynamic changes in the data. In this animation, watch five clusters drift over time, and the centers track them.<strong><strong> </strong></strong>\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image02.gif\" alt=\"\" width=\"85%\" /></p>\nMathematically, forgetfulness amounts to adding an extra parameter to the <b>update rule</b>: the equation describing how to update centers given a new batch of data. However, as a scalar value between 0 to 1, it is not a particularly intuitive parameter. So instead, we expose a <b>half-life</b>, which describes the time it takes before past data contributes to only one half of the current model. To demonstrate, we’ll use a one-dimensional version of the examples above. We start with data drawn from two clusters, and then switch to data from two different clusters. The half life determines how many batches it will take for the contribution from the initial set of points to reduce to half. You can see the effect of changing the half-life in the time it takes for the clusters adjust. With a half-life of 0.5 batches the change finishes in about 1 batch, but with a half-life of 5 it takes about 10 batches.\n[breadcrumb slug=\"image-3\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/image05.gif\" alt=\"\" width=\"85%\" />\n<img src=\"https://databricks.com/wp-content/uploads/2015/01/image01.gif\" alt=\"\" width=\"85%\" /></p>\nUsers may want to think about their half-life in terms of either the number of batches (which have a fixed duration in time), or the number of points. If you have 1000 data points in one batch and 10 in the other, perhaps you want those 1000 to have a proportionately larger impact. On the other hand, you might want to remain stable across fluctuations in data points, and instead treat all periods of time equally. To solve this, we've introduced the concept of a <b>time unit</b> that can be specified as either batches or points. Given a user-specified half life and time unit, the algorithm automatically calculates the appropriate forgetfulness behavior.\n\nA final feature included is a check to eliminate <strong>dying</strong> clusters. If there is a dramatic change in the data generating process, one of the estimated clusters may suddenly be far from any data, and stay stuck in its place. To prevent this scenario, clusters are checked for such behavior on each batch. A cluster detected as dying is eliminated, and the largest cluster is split in two. In this one-dimensional demo, two clusters are initially far apart, but then one changes to be much closer to the other. At first the incorrect cluster persists (top line), but soon it disappears, and the other cluster splits to correctly lock on to the new cluster centers.\n[breadcrumb slug=\"image-4\"]\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/7-cluster-dying.gif\" alt=\"\" width=\"85%\" /></p>\n\n<h2>Getting started</h2>\nTo get started using streaming k-means yourself, <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\">download Apache Spark 1.2</a> today, read more about <a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html\" target=\"_blank\">streaming k-means in the Apache Spark 1.2 documentation</a>, and try the <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\" target=\"_blank\">example code</a>. To generate your own visualizations of streaming clustering like the ones shown here, and explore the range of settings and behaviors, check out the code in the <a href=\"http://spark-packages.org/package/28\" target=\"_blank\">spark-ml-streaming</a> package.\n<h2>Looking forward</h2>\nMany algorithms and analyses can benefit from streaming implementations. Along with streaming linear regression (as of 1.1) and streaming clustering (as of 1.2), we plan to add streaming versions of of factorization and classification in future releases, incorporate them into the new Python Streaming API, and use our new forgetfulness parameterization across the algorithms as a unified way to control dynamic model updating.\n\nSpecial thanks to Xiangrui Meng, Tathagata Das, and Nicholas Sofroniew (for work on algorithm development) and Matthew Conlen (for visualizations).</td></tr><tr><td>List(Dave Wang (Databricks))</td><td>List(Announcements, Company Blog)</td><td>List(2015-02-05, 2015-02-05, UTC)</td><td>Recently <a href=\"http://www.infoworld.com/article/2871935/application-development/infoworlds-2015-technology-of-the-year-award-winners.html\" target=\"_blank\">Infoworld unveiled the 2015 Technology of the Year Award winners</a>, which range from open source software to stellar consumer technologies like the iPhone.  Being the <a title=\"Announcing Spark 1.2\" href=\"https://databricks.com/blog/2014/12/19/announcing-spark-1-2.html\" target=\"_blank\">creators behind Apache Spark</a>, Databricks is thrilled to see Spark in their ranks.  In fact, we built our flagship product, <a title=\"Databricks Cloud Overview\" href=\"https://databricks.com/product\">Databricks</a>, on top of Spark with the ambition to revolutionize big data processing in ways similar to how iPhone revolutionized the mobile experience.\n\nThe iPhone was revolutionary in a number of ways: first, it integrated a disparate set of consumer electronic capabilities such as mobile phone, camera, GPS, and even laptop; second, it created a seamless experience navigating these capabilities with iOS; lastly, it was easily extensible through 3rd party applications, which gave rise to a whole ecosystem of products built around the iPhone.  In short, the iPhone is <i>the</i> simple and powerful platform to meet all the mobile needs of users.\n\nDatabricks is precisely the analogous product in the big data world.\n\nFirst, Databricks unifies disparate functionalities: It runs 100% open source Spark, which is a lightning fast big data general processing engine that includes a broad set of standard libraries.  This means a user will have access to lightning fast performance up to 100x faster than Hadoop MapReduce both in memory and on disk, and also have capabilities such as <a title=\"An introduction to JSON support in Spark SQL\" href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\" target=\"_blank\">SQL</a>, <a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">machine learning</a>, <a title=\"Mining Ecommerce Graph Data with Spark at Alibaba Taobao\" href=\"https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html\" target=\"_blank\">graph processing</a> out of the box.  Users can get all the big data processing tools through Spark instead of integrate disparate tools.\n\nSecond, Databricks also includes features that makes deploying and working with Spark much simpler.  The cluster manager allows users to create, modify, and teardown Spark clusters in their Amazon Virtual Private Cloud (VPC) in a few clicks.  The interactive workspace allows users to easily visualize data and share results with colleagues.  The job scheduler allows production data processing pipelines to be built in a matter of minutes.  Through Databricks, users can have a seamless experience working with big data - whether it’s interactive exploration or batch processing.\n\nLastly, Databricks provides common connectors to allow 3rd party applications, such as <a title=\"Application Spotlight: Tableau Software\" href=\"https://databricks.com/blog/2014/10/15/application-spotlight-tableau-software.html\" target=\"_blank\">Tableau</a> and many of the common BI tools used by corporations today, to seamlessly integrate.  This allows non-technical users to gain access to big data, and further increases the usability of Databricks.\n\nWe built Databricks to make big data simple: easy to use, work at lightning fast speed, and provide instant results for all big data processing needs.  As Spark gains momentum, many are finding that Databricks is the best way to run Spark due to its rapid deployment, seamless user experience, and performance.  If you would like to try out Databricks, simply <a href=\"https://databricks.com/registration\" target=\"_blank\">sign up</a> for a trial!</td></tr><tr><td>List(Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2014-12-19, 2014-12-19, UTC)</td><td>We at Databricks are thrilled to announce the release of Apache Spark 1.2! Apache Spark 1.2 introduces many new features along with scalability, usability and performance improvements. This post will introduce some key features of Apache Spark 1.2 and provide context on the priorities of Spark for this and the next release. In the next two weeks, we’ll be publishing blog posts with more details on feature additions in each of the major components. Apache Spark 1.2 has been posted today on the <a href=\"http://spark.apache.org/releases/spark-release-1-2-0.html\">Apache Spark website</a>.\n\nLearn more about specific new features in related in-depth posts:\n<ul>\n \t<li><a title=\"Spark SQL Data Sources API: Unified Data Access for the Spark Platform\" href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">Spark SQL data sources API</a></li>\n \t<li><a title=\"An introduction to JSON support in Spark SQL\" href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\" target=\"_blank\">JSON support in Spark SQL</a><a title=\"Introducing streaming k-means in Spark 1.2\" href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\"> </a></li>\n \t<li><a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">ML pipeline API</a></li>\n \t<li><a title=\"Introducing streaming k-means in Spark 1.2\" href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\">Streaming k-means</a></li>\n \t<li><a title=\"Random Forests and Boosting in MLlib\" href=\"https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html\" target=\"_blank\">Random forests and GBTs</a></li>\n \t<li><a title=\"Improved Fault-tolerance and Zero Data Loss in Spark Streaming\" href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\" target=\"_blank\">Improved fault tolerance in Spark streaming</a></li>\n</ul>\n<h2>Optimizations in Spark's core engine</h2>\nApache Spark 1.2 includes several cross-cutting optimizations focused on performance for large scale workloads. Two new features Databricks developed for our <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">world record petabyte sort</a> with Spark are turned on by default in Apache Spark 1.2. The first is a re-architected network transfer subsystem that exploits Netty 4’s zero-copy IO and off heap buffer management. The second is Spark's sort based shuffle implementation, which we've now made the default after significant testing in Apache Spark 1.1. Together, we've seen these features give as much as 5X performance improvement for workloads with very large shuffles.\n<h2>Spark SQL data sources and Hive 13</h2>\nUntil now, Spark SQL has supported accessing any data described in an Apache Hive metastore, along with a small number of native bindings for popular formats such as Parquet and JSON. This release introduces a standard API for native integration with other file formats and storage systems. The API supports low level optimizations such as predicate pushdown and direct access to Spark SQL’s table catalog. Any data sources written for this API are automatically queryable in Java, Scala and Python. At Databricks, we’ve released an <a href=\"https://github.com/databricks/spark-avro\">Apache Avro connector</a> based on this API (itself requiring less than 100 lines of code), and we expect several other connectors to appear in the coming months from the community. Using the input API is as simple as listing the desired format:\n\n<em>Creating a Parquet Table</em><em>Creating a JSON Table</em>\n<table class=\"table\">\n<thead></thead>\n<tbody>\n<tr>\n<td width=\"50%\">\n\n<pre>CREATE TEMPORARY TABLE\n  users_parquet\nUSING\n  org.apache.spark.sql.parquet\nOPTIONS\n  (path 'hdfs://parquet/users');</pre>\n\n</td>\n<td width=\"50%\">\n\n<pre>CREATE TEMPORARY TABLE\n  users_json\nUSING\n  org.apache.spark.sql.json\nOPTIONS\n  (path 'hdfs://json/users');</pre>\n\n</td>\n</tr>\n</tbody>\n</table>\nNote that neither the schema nor the partitioning layout is specified here. Spark SQL is able to learn that automatically in both cases. For users reading data from Hive tables, we've bumped our support to Hive 0.13 and included support for its fixed-precision decimal type.\n<h2>Spark Streaming H/A and Python API</h2>\nIn this release, Spark Streaming adds a full H/A mode that uses a persistent Write Ahead Log (WAL) to provide recoverability for input sources if nodes crash. This feature removes any single-point-of-failure from Spark Streaming, a common request from production Spark Streaming users. The WAL mechanism is supported out-of-the-box for Apache Kafka, and the more general API for third-party connectors has been extended with durability support. In addition, this release adds a Python API for Spark Streaming, letting you create and transform streams entirely in Python.\n<h2>Machine learning pipelines</h2>\nWe've extended Spark's machine learning library with a new, higher-level API for constructing pipelines, in the <code>spark.ml</code> package. In practice, most machine learning workflows involve multiple preprocessing and featurization steps, as well as training and evaluating multiple models. The ML pipelines API provides first-class support for these types of pipelines, including the ability to search for parameters and automatically score models. It is modeled after high-level machine learning libraries like SciKit-Learn, and brings the same ease of use to learning on big data.\n<table class=\"table\">\n<thead>\n<tr>\n<th><em>Defining a three-stage ML pipeline</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n\n<pre>val tokenizer = new Tokenizer()\n  .setInputCol(\"text\")\n  .setOutputCol(\"words\")\nval hashingTF = new HashingTF()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"features\")\nval lr = new LogisticRegression().setMaxIter(10)\nval pipeline = new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, lr))</pre>\n\n</td>\n</tr>\n</tbody>\n</table>\nThe new ML API is experimental in Apache Spark 1.2 as we get feedback from users, but will be stabilized in 1.3.\n<h2>Stable GraphX API</h2>\nThe GraphX project graduates from alpha in this release, providing a stable API. This means applications written against GraphX can be safely migrated to future Spark 1.X versions without code changes. Coinciding with API stabilization, a handful of issues have been fixed which affect very large scale and highly iterative graphs seen in production workloads.\n\n<hr />\n\nThis post only scratches the surface of interesting features in Apache Spark 1.2. Overall, this release contains more than 1000 patches from 172 contributors<b> </b>making it our largest yet despite a year of tremendous growth. Head over to the <a href=\"http://spark.apache.org/releases/spark-release-1-2-0.html\">official release notes</a> to learn more about this release, and watch the Databricks blog for more detailed posts about the major features in the next few days!</td></tr><tr><td>List(Xiangrui Meng, Patrick Wendell)</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(2014-12-22, 2014-12-22, UTC)</td><td>Today, we are happy to announce <em>Apache Spark Packages</em> (<a title=\"http://spark-packages.org\" href=\"http://spark-packages.org\">http://spark-packages.org</a>), a community package index to track the growing number of open source packages and libraries that work with Apache Spark. <em>Spark Packages</em> makes it easy for users to find, discuss, rate, and install packages for any version of Spark, and makes it easy for developers to contribute packages.\n\n<!--more-->\n\n<em>Spark Packages</em> will feature integrations with various data sources, management tools, higher level domain-specific libraries, machine learning algorithms, code samples, and other Spark content. Thanks to the package authors, the initial listing of packages includes <a href=\"http://spark-packages.org/package/6\">scientific computing libraries</a>, a <a href=\"http://spark-packages.org/package/10\">job execution server</a>, a connector for <a href=\"http://spark-packages.org/package/3\">importing Avro data</a>, tools for launching Spark on <a href=\"http://spark-packages.org/package/9\">Google Compute Engine</a>, and many others. We expect this list to grow substantially in 2015, and to help fuel this growth we’re continuing to invest in extension points to Spark such as the Spark SQL data sources API, the Spark streaming Receiver API, and the Spark ML pipeline API. Package authors who submit a listing retain full rights to your code, including your choice of open-source license.\n\nPlease give <em>Spark Packages</em> a try and let us know if you have any questions when working with the site! We expect to extend the site in the coming months while also building mechanisms in Spark to make using packages even easier. We hope <em>Spark Packages</em> lets you find even more great ways to work with Spark.</td></tr><tr><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>List(Engineering Blog, Machine Learning)</td><td>List(2015-01-07, 2015-01-07, UTC)</td><td>MLlib’s goal is to make practical machine learning (ML) scalable and easy. Besides new algorithms and performance improvements that we have seen in each release, a great deal of time and effort has been spent on making MLlib <i>easy</i>. Similar to Spark Core, MLlib provides APIs in three languages: Python, Java, and Scala, along with user guide and example code, to ease the learning curve for users coming from different backgrounds. In Apache Spark 1.2, Databricks, jointly with AMPLab, UC Berkeley, continues this effort by introducing a pipeline API to MLlib for easy creation and tuning of practical ML pipelines.\n\nA practical ML pipeline often involves a sequence of data pre-processing, feature extraction, model fitting, and validation stages. For example, classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. Though there are many libraries we can use for each stage, connecting the dots is not as easy as it may look, especially with large-scale datasets. Most ML libraries are not designed for distributed computation or they do not provide native support for pipeline creation and tuning. Unfortunately, this problem is often ignored in academia, and it has received largely ad-hoc treatment in industry, where development tends to occur in manual one-off pipeline implementations.\n\nIn this post, we briefly describe the work done to address ML pipelines in MLlib, a joint effort between Databricks and AMPLab, UC Berkeley, and inspired by the <a href=\"http://scikit-learn.org/\" target=\"_blank\">scikit-learn</a> project and some earlier work on <a href=\"https://amplab.cs.berkeley.edu/publication/mli-an-api-for-distributed-machine-learning/\" target=\"_blank\">MLI</a>.\n<h2>Dataset abstraction</h2>\nIn the new pipeline design, a dataset is represented by Spark SQL’s SchemaRDD and an ML pipeline by a sequence of dataset transformations. <strong>(Update</strong>: SchemaRDD was renamed to DataFrame in Spark 1.3).  Each transformation takes an input dataset and outputs the transformed dataset, which becomes the input to the next stage. We leverage on Spark SQL for several reasons: data import/export, flexible column types and operations, and execution plan optimization.\n\nData import/export is the start/end point of an ML pipeline.  MLlib currently provides import/export utilities for several application-specific types: LabeledPoint for classification and regression, Rating for collaborative filtering, and so on.  However, realistic datasets may contain many types, such as user/item IDs, timestamps, or raw records.  The current utilities cannot easily handle datasets with combinations of these types, and they use inefficient text storage formats adopted from other ML libraries.\n\nFeature transformations usually form the majority of a practical ML pipeline. A feature transformation can be viewed as appending new columns created from existing columns. For example, text tokenization breaks a document up into a bag of words, and tf-idf converts a bag of words into a feature vector, while during the transformations the labels need to be preserved for model fitting. More complex feature transformations are quite common in practice. Hence, the dataset needs to support columns of different types, including dense and sparse vectors, and operations that create new columns from existing ones.\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/pipeline-0.png\" alt=\"\" width=\"90%\" /></p>\nIn the example above, id, text, and words are carried over during transformations. They are unnecessary for model fitting, but useful in prediction and model inspection. It doesn’t provide much information if the prediction dataset only contains the predicted labels. If we want to inspect the prediction results, e.g., checking false positives, it is quite useful to look at the predicted labels along with the raw input text and tokenized words. The columns needed at each stage are quite different. It would be ideal that the underlying execution engine can optimize for us and only load the required columns.\n\nFortunately, Spark SQL already provides most of the desired functions and we don’t need to reinvent the wheel. Spark SQL supports import/export SchemaRDDs from/to Parquet, an efficient columnar storage format, and easy conversions between RDDs and SchemaRDDs. It also supports pluggable external data sources like Hive and <a href=\"http://spark-packages.org/package/3\" target=\"_blank\">Avro</a>. Creating (or declaring to be more precise) new columns from existing columns is easy with user-defined functions. The materialization of SchemaRDD is lazy. Spark SQL knows how to optimize the execution plan based on the columns requested, which fits our needs well. SchemaRDD supports standard data types. To make it a better fit for ML, we worked together with the Spark SQL team and added Vector type as a user-defined type that supports both dense and sparse feature vectors.\n\nWe show a simple Scala code example for ML dataset import/export and simple operations. More complete dataset examples in <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/scala/org/apache/spark/examples/mllib/DatasetExample.scala\" target=\"_blank\">Scala</a> and <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/python/mllib/dataset_example.py\" target=\"_blank\">Python</a> can be found under the `examples/` folder of the Spark repository. We refer users to <a href=\"http://spark.apache.org/docs/1.2.0/sql-programming-guide.html\" target=\"_blank\">Spark SQL’s user guide</a> to learn more about SchemaRDD and the operations it supports.\n\n[scala]\nval sqlContext = SQLContext(sc)\nimport sqlContext._ // implicit conversions\n\n// Load a LIBSVM file into an RDD[LabeledPoint].\nval labeledPointRDD: RDD[LabeledPoint] =\n  MLUtils.loadLibSVMFile(&quot;/path/to/libsvm&quot;)\n\n// Save it as a Parquet file with implicit conversion\n// from RDD[LabeledPoint] to SchemaRDD.\nlabeledPointRDD.saveAsParquetFile(&quot;/path/to/parquet&quot;)\n\n// Load the parquet file back into a SchemaRDD.\nval dataset = parquetFile(&quot;/path/to/parquet&quot;)\n\n// Collect the feature vectors and print them.\ndataset.select('features).collect().foreach(println)\n[/scala]\n\n<h2>Pipeline</h2>\nThe new pipeline API lives under a new package named “spark.ml”. A pipeline consists of a sequence of stages. There are two basic types of pipeline stages: Transformer and Estimator. A Transformer takes a dataset as input and produces an augmented dataset as output. E.g., a tokenizer is a Transformer that transforms a dataset with text into an dataset with tokenized words. An Estimator must be first fit on the input dataset to produce a model, which is a Transformer that transforms the input dataset. E.g., logistic regression is an Estimator that trains on a dataset with labels and features and produces a logistic regression model.\n\nCreating a pipeline is easy: simply declare its stages, configure their parameters, and chain them in a pipeline object. For example the following code creates a simple text classification pipeline consisting of a tokenizer, a hashing term frequency feature extractor, and logistic regression.\n\n[scala]\nval tokenizer = new Tokenizer()\n  .setInputCol(&quot;text&quot;)\n  .setOutputCol(&quot;words&quot;)\nval hashingTF = new HashingTF()\n  .setNumFeatures(1000)\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(&quot;features&quot;)\nval lr = new LogisticRegression()\n  .setMaxIter(10)\n  .setRegParam(0.01)\nval pipeline = new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, lr))\n[/scala]\n\nThe pipeline itself is an Estimator, and hence we can call fit on the entire pipeline easily.\n\n[scala]\nval model = pipeline.fit(trainingDataset)\n[/scala]\n\nThe fitted model consists of the tokenizer, the hashing TF feature extractor, and the fitted logistic regression model. The following diagram draws the workflow, where the dash lines only happen during pipeline fitting.\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/pipeline-1.png\" alt=\"\" width=\"90%\" /></p>\nThe fitted pipeline model is a transformer that can be used for prediction, model validation, and model inspection.\n\n[scala]\nmodel.transform(testDataset)\n  .select('text, 'label, 'prediction)\n  .collect()\n  .foreach(println)\n[/scala]\n\nOne unfortunate characteristic of ML algorithms is that they have many hyperparameters that must be tuned. These hyperparameters - e.g. degree of regularization - are distinct from the model parameters being optimized by MLlib. It is hard to guess the best combination of hyperparameters without expert knowledge on both the data and the algorithm. Even with expert knowledge, it may become unreliable as the size of the pipeline and the number of hyperparameters grows. Hyperparameter tuning (choosing parameters based on performance on held-out data) is usually necessary to obtain meaningful results in practice. For example, we have two hyperparameters to tune in the following pipeline and we put three candidate values for each. Therefore, there are nine combinations in total (four shown in the diagram below) and we want to find the one that leads to the model with the best evaluation result.\n<p align=\"center\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/pipeline-2.png\" alt=\"\" width=\"90%\" /></p>\nWe support cross-validation for hyperparameter tuning. We view cross-validation as a meta-algorithm, which tries to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one. Note that there is no specific requirement on the underlying estimator, which could be a pipeline, as long as it could be paired with an Evaluator that outputs a scalar metric from predictions, e.g., precision. Tuning a pipeline is easy:\n\n[scala]\n// Build a parameter grid.\nval paramGrid = new ParamGridBuilder()\n  .addGrid(hashingTF.numFeatures, Array(10, 20, 40))\n  .addGrid(lr.regParam, Array(0.01, 0.1, 1.0))\n  .build()\n\n// Set up cross-validation.\nval cv = new CrossValidator()\n  .setNumFolds(3)\n  .setEstimator(pipeline)\n  .setEstimatorParamMaps(paramGrid)\n  .setEvaluator(new BinaryClassificationEvaluator)\n\n// Fit a model with cross-validation.\nval cvModel = cv.fit(trainingDataset)\n[/scala]\n\nIt is important to note that users can embed their own transformers or estimators into an ML pipeline, as long as they implement the pipeline interfaces. The API makes it easy to use and share code maintained outside MLlib. More complete code examples in <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCrossValidatorExample.java\" target=\"_blank\">Java</a> and <a href=\"https://github.com/apache/spark/blob/branch-1.2/examples/src/main/scala/org/apache/spark/examples/ml/CrossValidatorExample.scala\" target=\"_blank\">Scala</a> can be found under the ‘examples/’ folder of the Spark repository. We refer users to the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">spark.ml user guide</a> for more information about the pipeline API.\n<h2>Concluding remarks</h2>\nThe blog post describes the ML pipeline API introduced in Spark 1.2 and the rationale behind it. The work is covered by several JIRAs: <a href=\"https://issues.apache.org/jira/browse/SPARK-3530\" target=\"_blank\">SPARK-3530</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-3569\" target=\"_blank\">SPARK-3569</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-3572\" target=\"_blank\">SPARK-3572</a>, <a href=\"https://issues.apache.org/jira/browse/SPARK-4192\" target=\"_blank\">SPARK-4192</a>, and <a href=\"https://issues.apache.org/jira/browse/SPARK-4209\" target=\"_blank\">SPARK-4209</a>. We refer users to the design docs posted on each JIRA page for more information about the design choices. And we would like to thank everyone who participated in the discussion and provided valuable feedback.\n\nThat being said, the pipeline API is experimental in Spark 1.2 and the work is still far from done. For example, more feature transformers can help users quickly assemble pipelines. We would like to mention some ongoing work relevant to the pipeline API:\n<ul>\n \t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-5097\" target=\"_blank\">SPARK-5097</a>: Adding data frame APIs to SchemaRDD</li>\n \t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-4586\" target=\"_blank\">SPARK-4586</a>: Python API for ML pipeline</li>\n \t<li><a href=\"https://issues.apache.org/jira/browse/SPARK-3702\" target=\"_blank\">SPARK-3702</a>: Class hierarchy for learning algorithms and models</li>\n</ul>\nThe pipeline API is part of Spark 1.2, which is available for download at <a href=\"http://spark.apache.org/\" target=\"_blank\">http://spark.apache.org/</a>. We look forward to hearing back from you about it, and we welcome your contributions and feedback.</td></tr><tr><td>List(Michael Armbrust)</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-01-09, 2015-01-09, UTC)</td><td>Since the inception of Spark SQL in Apache Spark 1.0, one of its most popular uses has been as a conduit for pulling data into the Spark platform.  Early users loved Spark SQL’s support for reading data from existing Apache Hive tables as well as from the popular Parquet columnar format. We’ve since added support for other formats, such as <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets\">JSON</a>.  In Apache Spark 1.2, we've taken the next step to allow Spark to integrate natively with a far larger number of input sources.  These new integrations are made possible through the inclusion of the new Spark SQL Data Sources API.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/01/DataSourcesApiDiagram.png\"><img class=\"wp-image-2372 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/01/DataSourcesApiDiagram-1024x526.png\" alt=\"DataSourcesApiDiagram\" width=\"516\" height=\"265\" /></a>\n\nThe Data Sources API provides a pluggable mechanism for accessing structured data though Spark SQL. Data sources can be more than just simple pipes that convert data and pull it into Spark. The tight optimizer integration provided by this API means that filtering and column pruning can be pushed all the way down to the data source in many cases.  Such integrated optimizations can vastly reduce the amount of data that needs to be processed and thus can significantly speed up Spark jobs.\n\nUsing a data sources is as easy as referencing it from SQL (or your favorite Spark language):\n\n<pre>CREATE TEMPORARY TABLE episodes\n    USING com.databricks.spark.avro\n    OPTIONS (path \"episodes.avro\")</pre>\n\nAnother strength of the Data Sources API is that it gives users the ability to manipulate data in all of the languages that Spark supports, regardless of how the data is sourced. Data sources that are implemented in Scala, for example, can be used by pySpark users without any extra effort required of the library developer. Furthermore, Spark SQL makes it easy to join data from different data sources using a single interface. Taken together, these capabilities further unify the big data analytics solution provided by Apache Spark 1.2.\n\nEven though this API is still young, there are already several libraries built on top of it, including <a href=\"http://spark-packages.org/package/3\">Apache Avro</a>, <a href=\"http://spark-packages.org/package/12\">Comma Separated Values (csv)</a>, and even <a href=\"https://github.com/mraad/spark-dbf\">dBASE Table File Format (dbf</a>).  Now that Apache Spark 1.2 has been officially released, we expect this list to grow quickly. We know of efforts underway to support HBase, JDBC, and more. Check out <a href=\"http://spark-packages.org/\">Spark Packages</a> to find an up-to-date list of libraries that are available.<strong><strong>\n</strong></strong>\n\nFor developers that are interested in writing a library for their favorite format, we suggest that you study <a href=\"https://github.com/databricks/spark-avro\">the reference library for reading Apache Avro</a>, check out the <a href=\"https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/sources\">example sources</a>, or <a href=\"http://www.youtube.com/watch?v=GQSNJAzxOr8\">watch this meetup video</a>.\n\nAdditionally, stay tuned for extensions to this API.  In Apache Spark 1.3 we are hoping to add support for partitioning, persistent tables, and optional user specified schema.</td></tr><tr><td>List(Joseph K. Bradley (Databricks), Manish Amde (Origami Logic))</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-01-21, 2015-01-21, UTC)</td><td><div class=\"post-meta\">This is a post written together with Manish Amde from <a href=\"http://www.origamilogic.com/\">Origami Logic</a>.</div>\n\n<hr />\n\nApache Spark 1.2 introduces <a href=\"http://en.wikipedia.org/wiki/Random_forest\">Random Forests</a> and <a href=\"http://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting\">Gradient-Boosted Trees (GBTs)</a> into MLlib. Suitable for both classification and regression, they are among the most successful and widely deployed machine learning methods. Random Forests and GBTs are <i>ensemble learning algorithms</i>, which combine multiple decision trees to produce even more powerful models. In this post, we describe these models and the distributed implementation in MLlib. We also present simple examples and provide pointers on how to get started.\n<h2>Ensemble Methods</h2>\nSimply put, <a href=\"http://en.wikipedia.org/wiki/Ensemble_learning\">ensemble learning algorithms</a> build upon other machine learning methods by combining models. The combination can be more powerful and accurate than any of the individual models.\n\nIn MLlib 1.2, we use <a href=\"http://en.wikipedia.org/wiki/Decision_tree\">Decision Trees</a> as the base models. We provide two ensemble methods: <a href=\"http://en.wikipedia.org/wiki/Random_forest\">Random Forests</a> and <a href=\"http://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting\">Gradient-Boosted Trees (GBTs)</a>. The main difference between these two algorithms is the order in which each component tree is trained.\n\nRandom Forests train each tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data.\n\nGBTs train one tree at a time, where each new tree helps to correct errors made by previously trained trees. With each tree added, the model becomes even more expressive.\n\nIn the end, both methods produce a weighted collection of Decision Trees. The ensemble model makes predictions by combining results from the individual trees. The figure below shows a simple example of an ensemble with three trees.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensemble-example.png\"><img class=\"alignnone size-full wp-image-2273\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensemble-example.png\" alt=\"Ensemble example\" width=\"372\" height=\"265\" /></a></p>\nIn the example regression ensemble above, each tree predicts a real value. These three predictions are then combined to produce the ensemble's final prediction. Here, we combine predictions using the mean (but the algorithms use different techniques depending on the prediction task).\n<h2>Distributed Learning of Ensembles</h2>\nIn MLlib, both Random Forests and GBTs partition data by instances (rows). The implementation builds upon the original Decision Tree code, which distributes learning of single trees (described in <a href=\"https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html\">an earlier blog post</a>). Many of our optimizations are based upon <a href=\"http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36296.pdf\">Google's PLANET project</a>, one of the major published works on learning ensembles of trees in the distributed setting.\n\n<i>Random Forests</i>: Since each tree in a Random Forest is trained independently, multiple trees can be trained in parallel (in addition to the parallelization for single trees). MLlib does exactly that: A variable number of sub-trees are trained in parallel, where the number is optimized on each iteration based on memory constraints.\n\n<i>GBTs</i>: Since GBTs must train one tree at a time, training is only parallelized at the single tree level.\n\nWe would like to highlight two key optimizations used in MLlib:\n<ul>\n \t<li>Memory: Random Forests use a different subsample of the data to train each tree. Instead of replicating data explicitly, we save memory by using a TreePoint structure which stores the number of replicas of each instance in each subsample.</li>\n \t<li>Communication: Whereas Decision Trees are usually trained by selecting from all features at each decision node in the tree, Random Forests often limit the selection to a random subset of features at each node. MLlib’s implementation takes advantage of this subsampling to reduce communication: e.g., if only 1/3 of the features are used at each node, then we can reduce communication by a factor of 1/3.</li>\n</ul>\nFor more details, see the <a href=\"http://spark.apache.org/docs/latest/mllib-ensembles.html\">Ensembles Section in the MLlib Programming Guide</a>.\n<h2>Using MLlib Ensembles</h2>\nWe demonstrate how to learn ensemble models using MLlib. The following Scala examples show how to read in a dataset, split the data into training and test sets, learn a model, and print the model and its test accuracy. Refer to the <a href=\"http://spark.apache.org/docs/latest/mllib-ensembles.html\">MLlib Programming Guide</a> for examples in Java and Python. Note that GBTs do not yet have a Python API, but we expect it to be in the Spark 1.3 release (via <a href=\"https://www.github.com/apache/spark/pull/3951\">Github PR 3951</a>).\n<h4>Random Forest Example</h4>\n\n[scala]\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file.\nval data =\n  MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)\n// Split data into training/test sets\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n\n// Train a RandomForest model.\nval treeStrategy = Strategy.defaultStrategy(&quot;Classification&quot;)\nval numTrees = 3 // Use more in practice.\nval featureSubsetStrategy = &quot;auto&quot; // Let the algorithm choose.\nval model = RandomForest.trainClassifier(trainingData,\n  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)\n\n// Evaluate model on test instances and compute test error\nval testErr = testData.map { point =&gt;\n  val prediction = model.predict(point.features)\n  if (point.label == prediction) 1.0 else 0.0\n}.mean()\nprintln(&quot;Test Error = &quot; + testErr)\nprintln(&quot;Learned Random Forest:n&quot; + model.toDebugString)\n[/scala]\n\n<h4>Gradient-Boosted Trees Example</h4>\n\n[scala]\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file.\nval data =\n  MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)\n// Split data into training/test sets\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n\n// Train a GradientBoostedTrees model.\nval boostingStrategy =\n  BoostingStrategy.defaultParams(&quot;Classification&quot;)\nboostingStrategy.numIterations = 3 // Note: Use more in practice\nval model =\n  GradientBoostedTrees.train(trainingData, boostingStrategy)\n\n// Evaluate model on test instances and compute test error\nval testErr = testData.map { point =&gt;\n  val prediction = model.predict(point.features)\n  if (point.label == prediction) 1.0 else 0.0\n}.mean()\nprintln(&quot;Test Error = &quot; + testErr)\nprintln(&quot;Learned GBT model:n&quot; + model.toDebugString)\n[/scala]\n\n<h2>Scalability</h2>\nWe demonstrate the scalability of MLlib ensembles with empirical results on a binary classification problem. Each figure below compares Gradient-Boosted Trees (\"GBT\") with Random Forests (\"RF\"), where the trees are built out to different maximum depths.\n\nThese tests were on a regression task of predicting song release dates from audio features (the <a href=\"https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD\">YearPredictionMSD dataset</a> from the UCI ML repository). We used EC2 r3.2xlarge machines. Algorithm parameters were left as defaults except where noted.\n<h4>Scaling model size: Training time and test error</h4>\nThe two figures below show the effect of increasing the number of trees in the ensemble. For both, increasing trees require more time to learn (first figure) but also provide better results in terms of test Mean Squared Error (MSE) (second figure).\n\nComparing the two methods, Random Forests are faster to train, but they often require deeper trees than GBTs to achieve the same error. GBTs can further reduce the error with each iteration, but they can begin to overfit (increase test error) after too many iterations. Random Forests do not overfit as easily, but their test error plateaus.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-time.png\"><img class=\"alignnone size-full wp-image-2280\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-time.png\" alt=\"Ensembles - trees x time\" width=\"440\" height=\"249.5\" /></a></p>\nBelow, for a basis for understanding the MSE, note that the left-most points show the error when using a single decision tree (of depths 2, 5, or 10, respectively).\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-mse.png\"><img class=\"alignnone size-full wp-image-2279\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-trees-x-mse.png\" alt=\"Ensembles - trees x mse\" width=\"440\" height=\"248.5\" /></a></p>\n<i>Details: 463,715 training instances. 16 workers.</i>\n<h4>Scaling training dataset size: Training time and test error</h4>\nThe next two figures show the effect of using larger training datasets. With more data, both methods take longer to train but achieve better test results.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-time.png\"><img class=\"alignnone size-full wp-image-2278\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-time.png\" alt=\"Ensembles - ntrain x time\" width=\"437.5\" height=\"249.5\" /></a></p>\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-mse.png\"><img class=\"alignnone size-full wp-image-2277\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-ntrain-x-mse.png\" alt=\"Ensembles - ntrain x mse\" width=\"435.5\" height=\"250.5\" /></a></p>\n<i>Details: 16 workers.</i>\n<h4>Strong scaling: Faster training with more workers</h4>\nThis final figure shows the effect of using a larger compute cluster to solve the same problem. Both methods are significantly faster when using more workers. For example, GBTs with depth-2 trees train about 4.7 times faster on 16 workers than on 2 workers, and larger datasets produce even better speedups.\n<p align=\"center\"><a href=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-workers-x-time.png\"><img class=\"alignnone size-full wp-image-2281\" src=\"https://databricks.com/wp-content/uploads/2015/01/Ensembles-workers-x-time1.png\" alt=\"Ensembles - workers x time\" width=\"442\" height=\"296.5\" /></a></p>\n<i>Details: 463,715 training instances.</i>\n<h2>What’s Next?</h2>\nGBTs will soon include a Python API. The other top item for future development is pluggability: ensemble methods can be applied to almost any classification or regression algorithm, not only Decision Trees. The Pipelines API introduced by Spark 1.2’s <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">experimental spark.ml package</a> will allow us to generalize ensemble methods to be truly pluggable.\n\nTo get started using decision trees yourself, <a href=\"http://spark.apache.org/\">download Spark 1.2 today</a>!\n<h2>Further Reading</h2>\n<ul>\n \t<li>See examples and the API in <a href=\"http://spark.apache.org/docs/latest/mllib-ensembles.html\">the MLlib ensembles documentation</a>.</li>\n \t<li>Learn more background info about the decision trees used to build ensembles in <a href=\"https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html\">this previous blog post</a>.</li>\n</ul>\n<h2>Acknowledgements</h2>\nMLlib ensemble algorithms have been developed collaboratively by the authors of this blog post, Qiping Li (Alibaba), Sung Chung (Alpine Data Labs), and Davies Liu (Databricks). We also thank Lee Yang, Andrew Feng, and Hirakendu Das (Yahoo) for help with design and testing. We will welcome your contributions too!</td></tr><tr><td>List(Tathagata Das)</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2015-01-15, 2015-01-15, UTC)</td><td>Real-time stream processing systems must be operational 24/7, which requires them to recover from all kinds of failures in the system. Since its beginning, Apache Spark Streaming has included support for recovering from failures of both driver and worker machines. However, for some data sources, input data could get lost while recovering from the failures. In Apache Spark 1.2, we have added preliminary support for write ahead logs (also known as journaling) to Spark Streaming to improve this recovery mechanism and give stronger guarantees of zero data loss for more data sources. In this blog, we are going to elaborate on how this feature works and how developers can enable it to get those guarantees in Spark Streaming applications.\n<h2>Background</h2>\nSpark and its RDD abstraction is designed to seamlessly handle failures of any worker nodes in the cluster. Since Spark Streaming is built on Spark, it enjoys the same fault-tolerance for worker nodes. However, the demand of high uptimes of a Spark Streaming application require that the application also has to recover from failures of the<a href=\"https://spark.apache.org/docs/latest/cluster-overview.html\"> driver process</a>, which is the main application process that coordinates all the workers. Making the Spark driver fault-tolerant is tricky because it is an arbitrary user program with arbitrary computation patterns. However, Spark Streaming applications have an inherent structure in the computation -- it runs the same Spark computation periodically on every micro-batch of data. This structure allows us to save (aka, checkpoint) the application state periodically to reliable storage and recover the state on driver restarts.\n\nFor sources like files, this driver recovery mechanism was sufficient to ensure zero data loss as all the data was reliably stored in a fault-tolerant file system like HDFS or S3. However, for other sources like Kafka and Flume, some of the received data that was buffered in memory but not yet processed could get lost. This is because of how Spark applications operate in a distributed manner. When the driver process fails, all the executors running in a standalone/yarn/mesos cluster are killed as well, along with any data in their memory. In case of Spark Streaming, all the data received from sources like Kafka and Flume are buffered in the memory of the executors until their processing has completed. This buffered data cannot be recovered even if the driver is restarted. To avoid this data loss, we have introduced write ahead logs in Spark Streaming in the Apache Spark 1.2 release.\n<h2>Write Ahead Logs</h2>\nWrite Ahead Logs (also known as a journal) are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do. Let us see how we use this concept to ensure the durability of the received data.\n\nSources like Kafka and Flume use Receivers to receive data. They run as long-running tasks in the executors, and are responsible for receiving the data from the source, and if supported by the source, acknowledge the received data. They store the received data in the memory of the executors and the driver then runs tasks on the executors to process the tasks.\n\nWhen write ahead logs are enabled, all the received data is also saved to log files in a fault-tolerant file system. This allows the received data to durable across any failure in Spark Streaming. Additionally, if the receiver correctly acknowledges receiving data only after the data has been to write ahead logs, the buffered but unsaved data can be resent by the source after the driver is restarted. These two together can ensure that there is zero data loss - all data is either recovered from the logs or resent by the source.\n<h2>Configuration</h2>\nWrite ahead logs can be enabled if required by do the following.\n<ul>\n \t<li>Setting the checkpoint directory using streamingContext.checkpoint(path-to-directory). This directory can be set to any Hadoop API compatible file system, and is used to save both streaming checkpoints as well as write ahead logs.</li>\n \t<li><a href=\"http://spark.apache.org/docs/latest/configuration.html#spark-properties\">Setting the SparkConf property</a> spark.streaming.receiver.writeAheadLog.enable to true (default is false).</li>\n</ul>\nWhen the logs are enabled, all receivers enjoy the benefit of recovering data that were reliably received. It is recommended that the in-memory replication be disabled (by setting the appropriate<a href=\"http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence\"> persistence level</a> in the input stream) as the fault-tolerant file system used for the write ahead log likely to be replicating the data as well.\n\nAdditionally, if you want to recover even the buffered data, you will have to use a source that support acking (like Kafka, Flume and Kinesis), and<a href=\"http://spark.apache.org/docs/latest/streaming-custom-receivers.html\"> implement a reliable receiver</a> that correctly acks the source when data is reliably stored in the log. The built in Kafka and Flume Polling receivers already are reliable.\n\nFinally, it is worth noting that there may be a slight reduction in the data ingestion throughput on enabling the write ahead logs. Since all the received data will be written to a fault-tolerant file system, the write throughput of the file system, and the network bandwidth used for the replication can become potential bottlenecks. In that case, either create<a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch\"> more receivers be used for increasing the parallelism of receiving the data</a> and/or use better hardware to increase the throughput of the fault-tolerant file system.\n<h2>Implementation Details</h2>\nLet us dive a bit deeper to understand how the write ahead logs work. In the context, let us walk through the general Spark Streaming architecture.\n\nWhen a Spark Streaming application starts (i.e., the driver starts), the associated StreamingContext (starting point of all streaming functionality) uses the SparkContext to launch Receivers as long running tasks. These Receivers receive and save the streaming data into Spark’s memory for processing. The lifecycle of this data received through users is as follows (refer to the diagram below).\n<ul>\n \t<li>Receiving data (blue arrows) - A Receiver chunks up the stream of data into blocks, that are stored in the memory of the executor. Additionally, if enabled, the data is also written to a write ahead log in a fault tolerant file systems.</li>\n \t<li>Notifying driver (green arrows) - The metadata of the received blocks are sent to the StreamingContext in the driver. This metadata includes  - (i) reference ids of the blocks for locating their data in the executor memory, (ii) offset information of the block data in the logs (if enabled).</li>\n \t<li>Processing the data (red arrow) - Every batch interval, the StreamingContext uses the block information to generate RDDs and jobs on them. The SparkContext executes these jobs by running tasks to process the in-memory blocks in the executors.</li>\n \t<li>Checkpointing the computation (orange arrow) - To recover, the streaming computation (i.e. the DStreams set up with the StreamingContext) is periodically checkpointed to another set of files in the same fault-tolerant file system.</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-52.png\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-52-1024x502.png\" alt=\"Checkpointing the computation and saving the received block data and metadata to write ahead logs\" width=\"625\" height=\"306\" /></a>\n\nWhen a failed driver is restart, the following occurs (see the next diagram).\n<ul>\n \t<li><i>Recover computation (orange arrow)</i> - The checkpointed information is used to restart the driver, reconstruct the contexts and restart all the receivers.</li>\n \t<li><i>Recover block metadata (green arrow)</i> - The metadata of all the blocks that will be necessary to continue the processing will be recovered.</li>\n \t<li><i>Re-generate incomplete jobs (red arrow)</i> - For the batches with processing that has not completed due to the failure, the RDDs and corresponding jobs are regenerated using the recovered block metadata.</li>\n \t<li><i>Read the block saved in the logs (blue arrow)</i> - When those jobs are executed, the block data is read directly from the write ahead logs. This recovers all the necessary data that were reliably saved to the logs.</li>\n \t<li><i>Resend unacknowledged data (purple arrow)</i> - The buffered data that was not saved to the log at the time of failure will be sent again by the source. as it had not been acknowledged by the receiver.</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-4.jpg\"><img src=\"https://databricks.com/wp-content/uploads/2015/01/blog-ha-4.jpg\" alt=\"Recovering the computation from and data on driver restart\" width=\"656\" height=\"324\" /></a>\n\nSo with the write ahead logs as well as the reliable receivers, Spark Streaming can guarantee that no input data will be lost due to driver failures (or for that matter, any failures).\n<h2>Future Directions</h2>\nSome of the possible future directions regarding write ahead logs are as follows.\n<ul>\n \t<li>Systems like Kafka can replicate data for reliability. Enabling write ahead logs effectively replicates the same data twice - once by Kafka and another time by Spark Streaming. Future versions of Spark will include native support for fault tolerance with Kafka that avoids a second log.</li>\n \t<li>Performance improvements (especially throughput) in writing to the write ahead logs.</li>\n</ul>\n<h2>Credits</h2>\nMajor credits for implementing this feature goes to the following.\n<ul>\n \t<li>Tathagata Das (Databricks) - Overall design and major parts of the implementation.</li>\n \t<li>Hari Shreedharan (Cloudera) - Writing and reading of write ahead logs.</li>\n \t<li>Saisai Shao (Intel) - Improvements to the built in Kafka support.</li>\n</ul>\n<h2>Further References</h2>\n<ul>\n \t<li>Refer to the <a href=\"https://spark.apache.org/docs/latest/streaming-programming-guide.html\">Spark Streaming Programming Guide</a> for more information about checkpoint and write ahead logs.</li>\n \t<li>Spark <a href=\"http://youtu.be/jcJq3ZalXD8?t=27m13s\">Meetup talk</a> on this topic</li>\n \t<li>Associated JIRA - <a href=\"https://issues.apache.org/jira/browse/SPARK-3129\">SPARK-3129</a></li>\n</ul></td></tr><tr><td>List(Kavitha Mariappan)</td><td>List(Announcements, Company Blog)</td><td>List(2015-01-27, 2015-01-27, UTC)</td><td>In partnership with <a href=\"https://typesafe.com/\">Typesafe</a>, we are excited to see the publication of the <a href=\"http://info.typesafe.com/COLL-20XX-Spark-Survey-Report_LP.html?lst=PR&amp;lsd=COLL-20XX-Spark-Survey-Trends-Adoption-Report\">survey report</a> representing the largest poll of Apache Spark developers to date. Spark is currently the most active open source project in big data and has been rapidly gaining traction over the past few years. This survey of over 2100 respondents further validates the wide variety of use cases and environments where it is being deployed.\n\nThe survey results indicate that 13% are already using Spark in production environments with 20% of the respondents with plans to deploy Spark in production environments in 2015, and 31% are currently in the process of evaluating it. In total, the survey covers over 500 enterprises that are using or planning to use Spark in production environments ranging from on-premise Hadoop clusters to public clouds, with data sources including key-value stores, relational databases, streaming data and file systems. Applications range from batch workloads to SQL queries, stream processing and machine learning, highlighting Spark’s unique capability as a simple, unified platform for data processing.\n\nAt Databricks and within the Spark community, this type of feedback helps us enhance Spark for more use cases making big data simpler for enterprises of all sizes. As the creators of and most active contributors to Spark, we are happy to see widespread adoption of Spark across a wide range of industries.\n\nIn 2015, we are working on a number of Spark initiatives, including integrated SQL and data frame capabilities, improved stream processing capabilities, extending Spark's machine learning library (MLlib), and richer APIs in Python and R. We have also taken Spark to the cloud in our hosted service,<a href=\"https://databricks.com/product\"> Databricks Cloud</a>, that enables users to leverage Apache Spark directly in the cloud giving them a powerful platform to manage clusters, interactively explore and visualize data, and build production data pipelines. If you would like to try Databricks Cloud, <a href=\"https://databricks.com/registration\">sign up now</a>.\n\nWe are excited to partner with Typesafe in this effort and in our shared vision to bring a comprehensive suite of application development tools for developers that let enterprises operate with more agility and speed. The feedback from this survey will be extremely valuable for both companies to collectively enhance the Spark developer experience.\n\nTo learn more, don't forget to check out this<a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1171473\"> Typesafe press release</a>.</td></tr><tr><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>List(Announcements, Company Blog)</td><td>List(2015-02-09, 2015-02-09, UTC)</td><td><a href=\"https://databricks.com/wp-content/uploads/2015/02/large-oreilly-book-cover.jpg\"><img class=\"size-medium wp-image-2486 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/02/large-oreilly-book-cover-228x300.jpg\" alt=\"large oreilly book cover\" width=\"228\" height=\"300\" /></a>\n\nToday we are happy to announce that the complete <a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\"><i>Learning Spark</i></a> book is available from O’Reilly in e-book form with the print copy expected to be available February 16th. At Databricks, as the creators behind Apache Spark, we have witnessed <a title=\"Big data projects are hungry for simpler and more powerful tools: Survey validates Apache Spark is gaining developer traction!\" href=\"https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html\" target=\"_blank\">explosive growth in the interest and adoption of Spark</a>, which has quickly become one of the most active software projects in Big Data. To continue fostering the developer and user communities around Spark we created a book to help engineers and data scientists learn Spark and use it to solve their most challenging problems.\n\nLearning Spark covers Spark’s rich collection of data programming APIs and libraries (e.g., <a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">MLlib</a>), which make it easy for data scientists to use cutting edge statistical approaches to tackle problems using data of unprecedented scale. Engineers, meanwhile, will learn how to write general-purpose distributed programs in Spark as well as configure and operate production deployments of Spark.\n\nThe Learning Spark book does not require any existing Spark or distributed systems knowledge, though some knowledge of Scala, Java, or Python might be helpful.\n\nThe topics covered include Spark’s core general purpose distributed computing engine, as well as some of Spark’s most popular components including <a title=\"Spark SQL Data Sources API: Unified Data Access for the Spark Platform\" href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">Spark SQL</a>, Spark Streaming, and Spark's Machine Learning library <a title=\"ML Pipelines: A New High-Level API for MLlib\" href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\" target=\"_blank\">MLlib</a>. Both new and existing Spark practitioners will be able to learn Spark best practices as well as important tuning tricks and debugging skills.\n\nThe book is available today from <a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\">O’Reilly</a>, <a href=\"http://amzn.to/1Cw1BTa\" target=\"_blank\">Amazon</a>, and <a href=\"https://books.google.com/books?id=tOptBgAAQBAJ&amp;dq=learning+spark&amp;source=gbs_navlinks_s\" target=\"_blank\">others</a> in e-book form<strong>, </strong>as well as print pre-order (expected availability of February 16th) from <a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\">O’Reilly</a>, <a href=\"http://amzn.to/1KsFSuR\" target=\"_blank\">Amazon</a>.  The code examples from the book are available on the books <a href=\"https://github.com/databricks/learning-spark\" target=\"_blank\">GitHub</a> as well as notebooks in the “learning_spark” folder in <a title=\"Databricks Cloud Overview\" href=\"https://databricks.com/product\" target=\"_blank\">Databricks Cloud</a>.\n\nWe are also excited to share the discount code <strong><del>BWORM</del> AUTHD</strong>. This discount is for 40% off print or 50% off ebooks when you buy directly from O'Reilly.\n\nThe authors, Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia will attend <a href=\"http://go.databricks.com/databricks-at-strata-hadoop-world-san-jose\" target=\"_blank\">Strata San Jose</a> (February 17 - 20th 2015). We will be giving talks and on Thursday morning we will be signing books. Please stop by <strong>booth 1021</strong> and visit us.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Customers)</td><td>List(2015-02-13, 2015-02-13, UTC)</td><td>We're really excited to share that <a href=\"http://www.automatic.com\">Automatic Labs </a>has selected Databricks as its preferred big data processing platform.\n\nPress release: <a href=\"http://www.marketwired.com/press-release/automatic-labs-turns-databricks-cloud-faster-innovation-dramatic-cost-savings-1991316.htm\" target=\"_blank\">http://www.marketwired.com/press-release/automatic-labs-turns-databricks-cloud-faster-innovation-dramatic-cost-savings-1991316.htm</a>\n\nAutomatic Labs needed to run large and complex queries against their entire data set to explore and come up with new product ideas. Their prior solution using Postgres impeded the ability of Automatic’s team to efficiently explore data because queries took days to run and data could not be easily visualized, preventing Automatic Labs from bringing critical new products to market. They then deployed Databricks, our simple yet powerful unified big data processing platform on Amazon Web Services (AWS) and realized these key benefits:\n<ul>\n \t<li data-canvas-width=\"533.592\"><strong>Reduced time to bring product to market.</strong> Minimized the time to validate a product idea from months to days by speeding up the interactive exploration over Automatic’s entire data set, and completing queries in minutes instead of days.</li>\n \t<li data-canvas-width=\"533.592\"><strong>Eliminated DevOps and non-core activities.</strong> Freed up one full-time data scientist from non-core activities such as DevOps and infrastructure maintenance to perform core data science activities.</li>\n \t<li data-canvas-width=\"533.592\"><strong>Infrastructure savings.</strong> Realized savings of ten thousand dollars in one month alone on AWS costs due to the ability to instantly set up and tear-down Apache Spark clusters</li>\n</ul>\nWith a mission to connect all cars on the road to the internet, Automatic Labs is now able to run large and complex production workloads with Databricks to explore new product ideas and bring them to market faster, such as custom driving reports, recommendations for users regarding fuel-efficient driving and more.\n\nDownload this <a href=\"https://databricks.com/wp-content/uploads/2015/04/Databricks_Case_Study_Automatic_Labs.pdf\">case study</a> learn more about how Automatic Labs is using Databricks.\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-02-14, 2015-02-14, UTC)</td><td>2014 has been a year of <a href=\"https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html\">tremendous growth</a> for Apache Spark.  It became the most active open source project in the Big Data ecosystem with over 400 contributors, and was adopted by many platform vendors - including all of the major Hadoop distributors.  Through our ecosystem of products, partners, and training at Databricks, we also saw over 200 enterprises deploying Spark in production.\n\nTo help Spark achieve this growth, Databricks has worked broadly throughout the project to improve functionality and ease of use. Indeed, while the community has grown a lot, about 75% of the code added to Spark last year came from Databricks. In this post, we would like to highlight some of the additions we made to Spark in 2014, and provide a preview of our priorities in 2015.\n\nIn general, our approach to developing Spark is two-fold: improving <i>usability and performance</i> for the core engine, and <i>expanding the functionality</i> of libraries on top, such as streaming, SQL, and machine learning. Because all these libraries use the same core engine, they benefit from the same improvements in deployability, performance, etc.\n<h2>Major Spark Additions in 2014</h2>\nOn the core engine, here are the major improvements we’ve made in 2014:\n<ul>\n \t<li><b>Language support:</b> A major requirement for many enterprises was to make Spark available in languages their users were most familiar with, such as Java and Scala. Databricks <a href=\"https://databricks.com/blog/2014/04/14/spark-with-java-8.html\">led the work to integrate Spark with Java 8</a>, offering much simpler syntax to Java users, and led major additions to the Python API including performance improvements and the Python interfaces to MLlib, Spark Streaming and Spark SQL.</li>\n \t<li><b>Production management:</b> We helped to add high-availability features to the <a href=\"https://issues.apache.org/jira/browse/SPARK-610\">Spark standalone master</a> (allowing master recovery through ZooKeeper) and to <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">Spark Streaming</a> (allowing storing input reliably even from unreliable data sources to allow fault recovery later). We also worked with the community to make Spark scale dynamically on YARN, leading to better resource utilization, and to help integrate with Hadoop ecosystem features such as the Hadoop security model.</li>\n \t<li><b>Performance and stability:</b> We rewrote Spark’s shuffle and network layers to provide significantly higher performance, and used this work to <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">break the world record in sorting</a> using Spark, beating the previous Hadoop-based record by 30x in per-node performance. More generally, we have worked broadly to make Spark operators run better on disk, allowing great performance at any scale from petabytes to megabytes.</li>\n</ul>\nOn the libraries side, we’ve also had the fastest growth in Spark’s standard library to date. Databricks contributed the following:\n<ul>\n \t<li><b>Spark SQL:</b> We contributed a new module for structured data makes it much easier to use Spark with data sources like Apache Hive, Parquet and <a title=\"An introduction to JSON support in Spark SQL\" href=\"https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\">JSON</a>, and provides fast SQL connectivity to BI tools like <a title=\"MicroStrategy “Certified on Spark”\" href=\"https://databricks.com/blog/2014/06/04/microstrategy-certified-on-spark.html\">MicroStrategy</a>, <a title=\"Application Spotlight: Qlik\" href=\"https://databricks.com/blog/2014/06/24/application-spotlight-qlik.html\">Qlik</a> and <a title=\"Application Spotlight: Tableau Software\" href=\"https://databricks.com/blog/2014/10/15/application-spotlight-tableau-software.html\">Tableau</a>. Through Spark SQL, both developers and analysts can now more easily leverage Spark clusters.</li>\n \t<li><b>Machine learning library:</b> Databricks contributed multiple <a href=\"https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html\">new</a> <a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\">algorithms</a> and <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">optimizations</a> to MLlib, Spark’s machine learning library, speeding up some tasks by as much as <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">5x</a>. We also contributed a <a href=\"https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html\">statistics library</a> as a new, high-level <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">pipeline API</a> to make it easier to write complete machine learning applications.</li>\n \t<li><b>Graph processing:</b> We worked with UC Berkeley to add <a href=\"https://spark.apache.org/graphx/\">GraphX</a> as the standard graph analytics library in Spark, giving users access to a variety of graph processing algorithms.</li>\n \t<li><b>API stability in Spark 1.0:</b> On a more technical but very important level, we worked with the community to define <a href=\"https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy\">API stability guarantees</a> for Spark 1.x, which means that application written against Spark today will continue running on future versions. This is a crucial feature for enterprises and developers as it allows application portability across vendors and into future versions of Spark.</li>\n</ul>\nLooking back, it’s a bit hard to imagine that a year ago, Spark didn’t have built-in BI connectivity, rich monitoring, or about half of the higher-level libraries it contains today. Nonetheless, this is the rate at which fast-growing projects move. We’re thrilled to continue working with the community to bring even more great features to Spark.\n<h2>What’s Next</h2>\nEven though 2014 has been a great year for Spark, we know that we are only at the beginning of enterprise use of both Spark and big data in general. At Databricks, we’re focused on a handful of major initiatives for Spark in 2015:\n<ul>\n \t<li><strong>Empowering large scale data science.</strong> In 2015, Spark will expand its focus on data scientists by providing higher level, powerful API’s for statistical and analytical processing. The <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">SparkR</a> project, which allows use of Spark from R, is quickly coming of age, and work to <a href=\"https://issues.apache.org/jira/browse/SPARK-5654\">merge SparkR into Spark</a> is already under way. We’re also introducing a <a href=\"https://issues.apache.org/jira/browse/SPARK-5097\">data frame library</a> for use across all of Spark’s language API’s (Java, Scala, Python, and R) and a <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">machine learning pipeline API</a> in MLlib designed to inter-operate with data frames. The data frame library makes working with datasets, small or large, approachable for a wide range of users.</li>\n</ul>\n<ul>\n \t<li><strong>Rich data source integration.</strong> The data management ecosystem is home to a variety of data sources and sinks. Our work on a <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">pluggable data source API for Spark SQL will connect Spark </a>to many traditional enterprise data sources in addition to the new wave of big data / NoSQL storage systems. Work is already under way to connect to JDBC, HBase, and DBF files. To showcase data sources and other Spark integrations from the broader community, we have also recently founded <a href=\"http://spark-packages.org/\">Spark packages</a>, a community index to track third-party libraries available for Spark. Spark packages has over 30 libraries today; we expect it to grow substantially in 2015.</li>\n</ul>\n<ul>\n \t<li><strong>Simplifying deployment with Databricks Cloud.</strong> Our main goal at Databricks remains to make big data simple. This extends beyond designing concise, elegant API’s available in Spark to providing a hassle-free runtime environment for our users. With <a href=\"https://databricks.com/product/databricks-cloud\">Databricks Cloud</a>, we make it easy for users to get started with Spark and big data within minutes, bypassing the many months of setup traditionally needed for a big data project.</li>\n</ul>\nOf course, you can also expect “more of the same”, and continued work on performance and capabilities through Spark. If you’d like to find out more about the latest Spark use cases and developments, sign up for <a href=\"http://spark-summit.org/east/2015\">Spark Summit East</a> in New York City in March. <a title=\"Spark Summit East 2015 Agenda is Now Available\" href=\"https://databricks.com/blog/2015/01/20/spark-summit-east-2015-agenda-is-now-available.html\">The agenda for the conference was recently posted</a>, and it’s going to be our best community conference yet, with high-quality talks from industries including healthcare, finance and transportation.</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-02-19, 2015-02-19, UTC)</td><td>This is a guest blog from our one of our partners: <a href=\"http://www.memsql.com/\" target=\"_blank\">MemSQL</a>\n\n<hr />\n\n&nbsp;\n<h2>Summary</h2>\nCoupling operational data with the most advanced analytics puts data-driven business ahead. The MemSQL Apache Spark Connector enables such configurations.\n<h2>Meeting Transactional and Analytical Needs</h2>\nTransactional databases form the core of modern business operations. Whether that transaction is financial, physical in terms of inventory changes, or experiential in terms of a customer engagement, the transaction itself moves our business forward.\n\nBut while transactions represent the state of our business, analytics tell us patterns of the past, and help us predict patterns of the future. Analytics can tell us what levers influence profitability and put us ahead of the pack.\n\nSuccess in digital business requires both transactional and analytical prowess, including the foremost means to analyze data.\n<h2>Speed and Agility with MemSQL and Apache Spark</h2>\nAs a real-time database for transactions and analytics, MemSQL helps companies simultaneously ingest and query data with a focus on SQL operations. SQL is the lingua franca of business database operations and provides rich capabilities for complex queries, but there are some thing even SQL cannot accomplish.\n\nIn the cases where analysts and data scientists want the ability to manipulate and explore data in new ways, Apache Spark has emerged as the premier data processing framework that is fast, programmatic, and scalable. So that MemSQL users can take advantage of this functionality in Spark, MemSQL recently introduced the MemSQL Spark Connector.\n<h2>MemSQL Spark Connector Architecture</h2>\nThe MemSQL Spark Connector combines the memory-optimized and distributed architectures of both MemSQL and Spark to drive a high-throughput, highly parallelized, bi-directional link between two clusters.\n\nTwo primary components of the MemSQL Spark Connector enable Spark to query from and write to MemSQL.\n<ul>\n \t<li>A <em>MemSQLRDD</em> class for loading data from a MemSQL query</li>\n \t<li>A <em>saveToMemsql</em> function for persisting results to a MemSQL table</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_1.png\"><img class=\"aligncenter size-large wp-image-2750\" src=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_1-1024x461.png\" alt=\"memSQL_1\" width=\"1024\" height=\"461\" /></a>\n<p style=\"text-align: center;\"><b>Figure 1: MemSQL Spark Connector Architecture</b></p>\n\n<h2>Bringing Data to the Light of Day</h2>\nThe MemSQL Spark Connector takes the most current operational data and makes it accessible from Spark, expanding the analytics capabilities of MemSQL with the full range of Spark tools and libraries.\n\nMemSQL users can employ Spark’s rich analytical functionality through the following steps.\n<ul>\n \t<li>Set up a replicated cluster providing clear demarcation between operations and analytics teams</li>\n \t<li>Give Spark access to live production data for the most recent and relevant results</li>\n \t<li>Allow Spark to write results set back to the primary MemSQL cluster to put new analyses into production</li>\n</ul>\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_2.png\"><img class=\"aligncenter size-large wp-image-2752\" src=\"https://databricks.com/wp-content/uploads/2015/02/memSQL_2-1024x359.png\" alt=\"memSQL_2\" width=\"1024\" height=\"359\" /></a>\n<p style=\"text-align: center;\"><b>Figure 2: Extend MemSQL Analytics</b></p>\n\n<h2>Twin Power of Memory Optimized Clusters</h2>\nWith both clusters operating at lightening fast memory optimized speeds and able to parallel process data transfers between Spark RDDs and MemSQL tables, the combination delivers top performance.\n\nGiven the native integration with Spark, data transfer is convenient as an advanced SQL query can be leveraged to push down computation to MemSQL and only transfer the data needed.\n\nFor more information on the MemSQL Spark Connector please visit:\n\n<a href=\"https://github.com/memsql/memsql-spark-connector\">Github Site for MemSQL Spark Connector</a>\n\n<a href=\"http://blog.memsql.com/memsql-spark-connector/\">MemSQL Technical Blog Post</a>\n\n<a href=\"http://www.memsql.com/download/\">MemSQL free 30 day trial</a></td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-02-17, 2015-02-17, UTC)</td><td>Today, we are excited to announce a new DataFrame API designed to make big data processing even easier for a wider audience.\n\nWhen we first open sourced Apache Spark, we aimed to provide a simple API for distributed data processing in general-purpose programming languages (Java, Python, Scala). Spark enabled distributed data processing through functional transformations on distributed collections of data (RDDs). This was an incredibly powerful API: tasks that used to take thousands of lines of code to express could be reduced to dozens.\n\nAs Spark continues to grow, we want to enable wider audiences beyond “Big Data” engineers to leverage the power of distributed processing. The new DataFrames API was created with this goal in mind.  This API is inspired by data frames in R and Python (Pandas), but designed from the ground-up to support modern big data and data science applications. As an extension to the existing RDD API, DataFrames feature:\n<ul>\n \t<li>Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster</li>\n \t<li>Support for a wide array of data formats and storage systems</li>\n \t<li>State-of-the-art optimization and code generation through the Spark SQL <a href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Catalyst</a> optimizer</li>\n \t<li>Seamless integration with all big data tooling and infrastructure via Spark</li>\n \t<li>APIs for Python, Java, Scala, and R (in development via <a href=\"http://amplab-extras.github.io/SparkR-pkg/\">SparkR</a>)</li>\n</ul>\nFor new users familiar with data frames in other programming languages, this API should make them feel at home. For existing Spark users, this extended API will make Spark easier to program, and at the same time improve performance through intelligent optimizations and code-generation.\n<h2>What Are DataFrames?</h2>\nIn Spark, a DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n\nThe following example shows how to construct DataFrames in Python. A similar API is available in Scala and Java.\n\n[python]\n# Constructs a DataFrame from the users table in Hive.\nusers = context.table(\"users\")\n\n# from JSON files in S3\nlogs = context.load(\"s3n://path/to/data.json\", \"json\")\n[/python]\n<h2>How Can One Use DataFrames?</h2>\nOnce built, DataFrames provide a domain-specific language for distributed data manipulation.  Here is an example of using DataFrames to manipulate the demographic data of a large population of users:\n\n[python]\n# Create a new DataFrame that contains “young users” only\nyoung = users.filter(users.age &lt; 21)\n\n# Alternatively, using Pandas-like syntax\nyoung = users[users.age &lt; 21]\n\n# Increment everybody’s age by 1\nyoung.select(young.name, young.age + 1)\n\n# Count the number of young users by gender\nyoung.groupBy(\"gender\").count()\n\n# Join young users with another DataFrame called logs\nyoung.join(logs, logs.userId == users.userId, \"left_outer\")\n[/python]\n\nYou can also incorporate SQL while working with DataFrames, using Spark SQL. This example counts the number of users in the <i>young</i> DataFrame.\n\n[python]\nyoung.registerTempTable(\"young\")\ncontext.sql(\"SELECT count(*) FROM young\")\n[/python]\n\nIn Python, you can also convert freely between Pandas DataFrame and Spark DataFrame:\n\n[python]\n# Convert Spark DataFrame to Pandas\npandas_df = young.toPandas()\n\n# Create a Spark DataFrame from Pandas\nspark_df = context.createDataFrame(pandas_df)\n[/python]\n\nSimilar to RDDs, DataFrames are evaluated lazily. That is to say, computation only happens when an action (e.g. display result, save output) is required. This allows their executions to be optimized, by applying techniques such as predicate push-downs and bytecode generation, as explained later in the section \"Under the Hood: Intelligent Optimization and Code Generation\". All DataFrame operations are also automatically parallelized and distributed on clusters.\n<h2>Supported Data Formats and Sources</h2>\nModern applications often need to collect and analyze data from a variety of sources. Out of the box, DataFrame supports reading data from the most popular formats, including JSON files, Parquet files, Hive tables. It can read from local file systems, distributed file systems (HDFS), cloud storage (S3), and external relational database systems via JDBC. In addition, through Spark SQL’s <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">external data sources API</a>, DataFrames can be extended to support any third-party data formats or sources. Existing third-party extensions already include Avro, CSV, ElasticSearch, and Cassandra.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/Introducing-DataFrames-in-Spark-for-Large-Scale-Data-Science1.png\"><img class=\"aligncenter size-full wp-image-2764\" src=\"https://databricks.com/wp-content/uploads/2015/02/Introducing-DataFrames-in-Spark-for-Large-Scale-Data-Science1.png\" alt=\"Introducing DataFrames in Spark for Large Scale Data Science\" width=\"779\" height=\"258\" /></a>\n\nDataFrames’ support for data sources enables applications to easily combine data from disparate sources (known as federated query processing in database systems). For example, the following code snippet joins a site’s textual traffic log stored in S3 with a PostgreSQL database to count the number of times each user has visited the site.\n\n[python]\nusers = context.jdbc(\"jdbc:postgresql:production\", \"users\")\nlogs = context.load(\"/path/to/traffic.log\")\nlogs.join(users, logs.userId == users.userId, \"left_outer\") \\\n.groupBy(\"userId\").agg({\"*\": \"count\"})\n[/python]\n<h2>Application: Advanced Analytics and Machine Learning</h2>\nData scientists are employing increasingly sophisticated techniques that go beyond joins and aggregations. To support this, DataFrames can be used directly in MLlib’s <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">machine learning pipeline API</a>. In addition, programs can run arbitrarily complex user functions on DataFrames.\n\nMost common advanced analytics tasks can be specified using the new pipeline API in MLlib. For example, the following code creates a simple text classification pipeline consisting of a tokenizer, a hashing term frequency feature extractor, and logistic regression.\n\n[python]\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n[/python]\n\nOnce the pipeline is setup, we can use it to train on a DataFrame directly:\n\n[python]\ndf = context.load(\"/path/to/data\")\nmodel = pipeline.fit(df)\n[/python]\n\nFor more complicated tasks beyond what the machine learning pipeline API provides, applications can also apply arbitrarily complex functions on a DataFrame, which can also be manipulated using Spark’s existing RDD API. The following snippet performs a word count, the “hello world” of big data, on the “bio” column of a DataFrame.\n\n[python]\ndf = context.load(\"/path/to/people.json\")\n# RDD-style methods such as map, flatMap are available on DataFrames\n# Split the bio text into multiple words.\nwords = df.select(\"bio\").flatMap(lambda row: row.bio.split(\" \"))\n# Create a new DataFrame to count the number of words\nwords_df = words.map(lambda w: Row(word=w, cnt=1)).toDF()\nword_counts = words_df.groupBy(\"word\").sum()\n[/python]\n<h2>Under the Hood: Intelligent Optimization and Code Generation</h2>\nUnlike the eagerly evaluated data frames in R and Python, DataFrames in Spark have their execution automatically optimized by a query optimizer. Before any computation on a DataFrame starts, the <a href=\"https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html\">Catalyst optimizer</a> compiles the operations that were used to build the DataFrame into a physical plan for execution. Because the optimizer understands the semantics of operations and structure of the data, it can make intelligent decisions to speed up computation.\n\nAt a high level, there are two kinds of optimizations. First, Catalyst applies logical optimizations such as predicate pushdown. The optimizer can push filter predicates down into the data source, enabling the physical execution to skip irrelevant data. In the case of Parquet files, entire blocks can be skipped and comparisons on strings can be turned into cheaper integer comparisons via dictionary encoding. In the case of relational databases, predicates are pushed down into the external databases to reduce the amount of data traffic.\n\nSecond, Catalyst compiles operations into physical plans for execution and generates <a href=\"https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html\">JVM bytecode</a> for those plans that is often more optimized than hand-written code. For example, it can choose intelligently between broadcast joins and shuffle joins to reduce network traffic. It can also perform lower level optimizations such as eliminating expensive object allocations and reducing virtual function calls. As a result, we expect performance improvements for existing Spark programs when they migrate to DataFrames.\n\nSince the optimizer generates JVM bytecode for execution, Python users will experience the same high performance as Scala and Java users.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\"><img class=\"aligncenter size-large wp-image-2767\" src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM-1024x457.png\" alt=\"DataFrame performance\" width=\"1024\" height=\"457\" /></a>\n\nThe above chart compares the runtime performance of running group-by-aggregation on 10 million integer pairs on a single machine (<a href=\"https://gist.github.com/rxin/c1592c133e4bccf515dd\">source code</a>). Since both Scala and Python DataFrame operations are compiled into JVM bytecode for execution, there is little difference between the two languages, and both outperform the vanilla Python RDD variant by a factor of 5 and Scala RDD variant by a factor of 2.\n\nDataFrames were inspired by previous distributed data frame efforts, including Adatao's DDF and Ayasdi's BigDF. However, the main difference from these projects is that DataFrames go through the Catalyst optimizer, enabling optimized execution similar to that of Spark SQL queries. As we improve the Catalyst optimizer, the engine also becomes smarter, making applications faster with each new release of Spark.\n\nOur data science team at Databricks has been using this new DataFrame API on our internal data pipelines. It has brought performance improvements to our Spark programs while making them more concise and easier to understand. We are very excited about it and believe it will make big data processing more accessible to a wider array of users.\n\nThis API will be released as part of Spark 1.3 in early March. If you can’t wait, check out <a href=\"https://github.com/apache/spark/tree/branch-1.3\">Spark from GitHub</a> to try it out. If you are in the Bay Area at the Strata conference, please join us on <a href=\"http://www.meetup.com/spark-users/events/220031485/\">Feb 17 in San Jose for a meetup on this topic</a>.\n\nThis effort would not have been possible without the prior data frame implementations, and thus we would like to thank the developers of R, Pandas, DDF and BigDF for their work.\n\n<em>To try out DataFrames, <a href=\"https://databricks.com/try-databricks\">get a free trial of Databricks or use the Community Edition</a>.</em></td></tr><tr><td>null</td><td>List(Company Blog, Events)</td><td>List(2015-02-24, 2015-02-24, UTC)</td><td>The Strata + Hadoop World Conference in San Jose last week was abuzz with \"putting data to work\" in keeping with this year's conference theme. This was a significant shift from last year's event where organizations were highly focused on getting their arms around their big data projects and being steeped in evaluating the multitude of tools of new technologies available. Last week's event highlighted what is top of mind for enterprises and developers alike - how to turn their big data initiatives and projects into real business results?\n\nOne theme was loud and clear - Apache Spark's flame shone bright!  Derrick Harris from GigaOM summed this up aptly in his article \"<a href=\"https://gigaom.com/2015/02/20/for-now-spark-looks-like-the-future-of-big-data/\" target=\"_blank\">For now, Spark looks like the future of big data</a>\". To quote Derrick, <em>\"Titles can be misleading. For example, the O’Reilly Strata + Hadoop World conference took place in San Jose, California, this week but Hadoop wasn’t the star of the show. Based on the news I saw coming out of the event, it’s another Apache project — Spark — that has people excited.\"</em>\n\nTo preface David Ramel from his article yesterday entitled <a href=\"http://adtmag.com/articles/2015/02/23/spark-continues-rise.aspx\" target=\"_blank\">Spark Continues Big Data Ascension</a>, <em>\"The flurry of Spark-related news and product releases further cements the project as the darling of the open source Big Data movement, showing a \"hockey stick-like\" growth in a chart measuring Spark awareness, according to a recent survey. It has been recognized as the most active Apache Software Foundation project and, indeed, most active Big Data open source project of any kind.\"</em>\n\nThe show was also jam-packed with announcements from our partners in the Spark ecosystem, with <a title=\"Extending MemSQL Analytics with Spark\" href=\"https://databricks.com/blog/2015/02/19/extending-memsql-analytics-with-spark.html\" target=\"_blank\">MemSQL</a> and <a href=\"http://www.tableau.com/about/press-releases/2015/tableau-delivers-additional-flexibility-hadoop-community-spark-sql-suppo-0\" target=\"_blank\">Tableau</a> announcing new Spark connectors. At the same time, we announced <a href=\"http://www.marketwired.com/press-release/update-databricks-intel-collaborate-optimize-apache-spark-based-analytics-intelr-architecture-1993678.htm\">a collaboration effort with Intel</a> to optimize Spark-based analytics on Intel architecture.\n\nLooking beyond the four walls of the show, <a href=\"https://twitter.com/dberkholz/status/568561792751771648\" target=\"_blank\">Donnie Berkholz of Redmonk reflected</a> on the incredible hockey stick like surge of Spark interest amongst users, as observed on Stack Overflow:\n\n<img class=\"\" src=\"https://pbs.twimg.com/media/B-Pv0B-IAAAwjYz.png\" alt=\"Big Data activity on Stack Overflow\" width=\"394\" height=\"321\" />\n\nAs the team behind Spark, we at Databricks are thrilled to have the opportunity to respond to this intense interest with Spark and to connect with users.  In line with this year's conference theme of turning their data initiatives into value, we had the opportunity to interact with enterprise users were keen to share with and also learn about running Spark in production on <a href=\"http://www.slideshare.net/databricks/introducing-databricks-cloud-strata-sj\">Databricks Cloud</a>.\n\nFor those of you who missed our sessions at Strata SJ last week, here are the pointers to some of the presentations and training material:\n\nThe material from the Spark Camp training class that was attended by over 300 students can be found <a href=\"http://www.slideshare.net/databricks/sparkcamp-strata-ca-intro-to-apache-spark-with-handson-tutorials\">here</a>. Info on future Spark training classes, can be found on the <a href=\"https://databricks.com/services/spark-training\">training page</a> of our website.\n\nOur Strata San Jose 2015 presentations can be found on our <a href=\"http://www.slideshare.net/databricks\" target=\"_blank\">slideshare account</a>:\n<ul>\n\t<li><a href=\"http://www.slideshare.net/databricks/large-scalesparktalk\" target=\"_blank\"><em>Lessons from Running Large Scale Spark Workloads - </em>Reynold Xin, Matei Zaharia</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/spark-streaming-state-of-the-union-strata-san-jose-2015\" target=\"_blank\"><em>Spark Streaming — The State of the Union, and Beyond  - </em>Tathagata Das</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/new-directions-for-apache-spark-in-2015\" target=\"_blank\"><em>New Directions for Spark in 2015  - </em>Matei Zaharia</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/strata-debugging-talk\" target=\"_blank\"><em>Tuning and Debugging in Apache Spark - </em>Patrick Wendell</a></li>\n\t<li><a href=\"http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs\" target=\"_blank\"><em>Everyday I’m Shuffling – Tips for Writing Better Spark Programs - </em>Vida Ha, Holden Karau</a></li>\n</ul>\nFor all of you east coast Spark enthusiasts, we will be holding the inaugural <a href=\"http://spark-summit.org/east\" target=\"_blank\">Spark Summit East</a> in New York City on March 18th through 19th.  The <a title=\"Spark Summit East 2015 Agenda is Now Available\" href=\"http://spark-summit.org/east/2015/agenda\" target=\"_blank\">agenda has been released</a>, and there will be many more informative sessions for all.  <a href=\"http://prevalentdesignevents.com/sparksummit2015/east/registration.aspx\" target=\"_blank\">Register today</a> if you have not done so!\n\nTo keep up with Databricks news, don't forget to sign up for our monthly newsletter <a href=\"http://go.databricks.com/newsletter-registration\" target=\"_blank\">here</a>.</td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-03-04, 2015-03-04, UTC)</td><td><div class=\"article-body\">\n\nEnterprises have been collecting ever-larger amounts of data with the goal of extracting insights and creating value. Yet despite a few innovative companies who are able to successfully exploit big data, the promised returns of big data remain elusive beyond the grasp of many enterprises.\n\nOne notable and rapidly growing open source technology that has emerged in the big data space is Apache Spark.\n\nSpark is an open source data processing framework that was built for speed, ease of use, and scale. Much of its benefits are due to how it unifies critical data analytics capabilities such as SQL, machine learning and streaming in a single framework. This enables enterprises to simultaneously achieve high performance computing at scale while simplifying their data processing infrastructure by avoiding the difficult integration of many disparate and difficult tools with a single powerful yet simple alternative.\n\nWhile Spark appears to have the potential to solve many big data challenges facing enterprises, many continue to struggle. Why? Because capturing value from big data requires capabilities beyond data processing; enterprises are finding out that there are many challenges in their journey to operationalize their data pipeline.\n\nFirst there is the infrastructure issue requiring data teams to pre-provision, setup and manage on-premise clusters that are both costly and time consuming. After solving the imminent infrastructure challenges, enterprises still have to contend with primitive tools that are difficult to use where working with data, code, and visualization requires switching between different software. These tools also force individuals to work in silos, stifling collaboration and making the sharing of work and communication of insights to the rest of the organization difficult.\n\nIn this typical scenario, enterprises are forced to take on the difficult task of building custom capabilities on top of Spark in order to operationalize it as an effective data platform. This severely reduces the productivity of data analytics teams, degrades their ability to focus on core tasks, and renders every big data project highly susceptible to failure. Indeed Gartner states that, “through 2017, 60% of big-data projects will fail to go beyond piloting and experimentation and will be abandoned.”\n\nInstead of attempting to operationalize Spark in-house, enterprises can benefit from obtaining Spark and the capabilities necessary to operationalize it in a single package that is easy to deploy, simple to learn, and provides the rich set of tools out-of-the-box. One of the key attributes of <a href=\"https://databricks.com/product/databricks-cloud\" target=\"_blank\" rel=\"nofollow\">Databricks Cloud</a> is its ability to provide Spark as-a-service to enterprises in a unified cloud-hosted data platform.\n\nDatabricks Cloud provides fully managed Spark clusters that can be dynamically scaled up and down in a matter of seconds. This frees enterprises to focus on extracting value out of their data instead of spending their valuable resources on operations. In addition to Spark as-a-service, Databricks Cloud includes other critical components required by enterprises to fully develop, test, deploy and manage their end-to-end data pipeline from prototype, all the way to production with no re-engineering required. These include:\n<ol>\n \t<li>An interactive workspace for exploration and visualization so teams can learn, work and collaborate in a single, easy to use environment;</li>\n \t<li>A production pipeline scheduler that helps projects go from prototype to production without re-engineering;</li>\n \t<li>An extensible platform that enables organizations to connect their existing data applications with Spark to disseminate the power of big data.</li>\n</ol>\nWith these critical components, enterprises could seamlessly transition from data ingest to exploration and production while leveraging the power of Spark. They will able to overcome the existing bottlenecks that impede their ability to operationalize Spark, and instead, focus on finding answers from their data, building data products, and ultimately capture the value promised by big data.\n\n</div></td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Customers)</td><td>List(2015-03-05, 2015-03-05, UTC)</td><td>We’re thrilled to share that Radius Intelligence has selected Databricks as its preferred big data processing platform, to deliver real-time insights in support of targeted marketing campaigns.\n\nPress release: <a href=\"http://www.marketwired.com/press-release/radius-intelligence-implements-databricks-cloud-maximize-data-processing-throughput-1997836.htm\" target=\"_blank\">http://www.marketwired.com/press-release/radius-intelligence-implements-databricks-cloud-maximize-data-processing-throughput-1997836.htm</a><a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1179658\" target=\"_blank\">\n</a>\nRadius is a marketing intelligence platform that enables B2B marketers to acquire new customers intelligently. By matching customer intelligence data to Radius’ weekly-updated data set of 25 million businesses in the US, marketers can deploy targeted campaigns of greats leads — net-new and existing opportunities — to their Salesforce.com instance. Radius provides two specific features, Segments and Insights, that use a combination of customer data and external data to allow CMOs to predict their future marketing and campaign success. On a daily basis, Radius processes billions of data points from customers and external data sources.Previously, Radius used Amazon Elastic MapReduce to process data which hampered team effectiveness, code maintainability, and ability to test new methods. Radius then chose Apache Spark for their data processing framework to maximize throughput, maximize speed, and maximize engineer productivity. In order to fully harness the power of Apache Spark, Radius deployed Databricks to maintain their Spark infrastructure and to provide additional critical data processing components on top of Spark.\n\n<article class=\"blog-post-article\">Since implementing Spark on Databricks, Radius’ core data index build now takes a few hours compared to more than a day with a MapReduce based system. Databricks has enabled the Radius teams to work together — data scientists and data engineers — on difficult problems that require a combination of quality development and the scientific method. The process of testing hypothesis now can be done in a matter of minutes and in real-time rather than over the course of days.</article>Since implementing  Databricks, Radius has seen the following company-wide benefits:\n<ul>\n \t<li>Dramatic increase in overall team effectiveness</li>\n \t<li>Improved codebase maintainability</li>\n \t<li>Focus on data science instead of performance optimizations</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/wp-content/uploads/2015/04/Databricks_Case_Study_Radius.pdf\" target=\"_blank\">case study</a> to learn more about how Radius Intelligence is using Databricks.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Customers)</td><td>List(2015-03-10, 2015-03-10, UTC)</td><td>We’re really excited to announce that Sharethrough has selected Databricks to discover hidden patterns in customer behavior data.\n\nPress release: <a href=\"http://www.marketwired.com/press-release/sharethrough-implements-databricks-cloud-discover-hidden-patterns-advertising-serving-1998953.htm\" target=\"_blank\">http://www.marketwired.com/press-release/sharethrough-implements-databricks-cloud-discover-hidden-patterns-advertising-serving-1998953.htm</a>\n\nSharethrough builds software for delivering ads into the natural flow of content sites and apps (also known as native advertising). Because Sharethrough serves ads on some of the most popular digital properties such as Forbes and People, the need for a high-performance big data scale processing platform permeates every aspect of their business.\n\nInitially, Sharethrough attempted to establish a big data platform with self-hosted Hadoop clusters, leveraging Hive as the ad hoc query tool. However, this initial platform severely impeded the productivity of the Sharethrough team because the self-hosted clusters were too labor intensive to maintain and Hive was too slow for ad hoc querying.\n\nTo overcome these challenges, Sharethrough turned to Databricks to implement a big data platform that is simultaneously high performance and easy to maintain. They were able to deploy Databricks in Sharethrough’s Virtual Private Cloud (VPC) in AWS within days. The cluster management interface in Databricks was simple enough to enable their engineering team to create, scale, and terminate Spark clusters with a few clicks, instead of dedicating full-time engineers to this task, as was the case with the self-hosted Hadoop clusters.\n\nOnce the Apache Spark clusters were in place, Sharethrough was able to easily bring their clickstream data from AWS S3 into the interactive workspace of Databricks. The interactive workspace provides “notebooks”, enabling users to work with the data in their preferred language - SQL, Python, Java, or Scala.\n\nAs a result of deploying Databricks, Sharethrough gained a number of benefits:\n<ul>\n \t<li>Faster prototyping of new applications</li>\n \t<li>Easier debugging of complex pipelines</li>\n \t<li>Improved overall engineering team productivity.</li>\n</ul>\nDownload this <a title=\"Databricks customer case study\" href=\"https://databricks.com/wp-content/uploads/2015/03/Databricks_Case_Study_Sharethrough.pdf\" target=\"_blank\">case study</a> learn more about how Sharethrough is using Databricks.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-03-13, 2015-03-13, UTC)</td><td>Today I’m excited to announce the general availability of Apache Spark 1.3! Apache Spark 1.3 introduces the widely anticipated DataFrame API, an evolution of Spark’s RDD abstraction designed to make crunching large datasets simple and fast. Apache Spark 1.3 also boasts a large number of improvements across the stack, from Streaming, to ML, to SQL. The release has been posted today on the Apache Spark website.\n\nWe’ll be publishing in depth overview posts covering Spark’s new features over the coming weeks. Some of the salient features of this release are:\n<h2>A new DataFrame API</h2>\nThe DataFrame API that we <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">recently announced</a> officially ships in Apache Spark 1.3. DataFrames evolve Spark’s RDD model, making operations with structured datasets even faster and easier. They are inspired by, and fully interoperable with, Pandas and R data frames, and are available in Spark’s Java, Scala, and Python API’s as well as the upcoming (unreleased) R API. DataFrames introduce new simplified operators for filtering, aggregating, and projecting over large datasets. Internally, DataFrames leverage the Spark SQL logical optimizer to intelligently plan the physical execution of operations to work well on large datasets. This planning permeates all the way into physical storage, where optimizations such as predicate pushdown are applied based on analysis of user programs. Read more about the data frames API in the SQL programming guide.\n\n<pre># Constructs a DataFrame from a JSON dataset.\nusers = context.load(\"s3n://path/to/users.json\", \"json\")\n\n# Create a new DataFrame that contains \"young users\" only\nyoung = users.filter(users.age < 21)\n\n# Alternatively, using Pandas-like syntax\nyoung = users[users.age < 21]\n\n# DataFrame's support existing RDD operators\nprint(\"Young users: \" + young.count())</pre>\n\n<h2>Spark SQL Graduates from Alpha</h2>\nSpark SQL graduates from an alpha component in this release, guaranteeing compatibility for the SQL dialect and semantics in future releases. Spark SQL’s data source API now fully interoperates with the new DataFrame component, allowing users to create DataFrames directly from Hive tables, Parquet files, and other sources. Users can also intermix SQL and data frame operators on the same data sets. New in 1.3 is the ability to read and write tables from a JDBC connection, with native support for Postgres and MySQL and other RDBMS systems. That API adds has write support for producing output tables as well, to JDBC or any other source.\n\n<pre>> CREATE TEMPORARY TABLE impressions\n  USING org.apache.spark.sql.jdbc\n  OPTIONS (\n    url \"jdbc:postgresql:dbserver\",\n    dbtable \"impressions\"\n  )\n\n> SELECT COUNT(*) FROM impressions</pre>\n\n<h2>Built-in Support for Spark Packages</h2>\nWe <a href=\"https://databricks.com/blog/2014/12/22/announcing-spark-packages.html\">earlier announced</a> an initiative to create a community package repository for Spark at the end of 2014. Today <a href=\"http://spark-packages.org/\" target=\"_blank\">Spark Packages</a> has 45 community projects catering to Spark developers, including data source integrations, testing utilities, and tutorials. To make packages easy for Spark users, Apache Spark 1.3 includes support for pulling published packages into the Spark shell or a program with a single flag.\n<pre># Launching Spark shell with a package\n./bin/spark-shell --packages databricks/spark-avro:0.2</pre>\nFor developers, Spark Packages has also created an <a href=\"https://github.com/databricks/sbt-spark-package\" target=\"_blank\">SBT plugin</a> to make publishing packages easy and introduced automatic Spark compatibility checks of new releases.\n<h2>Lower Level Kafka Support in Spark Streaming</h2>\nOver the last few releases, Kafka has become a popular input source for Spark streaming. Apache Spark 1.3 adds a new Kakfa streaming source that leverages Kafka’s replay capabilities to provide reliable delivery semantics without the use of a write ahead log. It also provides primitives which enable exactly once guarantees for applications that have strong consistently requirements. Kafka support adds a Python API in this release, along with new primitives for creating Python API’s in the future. For a full list of Spark streaming features see the upstream release notes.\n<h2>New Algorithms in MLlib</h2>\nApache Spark 1.3 provides a rich set of new algorithms. The latent Dirichlet allocation (LDA) is one of the first topic modeling algorithms to appear in MLlib. We’ll be documenting LDA in more detail in a follow-up post. Spark’s logistic regression has been generalized to multinomial logistic regression for multiclass classification. This release also adds improved clustering through Gaussian mixture models and power iteration clustering, and frequent itemsets mining through FP-growth. Finally, an efficient block matrix abstraction is introduced for distributed linear algebra. Several other algorithms and utilities are added and discussed in the full release notes.\n<h2>Related in-depth blog posts:</h2>\n<ul>\n \t<li><a title=\"What’s new for Spark SQL in Spark 1.3\" href=\"https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html\" target=\"_blank\">What’s new for Spark SQL in Spark 1.3</a></li>\n \t<li><a href=\"https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html\" target=\"_blank\">Topic modeling with LDA: MLlib meets GraphX</a></li>\n \t<li><a title=\"Improvements to Kafka integration of Spark Streaming\" href=\"https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html\" target=\"_blank\">Improvements to Kafka integration of Spark Streaming</a></li>\n</ul>\n\n<hr />\n\nThis post only scratches the surface of interesting features in Apache Spark 1.3. Overall, this release contains more than 1000 patches from 176 contributors making it our largest yet. Head over to the <a href=\"http://spark.apache.org/releases/spark-release-1-3-0.html\" target=\"_blank\">official release notes</a> to learn more about this release, and watch the Databricks blog for more detailed posts about the major features in the next few weeks!</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-03-19, 2015-03-19, UTC)</td><td>This is a guest blog from our one of our partners: <a href=\"http://uncharted.software/\">Uncharted</a> formerly known as Oculus Info, Inc.\n\n<hr />\n\n<strong>About PanTera<sup>TM</sup> </strong>\n\nPanTera was created with the fundamental guiding principles that visualization, interaction, and nuance are critical to understanding data. PanTera unlocks the specific opportunity and richness presented by large amounts of data by enabling interactive visual exploration at scale while preserving detail. These visualizations harness the power of human perception to rapidly identify patterns and form new hypotheses.\n\nCurrent tools have powerful libraries for data manipulation and analysis but largely rely on sampling or aggregation for visualization. This often means seeing the big picture at the expense of the whole picture. At Uncharted, we avoided this compromise by creating an application to allow the exploration of the entirety of large datasets with the ability to focus on any area of interest, all the way down to a single data point.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/BlogPost-NewYorkMagnify.png\"><img class=\"alignnone wp-image-2995 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/03/BlogPost-NewYorkMagnify-1024x721.png\" alt=\"BlogPost-NewYorkMagnify\" width=\"1024\" height=\"721\" /></a>\n\n<em>Figure 1: 1.6 B Geolocated Instagram posts provide an overview of global usage. Zooming in on New York City, reveals detail showing quantization of some of the location data</em>\n\nPanTera and the Databricks make the visual exploration of big data accessible to any analyst, on a platform that is powerful enough for even the most experienced data scientists and developers. The application lets analysts work with multi-dimensional, high-resolution data without making assumptions for them.\n\n<strong>Built with the Speed and Flexibility of Spark</strong>\n\nPanTera allows users to quickly and iteratively explore the entirety of large datasets. To accomplish this, PanTera harnesses the power of Apache Spark to create a tile-based visualization system with parallels to browser-based, geographic maps such as Google Maps. Uncharted chose Spark because it gave us the most flexibility and speed.\n\nSpark allows us the flexibility to work with pre-existing or live streaming data, and to implement batch or on-demand processing, all with a single code base. Spark’s ability to handle SQL queries as well as Scala and Java code let us quickly create a tool that can handle tiling any kind of data, numerical or unstructured text. Most importantly, Spark’s memory-optimized computation caches entire datasets, allowing us to make deep exploration fast and easy.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Bitcoin-Screen-Shot.png\"><img class=\"alignnone wp-image-2996 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/03/Bitcoin-Screen-Shot-1024x684.png\" alt=\"Bitcoin Screen Shot\" width=\"1024\" height=\"684\" /></a>\n\n<em>Figure 2: Cross-Plot of 13M Bitcoin Transactions in PanTera</em>\n\n<strong>Discovery with the ease of Databricks</strong>\n\nThe real-time exploratory nature of PanTera is only possible with on-demand access to cluster computing resources. Databricks allows easy provisioning of the right cluster for the job, and enables flexibility as needs change. Essential elements of data capture and ETL are also handled by Databricks, allowing us to focus on creating great visualizations.\n\nFor our users, Databricks extends PanTera’s capabilities by creating an iterative workflow where analysts visualize data, then perform data manipulation in the Databricks Notebook and visualize again. These critical Databricks features enable our users to perform great analysis and avoid infrastructure headaches.\n\nDon’t forget to check out<a href=\"http://pantera.io\"> our website</a> to learn more about PanTera and experience how our visual interface for big data enhances statistical exploration on Databricks.</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-03-17, 2015-03-17, UTC)</td><td>This is a guest blog from our one of our partners: <a href=\"http://tresata.com/\">Tresata</a>\n\n<hr />\n\nTresata and Databricks announced a real-time, Apache Spark and Hadoop-powered Anti-Money Laundering solution earlier today. Tresata’s predictive analytics application TEAK, offers for the first time in the market an at-scale, real-time AML investigation and resolution engine.\n\nThe performance, speed, predictive power and precision TEAK delivers would not have been possible without its Spark underpinnings.\n\nAdditionally, by being one of the first business applications to be certified to run on Databricks Cloud, Tresata’s TEAK is breaking new ground and offering Banks, Retailers, Telcos &amp; Regulators the only quick start, rapidly scalable AML solution.\n\nTresata has always been at the leading edge of developing industry leading data analytics applications for complex, critical and crucial business processes. It was the first analytics application company powering its entire software with Hadoop, was the first to use cascading, the first to use scalding, and the first to use Spark. In keeping with the ‘trend’, we are proud to be the first to use the Databricks Cloud for deploying a core industry application.\n\nWhen our application engines became one of the <a href=\"https://databricks.com/blog/2014/03/18/Spark-certification.html\">first to be “Certified on Spark”</a>, we immediately saw the massive productivity boost our customers achieved leveraging a 100% Spark-powered analytics platform. This gave us an incentive to work closely with Databricks to also enable one of our recent analytics applications to work on the Databricks Cloud, especially given the enormity of the challenge.\n\n<em>Tresata has always believed that new big data technologies will unleash trillions of dollars of economic value. Money laundering is just one such area where it by itself is a trillion dollar problem…annually. </em>\n\nThe illegal process of taking ‘dirty’ money and making it ‘clean’ requires passing funds through an intricate and interconnected network of people, places &amp; things and their inherent but otherwise unseen interconnections. Why is it that current AML solutions have been solely focused on entity-level (person, business, corporation) or transaction level risk scores, without viewing them within the context of the greater network risk score?\n\nThe short answer is a lack of technological prowess that unites all dimensions of accurately predicting AML. And in real-time.\n\nUntil now.\n\nRecognizing this fatal flaw in current AML solutions - taking an entity-level look at what is a massive network problem – Tresata incubated TEAK in its R&amp;D lab almost two years ago. Powered by the only real-time Spark certified network discovery, traversal and query engine (<a href=\"http://tresata.com/platform/\">Tresata ORION</a>), Tresata successfully deployed its advanced algorithms and capability to look at entire networks in real time to bring a new dimension to this problem – Tresata Network Scores.\n\nThese scores are at the heart of TEAK’s success at precisely predicting fraudulent transactions from not just entity dynamics but based on the entire supply-chain of money movement.\n\nThis required some breakthrough technological innovations, which would not have been possible without Spark, namely:\n<ul>\n \t<li><strong>Interactive Real-Time Scoring Engine:</strong> in memory RDDs &amp; optimized data structures provide incredibly fast refresh response (typically few seconds, no more than 20 seconds for complex queries)</li>\n \t<li><strong>At-scale immediate response graph traversal:</strong> with an easy to use query language (QUE – our SQL-like graph query language) and a property based graph model one can do multi-hop traversals in network analysis</li>\n \t<li><strong>Graph Search Engine:</strong> Ability to scan the entire dataset and enabled to do more than just point queries</li>\n \t<li><strong>Go beyond a few thousand of nodes:</strong> at scale graph engine that works for &gt; 50MM entities, hundreds of millions of edges and doesn’t skimp on performance</li>\n</ul>\nTresata is excited to partner with Databricks to provide an answer to one of the biggest challenges that governments, societies and institutions face today. It is our belief that with the rise of open-source, distributed computing, this collaboration with Databricks spurs a true business revolution – one that applies the massive power of Spark, Hadoop, clouds and algorithms to solving massive business problems.\n\nTresata’s AML solution - powered by Spark and delivered in Databricks cloud – delivers that power today.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Product)</td><td>List(2015-03-18, 2015-03-18, UTC)</td><td>Databricks now includes a new feature called Jobs, enabling support for running production pipelines, consisting of standalone Spark applications. Jobs includes a scheduler that enables data scientists and engineers to specify a periodic schedule for their production jobs, which will be executed according to the specified schedule.\n\n<b>Notebooks as Jobs</b>\n\nIn addition to supporting running standalone Apache Spark applications, the Jobs feature provides a unique capability that allows running Databricks notebooks as jobs. That is, a job can be specified to use an existing Notebook, which will then be executed according to the specified schedule. This enables seamless transition between interactive exploration and production. Thus, data scientists can use notebooks as they did before, to perform their interactive data explorations. Once the notebook is sufficiently developed, it can then be transitioned to production use as a Job, without requiring any time consuming code rewrites. The output of every run of a job, including graphical output, is also stored as a notebook, which can be opened and used as any other notebook, allowing interactive debugging or further post-hoc exploration. This way, data scientists can repeatedly iterate and improve their jobs without having to spend time rewriting and moving code between different systems.\n\n&nbsp;\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/jobs-screenshot.png\"><img class=\"alignnone wp-image-3034 size-large\" src=\"https://databricks.com/wp-content/uploads/2015/03/jobs-screenshot-1024x433.png\" alt=\"jobs-screenshot\" width=\"1024\" height=\"433\" /></a>\n\n&nbsp;\n\n<b>Notebooks as Workflows</b>\n\nIn addition to running notebooks as jobs, users can run compiled applications and libraries as jobs. We have found users to often use a notebook to specify a workflow that calls other standalone jobs. Such workflows can conveniently be scripted in a language such as Python, using simple if-statements and exception-handling. Using notebooks in this way to specify production workflows is very powerful, as virtually any pattern can be expressed using a notebook.\n\n<b>Flexible Cluster Support</b>\n\nJobs integrate with Databricks’ existing clusters. A job can be specified to use an existing Databricks cluster. Furthermore, a job can be specified to have its own dedicated cluster that is launched and torn down on every run. This will ensure that the job gets its own dedicated cluster, isolating it from errors caused by other users and jobs. Clusters can be launched on AWS on-demand instances, as well as the much cheaper spot instances. Furthermore, there is support for a hybrid mode called, fallback-on-demand, which tries to launch most of the cluster machines on spot instances, but will fallback on on-demand instances if the supply of spot instances is limited. This way, organizations can be sure to get the clusters they request, while cutting costs when possible, by using spot instances.\n\n<b>Notification Support</b>\n\nThe job feature comes with a notification system, which can be configured to send an email to a set of users whenever a production job completes or fails. This is particularly important as jobs run with no human-in-the-loop, requiring attention whenever something goes wrong.\n\nThe launch of the jobs feature is aimed at further improving the end-to-end user experience of Databricks. Notebooks can now be used in production workloads, in addition to being useful as Libraries (notebooks can call other notebooks), Dashboards, and online collaboration. Although this is the first official release of the Jobs feature, we have several customers already using it in production environments as part of our early adopter program.\n\nWe would love to hear your feedback - please let us know what you think about this new feature!</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-03-20, 2015-03-20, UTC)</td><td>[sidenote]<strong>Update August 4th 2016:</strong>\nSince this original post, MongoDB has released a new Databricks-certified connector for Apache Spark. See the <a href=\"https://databricks.com/blog/2016/07/21/the-new-mongodb-connector-for-apache-spark-in-action-building-a-movie-recommendation-engine.html\">updated blog post</a> for a tutorial and notebook on using the new MongoDB Connector for Apache Spark.[/sidenote]\n\n<hr/>\n\n[sidenote]This is a guest blog from Matt Kalan, a Senior Solution Architect at <a href=\"http://www.mongodb.com/\" target=\"_blank\">MongoDB</a>[/sidenote]\n\n<hr />\n\n<h2>Introduction</h2>\nThe broad spectrum of data management technologies available today makes it difficult for users to discern hype from reality. While I know the immense value of <a href=\"http://www.mongodb.com/\" target=\"_blank\">MongoDB</a> as a real-time, distributed operational database for applications, I started to experiment with Apache Spark because I wanted to understand the options available for analytics and batch operations.\n\nI started with a simple example of taking 1-minute time series intervals of stock prices with the opening (first) price, high (max), low (min), and closing (last) price of each time interval and turning them into 5-minute intervals (called OHLC bars).   The 1-minute data is stored in MongoDB and is then processed in Spark via the MongoDB Hadoop Connector, which allows MongoDB to be an input or output to/from Spark.\n\nOne might imagine that a more typical example is that you record this market data in MongoDB for real-time purposes but then potentially run the analytical models in another environment offline. Of course the models would be way more complicated – this is just as a Hello World level example. I chose OHLC bars just because that was the data I found easily.\n<h2>Summary</h2>\n<strong>Use case</strong>: aggregating 1-minute intervals of stock prices into 5-minute intervals\n\n<strong>Input</strong>: 1-minute stock prices intervals in a MongoDB database\n\n<strong>Simple Analysis:</strong> performed with Spark\n\n<strong>Output: </strong>5-minute stock price intervals in Spark and optionally write back into MongoDB\n\n<strong>Steps to set up the environment:</strong>\n<ul>\n \t<li><strong>Set up Spark environment</strong> – I installed Spark v1.2.0 in a VM on my Mac laptop</li>\n \t<li><strong>Download sample data</strong> – I acquired these data points in <a href=\"http://www.google.com/url?q=http%3A%2F%2Fwww.barchartmarketdata.com%2Fdata-samples%2Fmstf.csv&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNErKsHfV7l6PKvm6igSQIpuVbfabQ\">1 minute</a> increments from <a href=\"http://www.barchartmarketdata.com/sample-data-feeds\">this web page</a></li>\n \t<li><strong>Install MongoDB on the VM</strong> – I easily installed MongoDB with yum on CentOS with instructions from <a href=\"http://docs.mongodb.org/manual/tutorial/install-mongodb-on-red-hat-centos-or-fedora-linux/\">this page</a></li>\n \t<li><strong>Start MongoDB </strong>– a default configuration file is installed by yum so you can just run this to start on localhost and the default port <em>27017</em> :</li>\n</ul>\n<pre>mongod -f /etc/mongod.conf</pre>\n<ul>\n \t<li><strong>Load sample data</strong> – <a href=\"http://docs.mongodb.org/manual/reference/program/mongoimport/\">mongoimport</a> allows you to load CSV files directly as a flat document in MongoDB. The command is simply this:</li>\n</ul>\n<pre>mongoimport equities-msft-minute-bars-2009.csv --type csv --headerline -d marketdata -c minibars</pre>\n<ul>\n \t<li><strong>Install MongoDB Hadoop Connector</strong> – You can download the Hadoop Connector jar at: <a href=\"https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage\">Using the MongoDB Hadoop Connector with Spark</a>. If you use the Java interface for Spark, you would also download the MongoDB Java Driver jar. Any jars that you download can be added to Spark using the --jars option to the PySpark command. I used Python with Spark below (called PySpark).</li>\n</ul>\nFor the following examples, here is what a document looks like in the MongoDB collection (via the Mongo shell). You start the Mongo shell simply with the command “mongo” from the /bin directory of the MongoDB installation.\n<pre>&gt; use marketdata \n&gt; db.minbars.findOne()\n{\n\t\"_id\" : ObjectId(\"54c00d1816526bc59d84b97c\"),\n \t\"Symbol\" : \"MSFT\",\n\t\"Timestamp\" : \"2009-08-24 09:30\",\n\t\"Day\" : 24,\n \t\"Open\" : 24.41,\n\t\"High\" : 24.42,\n\t\"Low\" : 24.31,\n \t\"Close\" : 24.31,\n\t\"Volume\" : 683713\n}\n</pre>\n<h2>Spark Example</h2>\nFor my initial foray into Spark, I opted to use Python with the interactive shell command “PySpark”. This gave me an interactive Python environment for leveraging Spark classes. Python appears to be popular among quants because it is a more natural language to use for interactive querying compared to Java or Scala. I was able to successfully read from MongoDB in Spark, but make sure you upgrade to Spark v1.2.2 or v1.3.0 to address a bug in earlier versions of PySpark.\n\nThe benefits of Spark were immediately evident, and in line with what you would expect in an interactive environment – queries return quickly, much faster than Hive, due in part to the fact they are not compiled to MapReduce. While the latency is still higher than MongoDB’s internal querying and aggregation framework, there are more options for distributed, multi-threaded analysis with Spark, so it clearly has a role to play for data analytics.\n\n[python]\n# set up parameters for reading from MongoDB via Hadoop input format\nconfig = {&quot;mongo.input.uri&quot;: &quot;mongodb://localhost:27017/marketdata.minbars&quot;}\ninputFormatClassName = &quot;com.mongodb.hadoop.MongoInputFormat&quot;\n# these values worked but others might as well\nkeyClassName = &quot;org.apache.hadoop.io.Text&quot;\nvalueClassName = &quot;org.apache.hadoop.io.MapWritable&quot;\n \n# read the 1-minute bars from MongoDB into Spark RDD format\nminBarRawRDD = sc.newAPIHadoopRDD(inputFormatClassName, keyClassName, valueClassName, None, None, config)\n\n# configuration for output to MongoDB\nconfig[&quot;mongo.output.uri&quot;] = &quot;mongodb://localhost:27017/marketdata.fiveminutebars&quot;\noutputFormatClassName = &quot;com.mongodb.hadoop.MongoOutputFormat&quot;\n \n# takes the verbose raw structure (with extra metadata) and strips down to just the pricing data\nminBarRDD = minBarRawRDD.values()\n \nimport calendar, time, math\n \ndateFormatString = '%Y-%m-%d %H:%M'\n \n# sort by time and then group into each bar in 5 minutes\ngroupedBars = minBarRDD.sortBy(lambda doc: str(doc[&quot;Timestamp&quot;])).groupBy(lambda doc (doc[&quot;Symbol&quot;], math.floor(calendar.timegm(time.strptime(doc[&quot;Timestamp&quot;], dateFormatString)) / (5*60))))\n \n# define function for looking at each group and pulling out OHLC\n# assume each grouping is a tuple of (symbol, seconds since epoch) and a resultIterable of 1-minute OHLC records in the group\n \n# write function to take a (tuple, group); iterate through group; and manually pull OHLC\ndef ohlc(grouping):\n\tlow = sys.maxint\n\thigh = -sys.maxint\n\ti = 0\n\tgroupKey = grouping[0]\n\tgroup = grouping[1]\n\tfor doc in group:\n     \t#take time and open from first bar\n     \tif i == 0:\n          \topenTime = doc[&quot;Timestamp&quot;]\n          \topenPrice = doc[&quot;Open&quot;]\n\n     \t#assign min and max from the bar if appropriate\n     \tif doc[&quot;Low&quot;] &lt; low:\n          \tlow = doc[&quot;Low&quot;]\n\n     \tif doc[&quot;High&quot;] &gt; high:\n          \thigh = doc[&quot;High&quot;]\n\n     \ti = i + 1\n     \t# take close of last bar\n     \tif i == len(group):\n          \tclose = doc[&quot;Close&quot;]\n\t\toutputDoc = {&quot;Symbol&quot;: groupKey[0], \n    \t&quot;Timestamp&quot;: openTime,\n     \t&quot;Open&quot;: openPrice,\n     \t\t&quot;High&quot;: high,\n     \t\t&quot;Low&quot;: low,\n \t\t&quot;Close&quot;: close}\n\n\n\treturn (None, outputDoc)\n\n resultRDD = groupedBars.map(ohlc)\n \nresultRDD.saveAsNewAPIHadoopFile(&quot;file:///placeholder&quot;, outputFormatClassName, None, None, None, None, config)\n[/python]\n\nI saw the appeal of Spark from my first introduction. It was pretty easy to use. It is also especially nice in that it has operations that run on all elements in a list or a matrix of data. I can also see the appeal of having statistical capabilities like R, but in which the data can be distributed across many nodes easily (there is a Spark project for R as well).\n\nSpark is certainly new, and I had to use Spark v1.2.2 or later due to a bug (<a href=\"https://issues.apache.org/jira/browse/SPARK-5361\">SPARK-5361</a>) that initially prevented me from writing from PySpark to a Hadoop file (writing to Hadoop &amp; MongoDB in Java &amp; Scala should work). Another drawback I encountered was the difficulty to visualize data during an interactive session in PySpark. It reminded me of my college days being frustrated debugging matrices representing ray traces in Matlab, before they added better tooling. Likewise there are still challenges in displaying the data in the RDD structures; while there is a function collect() for creating lists that are more easily printable, some elements such as iterables remain difficult to display.\n<h2>Key Takeaways of Using MongoDB with Spark</h2>\n<h3>Spark is easy to integrate with MongoDB</h3>\nOverall it was useful to see how data in MongoDB can be accessed via Spark. In retrospect, I spent more time manipulating the data than I did integrating them with MongoDB, which is what I had hoped. I also started with a pre-configured VM on a single node instead of setting up the environment. I have since learned of the Databricks Cloud, which I expect would make a larger installation easy.\n<h3>Many real-world applications exist</h3>\nA real-life scenario for this kind of data manipulation is storing and querying real-time, intraday market data in MongoDB. Prices update throughout the current day, allowing users to querying them in real-time. Using Spark, after the end of day (even if the next day begins immediately like with FX), individual ticks can be aggregated into structures that are more efficient to access, such as these OHLC bars, or large documents with arrays of individual ticks for the day, by ticker symbol. This approach gives great write throughput during the day for capture, as well as blazing fast access to weeks, month, or years of prices. There are users of MongoDB whose systems follow this approach, and who have dramatically reduced latency for analytics, as well as reduced their hardware footprint. By storing the aggregated data back in MongoDB, you can index the data flexibly and retrieve it quickly.\n\nFor more information, you can visit:\n<ul>\n \t<li><a href=\"https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage\" rel=\"nofollow\">Documentation for Using MongoDB with Spark</a></li>\n \t<li><a href=\"http://www.mongodb.com/what-is-mongodb\">MongoDB Overview</a></li>\n \t<li><a href=\"http://www.mongodb.com/lp/white-paper/big-data-nosql\">MongoDB Big Data White Paper</a></li>\n \t<li><a href=\"https://www.mongodb.com/lp/download/mongodb-enterprise\">Download MongoDB Enterprise Edition</a></li>\n \t<li><a href=\"https://www.mongodb.org/downloads\">Download MongoDB Community edition</a></li>\n</ul></td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-03-24, 2015-03-24, UTC)</td><td>The Apache Spark 1.3 release represents a major milestone for Spark SQL.  In addition to several major features, we are very excited to announce that the project has officially graduated from Alpha, after being introduced only a little under a year ago.  In this blog post we will discuss exactly what this step means for compatibility moving forward, as well as highlight some of the major features of the release.\n<h2>Graduation from Alpha</h2>\nWhile we know many organizations (including all of Databricks' customers) have already begun using Spark SQL in production, the graduation from Alpha comes with a promise of stability for those building applications using this component.  Like the rest of the Spark stack, we now promise binary compatibility for all public interfaces through the Apache Spark 1.X release series.\n\nSince the SQL language itself and our interaction with Apache Hive represent a very large interface, we also wanted to take this chance to articulate our vision for how the project will continue to evolve. A large number of Spark SQL users have data in Hive metastores and legacy workloads which rely on Hive QL. As a result, Hive compatibility will remain a major focus for Spark SQL moving forward\n\nMore specifically, the HiveQL interface provided by the HiveContext remains the most complete dialect of SQL that we support and we are committed to continuing to maintain compatibility with this interface.  In places where our semantics differ in minor ways from Hive's (i.e. <a href=\"https://issues.apache.org/jira/browse/SPARK-5680\" target=\"_blank\">SPARK-5680</a>), we continue to aim to provide a superset of Hive's functionality.  Additionally, while we are excited about all of the new data sources that are available through the improved native Data Sources API (see more below), we will continue to support reading tables from the Hive Metastore using Hive's SerDes.\n\nThe new DataFrames API (also discussed below) is currently marked experimental.  Since this is the first release of this new interface, we wanted an opportunity to get feedback from users on the API before it is set in stone.  That said, we do not anticipate making any major breaking changes to DataFrames, and hope to remove the experimental tag from this part of Spark SQL in Apache Spark 1.4.  You can track progress and report any issues at <a href=\"https://issues.apache.org/jira/browse/SPARK-6116\">SPARK-6116</a>.\n<h2>Improved Data Sources API</h2>\nThe Data Sources API was another major focus for this release, and provides a single interface for loading and storing data using Spark SQL.  In addition to the sources that come prepackaged with the Apache Spark distribution, this API provides an integration point for external developers to add support for custom data sources.  At Databricks, we have already contributed libraries for reading data stored in <a href=\"http://spark-packages.org/package/databricks/spark-avro\">Apache Avro</a> or <a href=\"http://spark-packages.org/package/databricks/spark-csv\">CSV</a> and we look forward to contributions from others in the community (check out <a href=\"http://spark-packages.org/\">spark packages</a> for a full list of sources that are currently available).\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.42.56-PM.png\"><img class=\"aligncenter size-large wp-image-3072\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.42.56-PM-1024x305.png\" alt=\"Spark SQL data sources\" width=\"1024\" height=\"305\" /></a>\n<h2></h2>\n<h2>Unified Load/Save Interface</h2>\nIn this release we added a unified interface to SQLContext and DataFrame for loading and storing data using both the built-in and external data sources.  These functions provide a simple way to load and store data, independent of whether you are writing in Python, Scala, Java, R or SQL.  The examples below show how easy it is to both load data from Avro and convert it into parquet in different languages.\n<h3>Scala</h3>\n\n<pre>val df = sqlContext.load(\"/home/michael/data.avro\", \"com.databricks.spark.avro\")\ndf.save(\"/home/michael/data.parquet\", \"parquet\")</pre>\n\n<h3>Python</h3>\n\n<pre>df = sqlContext.load(\"/home/michael/data.avro\", \"com.databricks.spark.avro\")\ndf.save(\"/home/michael/data.parquet\", \"parquet\")</pre>\n\n<h3>Java</h3>\n\n<pre>DataFrame df = sqlContext.load(\"/home/michael/data.avro\", \"com.databricks.spark.avro\")\ndf.save(\"/home/michael/data.parquet\", \"parquet\")</pre>\n\n<h3>SQL</h3>\n\n<pre>> CREATE TABLE avroData\n  USING com.databricks.spark.avro\n  OPTIONS (\n    path \"/home/michael/data.avro\"\n  )\n\n> CREATE TABLE parquetData\n  USING parquet\n  OPTIONS (\n    path \"/home/michael/data/parquet\")\n  AS SELECT * FROM avroData</pre>\n\n<h2>Automatic Partition Discovery and Schema Migration for Parquet</h2>\nParquet has long been one of the fastest data sources supported by Spark SQL.  With its columnar format, queries against parquet tables can execute quickly by avoiding the cost of reading unneeded data.\n\nIn the Apache Spark 1.3 release we added two major features to this source.  First, organizations that store lots of data in parquet often find themselves evolving the schema over time by adding or removing columns.  With this release we add a new feature that will scan the metadata for all files, merging the schemas to come up with a unified representation of the data.  This functionality allows developers to read data where the schema has changed overtime, without the need to perform expensive manual conversions.\n\nAdditionally, the parquet datasource now supports auto-discovering data that has been partitioned into folders, and then prunes which folders are scanned based on predicates in queries made against this data.  This optimization means that you can greatly speed up may queries simply by breaking up your data into folders.  For example:\n\n<pre>/data/year=2014/file.parquet\n/data/year=2015/file.parquet\n...\n\nSELECT * FROM table WHERE year = 2015</pre>\n\nIn Apache Spark 1.4, we plan to provide an interface that will allow other formats, such as ORC, JSON and CSV, to take advantage of this partitioning functionality.\n<h2>Persistent Data Source Tables</h2>\nAnother feature that has been added in Apache Spark 1.3 is the ability to persist metadata about Spark SQL Data Source tables to the Hive metastore.  These tables allow multiple users to share the metadata about where data is located in a convenient manner.  Data Source tables can live alongside native Hive tables, which can also be read by Spark SQL.\n<h2>Reading from JDBC Sources</h2>\nFinally, a Data Source for reading from JDBC has been added as built-in source for Spark SQL.  Using this library, Spark SQL can extract data from any existing relational databases that supports JDBC.  Examples include mysql, postgres, H2, and more.  Reading data from one of these systems is as simple as creating a virtual table that points to the external table.  Data from this table can then be easily read in and joined with any of the other sources that Spark SQL supports.\n\n<pre>> CREATE TEMPORARY TABLE impressions\n  USING org.apache.spark.sql.jdbc\n  OPTIONS (\n    url \"jdbc:postgresql:dbserver\",\n    dbtable \"impressions\"\n  )\n\n> SELECT COUNT(*) FROM impressions</pre>\n\nThis functionality is a great improvement over Spark's earlier support for JDBC (i.e., <a href=\"https://spark.apache.org/docs/1.3.0/api/java/org/apache/spark/rdd/JdbcRDD.html\">JdbcRDD</a>).  Unlike the pure RDD implementation, this new DataSource supports automatically pushing down predicates, converts the data into a DataFrame that can be easily joined, and is accessible from Python, Java, and SQL in addition to Scala.\n<h2>Introducing DataFrames</h2>\nWhile we have already talked about the DataFrames in <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">other blog posts</a> and <a href=\"http://www.slideshare.net/databricks/spark-sqlsse2015public\">talks at the Spark Summit East</a>, any post about Apache Spark 1.3 would be remiss if it didn't mention this important new API. DataFrames evolve Spark’s RDD model, making it faster and easier for Spark developers to work with structured data by providing simplified methods for filtering, aggregating, and projecting over large datasets. Our DataFrame implementation was inspired by Pandas' and R's data frames, and are fully interoperable with these implementations.  Additionally, Spark SQL DataFrames are available in Spark’s Java, Scala, and Python API’s as well as the upcoming (unreleased) R API.\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.59.28-PM.png\"><img class=\"aligncenter size-large wp-image-3081\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-3.59.28-PM-1024x236.png\" alt=\"Spark SQL blog - dataframes\" width=\"1024\" height=\"236\" /></a>Internally, DataFrames take advantage of the Catalyst query optimizer to intelligently plan the execution of your big data analyses. This planning permeates all the way into physical storage, where optimizations such as predicate pushdown are applied based on analysis of user programs.  Since this planning is happening at the logical level, optimizations can even occur across function calls, as shown in the example below.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-4.00.33-PM.png\"><img class=\"aligncenter size-large wp-image-3082\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-23-at-4.00.33-PM-1024x577.png\" alt=\"dataframe optimizations\" width=\"1024\" height=\"577\" /></a>\n\nIn this example, Spark SQL is able to push the filtering of users by their location through the join, greatly reducing its cost to execute.  This optimization is possible even though the original author of the <code>add_demographics</code> function did not provide a parameter for specifying how to filter users!\n\nThis is only example of how Spark SQL DataFrames can make developers more efficient by providing a simple interface coupled with powerful optimization.\n\n&nbsp;\n\nTo learn more about Spark SQL, Dataframes, or Apache Spark 1.3, checkout the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">SQL programming guide</a> on the Apache Spark website. Stay tuned to this blog for updates on other components of the <a title=\"Announcing Spark 1.3!\" href=\"https://databricks.com/blog/2015/03/13/announcing-spark-1-3.html\">Apache Spark 1.3</a> release!</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-03-25, 2015-03-25, UTC)</td><td><em>Topic models</em> automatically infer the topics discussed in a collection of documents. These topics can be used to summarize and organize documents, or used for featurization and dimensionality reduction in later stages of a Machine Learning (ML) pipeline.\n\nWith Apache Spark 1.3, MLlib now supports <em>Latent Dirichlet Allocation (LDA)</em>, one of the most successful topic models. LDA is also the first MLlib algorithm built upon GraphX. In this blog post, we provide an overview of LDA and its use cases, and we explain how GraphX was a natural choice for implementation.\n<h2>Topic Models</h2>\nAt a high level, topic modeling aims to find structure within an unstructured collection of documents. After learning this “structure,” a topic model can answer questions such as: What is document X discussing? How similar are documents X and Y? If I am interested in topic Z, which documents should I read first?\n<h2>LDA</h2>\nTopic modeling is a very broad field. Apache Spark 1.3 adds Latent Dirichlet Allocation (LDA), arguably the most successful topic model to date. Initially developed for both text analysis and population genetics, LDA has since been extended and used in many applications from time series to image analysis. First, let’s describe LDA in terms of text analysis.\n\nWhat are topics? LDA is not given topics, so it must infer them from raw text. LDA defines a topic as a distribution over words. For example, when we run MLlib’s LDA on a <a href=\"http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\">dataset of articles from 20 newsgroups</a>, the first few topics are:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/20newsgroups.png\"><img class=\"aligncenter wp-image-3146\" src=\"https://databricks.com/wp-content/uploads/2015/03/20newsgroups.png\" alt=\"20newsgroups\" width=\"675\" height=\"290\" /></a>\n\n&nbsp;\n\nLooking at these highest-weighted words in 3 topics, we can quickly understand what each topic is about: sports, space exploration, and computers.  LDA’s success largely stems from its ability to produce interpretable topics.\n<h2>Use Cases</h2>\nIn addition to inferring these topics, LDA infers a distribution over topics for each document.  E.g., document X might be 60% about “space exploration,” 30% about “computers” and 10% about other topics.\n\nThese topic distributions can be used in many ways:\n<ul>\n \t<li><b>Clustering</b>: Topics are cluster centers and documents are associated with multiple clusters (topics).  This clustering can help organize or summarize document collections.\n<ul>\n \t<li>See <a href=\"http://www.cs.cmu.edu/~lemur/science/\">this generated summary of <i>Science</i> articles</a> from Prof. Blei and Lafferty.  Click on a topic to see a list of articles about that topic.</li>\n</ul>\n</li>\n \t<li><b>Feature generation</b>: LDA can generate features for other ML algorithms to use.  As mentioned above, LDA infers a distribution over topics for each document; with K topics, this gives K numerical features.  These features can then be plugged into algorithms such as Logistic Regression or Decision Trees for prediction tasks.</li>\n \t<li><strong>Dimensionality reduction:</strong> Each document’s distribution over topics gives a concise summary of the document.  Comparing documents in this reduced feature space can be more meaningful than comparing in the original feature space of words.</li>\n</ul>\n<h2>Using LDA in MLlib</h2>\nWe give a short example of using LDA.  We describe the process here and provide the actual code in <a href=\"https://gist.github.com/jkbradley/ab8ae22a8282b2c8ce33\">this Github gist</a>.  Our example first loads and pre-processes documents.  The most important part of preprocessing is choosing a vocabulary.  In our example, we split text into terms (words) and then remove (a) non-alphabetic terms, (b) short terms with &lt; 4 characters, and (c) the most common 20 terms (as “stopwords”).  In general, it is important to adjust this preprocessing for your particular dataset.\n\nWe then run LDA using 10 topics and 10 iterations.  It is often important to choose the number of topics based on your dataset.  Using other options as defaults, we train LDA on the Spark documentation Markdown files (spark/docs/*.md).\n\nWe end up with 10 topics.  Here are 5 hand-picked topics, each with its most important 5 terms.  Note how each corresponds nicely to a component of Spark!  (The quoted topic titles were added by hand to make this clear.)\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Spark-docs.png\"><img class=\"aligncenter size-large wp-image-3149\" src=\"https://databricks.com/wp-content/uploads/2015/03/Spark-docs-1024x351.png\" alt=\"Spark docs\" width=\"1024\" height=\"351\" /></a>\n\nLDA has Scala and Java APIs in Spark 1.3.  The Python API will be added soon.\n<h2>Implementation: GraphX</h2>\nThere are many algorithms for learning an LDA model.  We chose Expectation-Maximization (EM) for its simplicity and fast convergence.  Because EM for LDA has an implicit graphical structure, building LDA on top of GraphX was a natural choice.\n\nLDA has 2 main types of data: terms (words) and documents.  We store this data on a bipartite graph (illustrated below) which has term vertices (left) and document vertices (right).  Each term vertex stores weights indicating which topics that term is relevant to; likewise, each document vertex stores its current estimate of the topics discussed in the document.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/graph.png\"><img class=\"aligncenter wp-image-3150\" src=\"https://databricks.com/wp-content/uploads/2015/03/graph-1024x554.png\" alt=\"graph\" width=\"770\" height=\"417\" /></a>\n\nWhenever a term appears in a document, the graph has an edge between the corresponding term vertex and document vertex.  E.g., in the figure above, Article 1 contains the terms “hockey” and “system.”\n\nThese edges also illustrate the algorithm’s communication.  On each iteration, every vertex updates its data (topic weights) by collecting data from its neighbors.  Below, Article 2 updates its topic estimates by collecting data from connected term vertices.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/update.png\"><img class=\"aligncenter wp-image-3151\" src=\"https://databricks.com/wp-content/uploads/2015/03/update.png\" alt=\"update\" width=\"586\" height=\"294\" /></a>\n\nGraphX was thus a natural choice for LDA.  As MLlib grows, we expect more graph-structured learning algorithms in the future!\n<h2>Scalability</h2>\nParallelization of LDA is not straightforward, and there have been many research papers proposing different strategies.  The key problem is that all methods involve a large amount of communication.  This is evident in the graph description above: terms and documents need to update their neighbors with new data on each iteration, and there are <i>many</i> neighbors.\n\nWe chose the Expectation-Maximization algorithm partly because it converges to a solution in a small number of iterations.  Fewer iterations means less communication.\n\nBefore adding LDA to Spark, we ran tests on a large Wikipedia dataset.  Here are the numbers:\n<ul>\n \t<li>Training set size: 4.6 million documents</li>\n \t<li>Vocabulary size: 1.1 million terms</li>\n \t<li>Training set size: 1.1 billion tokens (~239 words/document)</li>\n \t<li>100 topics</li>\n \t<li>16-worker EC2 cluster</li>\n \t<li>Timing results: 176 sec/iteration on average over 10 iterations</li>\n</ul>\n<h2>What’s Next?</h2>\nSpark contributors are currently developing additional LDA algorithms: online Variational-Bayes (a fast approximate algorithm) and Gibbs sampling (a slower but sometimes more accurate algorithm).  We are also adding helper infrastructure such as Tokenizers for automatic data preparation and more prediction functionality.\n\nTo get started using LDA, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.3</a> today!\n\nTo see examples and learn the API details, check out the <a href=\"http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda\">MLlib documentation</a>.\n<h2>Acknowledgements</h2>\nThe development of LDA has been a collaboration between many Spark contributors:\n\nJoseph K. Bradley, Joseph Gonzalez, David Hall, Guoqiang Li, Xiangrui Meng, Pedro Rodriguez, Avanesov Valeriy, and Xusen Yin.\n<h2>Additional resources</h2>\nLearn more about topic models and LDA with these overviews:\n<ul>\n \t<li>Overview of topic models:  <a href=\"http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf\">D. Blei and J. Lafferty.   “Topic Models.”  In A. Srivastava and M. Sahami, editors, Text Mining: Classification, Clustering, and Applications. Chapman &amp; Hall/CRC Data Mining and Knowledge Discovery Series, 2009.</a></li>\n \t<li><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">Wikipedia on LDA</a>, with mathematical details</li>\n</ul>\nGet in-depth background from these research papers:\n<ul>\n \t<li>Original LDA papers\n<ul>\n \t<li><a href=\"http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf\">Blei, Ng, and Jordan.  \"Latent Dirichlet Allocation.\"  JMLR, 2003.</a>\n<ul>\n \t<li>Application: text document analysis</li>\n</ul>\n</li>\n \t<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/10835412\">Pritchard et al. “Inference of population structure using multilocus genotype data.” Genetics 155: 945--959, 2000.</a>\n<ul>\n \t<li>Application: population genetics analysis</li>\n</ul>\n</li>\n</ul>\n</li>\n \t<li>Paper which clearly explains several algorithms, including EM:  <a href=\"http://arxiv.org/pdf/1205.2662.pdf\">Asuncion, Welling, Smyth, and Teh. \"On Smoothing and Inference for Topic Models.\"  UAI, 2009.</a></li>\n</ul></td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2015-03-30, 2015-03-30, UTC)</td><td><a href=\"https://kafka.apache.org/\">Apache Kafka</a> is rapidly becoming one of the most popular open source stream ingestion platforms. We see the same trend among the users of Spark Streaming as well. Hence, in Apache Spark 1.3, we have focused on making significant improvements to the Kafka integration of Spark Streaming. This has resulted the following additions:\n<ol>\n \t<li>New<b> Direct API</b> for Kafka - This allows each Kafka record to be processed exactly once despite failures, without using <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">Write Ahead Logs</a>. This makes Spark Streaming + Kafka pipelines more efficient while providing stronger fault-tolerance guarantees.</li>\n \t<li><b>Python API</b> for Kafka - So that you can start processing Kafka data purely from Python.</li>\n</ol>\nIn this article, we are going to discuss these improvements in more detail.\n<h2>Direct API for Kafka</h2>\n<i>[Primary Contributor - Cody]</i>\n\nSpark Streaming has supported Kafka since its inception, and Spark Streaming has been used with Kafka in production at many places (see <a href=\"http://www.slideshare.net/databricks/spark-streaming-state-of-the-union-strata-san-jose-2015\">this</a> talk). However, the Spark community has demanded better fault-tolerance guarantees and stronger reliability semantics overtime.  To meet this demand, Spark 1.2 introduced <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">Write Ahead Logs (WAL)</a>. It ensures that no data received from any reliable data sources (i.e., transactional sources like Flume, Kafka, and Kinesis) will be lost due to failures (i.e., at-least-once semantics). Even for unreliable (i.e. non-transactional) sources like plain old sockets, it minimizes data loss.\n\nHowever, for sources that allow replaying of data streams from arbitrary positions in the streams (e.g. Kafka), we can achieve even stronger fault-tolerance semantics because these sources let Spark Streaming have more control on the consumption of the data stream. Spark 1.3 introduces the concept of a <b>Direct API,</b> which can achieve exactly-once semantics even without using Write Ahead Logs. Let’s look at the details of Spark’s direct API for Apache Kafka.\n<h2>How did we build it?</h2>\nAt a high-level, the earlier Kafka integration worked with Write Ahead Logs (WALs) as follows:\n<ol>\n \t<li>The Kafka data is continuously received by Kafka Receivers running in the Spark workers/executors. This used the high-level consumer API of Kafka.</li>\n \t<li>The received data is stored in Spark’s worker/executor memory as well as to the WAL (replicated on HDFS). The Kafka Receiver updated Kafka’s offsets to Zookeeper only after the data has been persisted to the log.</li>\n \t<li>The information about the received data and its WAL locations is also stored reliably. On failure, this information is used to re-read the data and continue processing.</li>\n</ol>\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.11.42-PM.png\"><img class=\"aligncenter wp-image-3174\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.11.42-PM-300x250.png\" alt=\"Streaming blog figure 1\" width=\"500\" height=\"417\" /></a>\n\n&nbsp;\n\nWhile this approach ensures that no data from Kafka is lost, there is still a small chance that some records may get processed more than once due to failures (that is, at-least-once semantics). This can occur when some received data is saved reliably to WAL but the system fails before updating the corresponding Kafka offsets in Zookeeper. This leads to an inconsistency - Spark Streaming considers that data to have been received, but Kafka considers that the data was not successfully sent as the offset in Zookeeper was not updated. Hence, Kafka will send the data again after the system recovers from the failure.\n\nThis inconsistency arises because the two systems cannot be atomically updated with the information that describes what has already been sent. To avoid this, only one system needs to maintain a consistent view of what has been sent or received. Additionally, that system needs to have complete control over the replay of the data stream during the recovery from failures. Therefore, we decided to keep all the consumed offset information <i>only</i> in Spark Streaming, which can use Kafka’s <a href=\"http://kafka.apache.org/documentation.html#simpleconsumerapi\">Simple Consumer API</a> to replay data from arbitrary offsets as required due to failures.\n\nTo build this (primary contributor was Cody), the new Direct Kafka API takes a completely different approach from Receivers and WALs. Instead of receiving the data continuously using Receivers and storing it in a WAL, we simply decide at the beginning of every batch interval what is the range of offsets to consume. Later, when each batch’s jobs are executed, the data corresponding to the offset ranges is read from Kafka for processing (similar to how HDFS files are read). These offsets are also saved reliably (with <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing\">checkpoints</a>) and used to recompute the data to recover from failures.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.14.11-PM.png\"><img class=\"aligncenter wp-image-3177\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-10.14.11-PM-300x206.png\" alt=\"streaming blog figure 2\" width=\"500\" height=\"344\" /></a>\n\nNote that Spark Streaming can reread and reprocess segments of the stream from Kafka to recover from failures. However, due to the exactly-once nature of RDD transformations, the final recomputed results are exactly same as that would have been without failures.\n\nThus this direct API eliminates the need for both WALs and Receivers for Kafka, while ensuring that each Kafka record is effectively received by Spark Streaming exactly once. This allows one to build a Spark Streaming + Kafka pipelines with end-to-end exactly-once semantics (if your updates to downstream systems are idempotent or transactional). Overall, it makes such streaming processing pipelines more fault-tolerant, efficient, and easier to use.\n<h2>How to use it?</h2>\nThe new API is simpler to use than the previous one.\n\n[scala]\n// Define the Kafka parameters, broker list must be specified\nval kafkaParams = Map(&quot;metadata.broker.list&quot; -&gt; &quot;localhost:9092,anotherhost:9092&quot;)\n\n// Define which topics to read from\nval topics = Set(&quot;sometopic&quot;, &quot;anothertopic&quot;)\n\n// Create the direct stream with the Kafka parameters and topics\nval kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](streamingContext, kafkaParams, topics)\n[/scala]\n\nSince this direct approach does not have any receivers, you do not have to worry about creating multiple input DStreams to create more receivers. Nor do you have to configure the number of Kafka partitions to be consumed per receiver. Each Kafka partition will be automatically read in parallel.  Furthermore, each Kafka partition will correspond to a RDD partition, thus simplifying the parallelism model.\n\nIn addition to the new streaming API, we have also introduced <code>KafkaUtils.createRDD(),</code> which can be used to run batch jobs on Kafka data.\n\n[scala]\n// Define the offset ranges to read in the batch job\nval offsetRanges = Array(\n  OffsetRange(&quot;some-topic&quot;, 0, 110, 220),\n  OffsetRange(&quot;some-topic&quot;, 1, 100, 313),\n  OffsetRange(&quot;another-topic&quot;, 0, 456, 789)\n)\n\n// Create the RDD based on the offset ranges\nval rdd = KafkaUtils.createRDD[String, String, StringDecoder, StringDecoder](sparkContext, kafkaParams, offsetRanges)\n[/scala]\n\nIf you want to learn more about the API and the details of how it was implemented, take a look at the following.\n<ul>\n \t<li><a href=\"http://spark.apache.org/docs/latest/streaming-kafka-integration.html\">Spark Streaming + Kafka Integration Guide</a></li>\n \t<li>Cody’s <a href=\"https://github.com/koeninger/kafka-exactly-once/blob/master/blogpost.md\">blog post</a> with more details</li>\n \t<li>Full word count example of the Direct API in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\">Scala</a> and <a href=\"https://github.com/apache/spark/blob/master/examples/scala-2.10/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\">Java</a></li>\n \t<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$\">Scala</a> and <a href=\"http://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/kafka/KafkaUtils.html\">Java</a> documentation of the Direct API</li>\n \t<li>Updated <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-semantics\">Fault-tolerance Semantics in Spark Streaming Programming Guide</a></li>\n</ul>\n<h2>Python API for Kafka</h2>\n<i>[Primary Contributor - Davies]</i>\n\nIn Spark 1.2, the basic Python API of Spark Streaming was added so that developers could write distributed stream processing applications purely in Python. In Spark 1.3, we have extended the Python API to include Kafka (primarily contributed by Davies Liu). With this, writing stream processing applications in Python with Kafka becomes a breeze. Here is a sample code.\n\n[python]\nkafkaStream = KafkaUtils.createStream(streamingContext, \n&quot;zookeeper-server:2181&quot;, &quot;consumer-group&quot;, {&quot;some-topic&quot;: 1})\n\nlines = kafkaStream.map(lambda x: x[1])\n[/python]\n\nSee the full <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/kafka_wordcount.py\">example</a> and the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark-streaming-kafka-module\">python docs</a>. Instructions to run the example can be found in the Kafka integration guide. Note that for running the example or any python applications using the Kafka API, you will have to add the Kafka Maven dependencies to the path. This is can be easily done in Spark 1.3 as you can directly add Maven dependencies to spark-submit (recommended way to launch Spark applications). See the “Deploying” section in the <a href=\"http://spark.apache.org/docs/latest/streaming-kafka-integration.html\">Kafka integration guide</a> for more details.\n\nAlso note that this is using the earlier Kafka API. Extending Python to the Direct API is <a href=\"https://issues.apache.org/jira/browse/SPARK-5946\">in progress</a> and expected to be available in Spark 1.4. Additionally, we want to add Python APIs for the rest of the built-in sources to the to achieve parity between the Scala, Java and Python streaming APIs.\n<h2>Future Directions</h2>\nWe will continuously improve the stability and performance of Kafka integration. Some of the improvements we intend to make are as follows:\n<ul>\n \t<li>Automatically updating Zookeeper as batches complete successfully, to make Zookeeper based Kafka monitoring tools work - <a href=\"https://issues.apache.org/jira/browse/SPARK-6051\">SPARK-6051</a></li>\n \t<li>Python API for the Direct API - <a href=\"https://issues.apache.org/jira/browse/SPARK-5946\">SPARK-5946</a></li>\n \t<li>Extending this direct API approach to Kinesis - <a href=\"https://issues.apache.org/jira/browse/SPARK-6599\">SPARK-6599</a></li>\n \t<li>Connection pooling for Kafka Simple Consumer API across batches</li>\n</ul>\n&nbsp;</td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-04-02, 2015-04-02, UTC)</td><td>We built Databricks on top of the Apache Spark framework to make data analysis simple. In the same spirit, we want to make the adoption and usage of Databricks simple. Developers and data scientists need to answer questions about Databricks and Spark quickly in order to get their work done, while project leads need to minimize the time their teams spend on ramping up on a new platform. In this blog post, we present a new feature in Databricks called <i>Integrated Search</i> that helps our users find relevant information quickly and easily.\n<h2>What is <i>Integrated Search</i>?</h2>\nThe <i>integrated search</i> feature helps users find relevant information about Databricks and Apache Spark from three powerful resources:\n<ul>\n \t<li><b>Databricks Guide: </b>An extensive guide with tutorials, sample applications, reference code, and documentation built into Databricks</li>\n \t<li><b><b>Databricks Forum:</b> </b>An <a href=\"https://forums.databricks.com/\">online forum</a> where customers can ask questions and find answers provided by our expert field engineers and developers who work on Spark, as well as other customers</li>\n \t<li><b>Databricks Workspace:</b> The workspace contains code previously written by you and other users of your Databricks instance, a valuable resource for finding code examples where your team may have solved a similar problem before</li>\n</ul>\nFor questions about how to use Databricks features and Apache Spark, <i>Integrated Search</i> provides a simple interface that allows users to quickly find answers without leaving Databricks and interrupting their workflow. In the example below, one can easily locate example applications of processing Parquet files in Databricks Guide, or find more in-depth discussions about how to use Parquet in the Databricks forum:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.06.18-AM.png\"><img class=\"aligncenter wp-image-3191\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.06.18-AM.png\" alt=\"Integrated search blog figure 1\" width=\"400\" height=\"365\" /></a>\n\nThere are also times when users want to find code examples written by others in their team, or they may simply want to locate some code they wrote in the past. For this scenario, <i>Integrated Search</i> can search through code and results in <i>Notebooks</i>, which are interactive documents users create in the Databricks Workspace. In the example below, users can find examples of code where their team used Spark broadcast variables, so they can quickly determine how to do so in their own code:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.07.17-AM.png\"><img class=\"aligncenter wp-image-3192\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-30-at-11.07.17-AM-1024x694.png\" alt=\"Integrated search blog figure 2\" width=\"400\" height=\"271\" /></a>\n<h2>Benefits for Databricks users</h2>\nRather than wasting time crawling irrelevant or out-of-date help sources, and switching between multiple websites to search code, product documentation, and forums, <i>Integrated Search</i> provides a one-stop-shop for getting answers about the nuts-and-bolts of creating Spark applications using Databricks. We hope this feature will help our customers master Apache Spark more quickly, so they can spend their valuable time developing exciting applications in Databricks.\n\nTo learn more about Databricks, check out our <a href=\"https://databricks.com/product/databricks-cloud\">products page</a> or the <a href=\"https://www.youtube.com/watch?v=IBwYINZdQgQ\">overview video</a>\n\nIf you’re interested in trying out Databricks, <a href=\"https://databricks.com/registration\">sign up for a trial</a>!\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-03-31, 2015-03-31, UTC)</td><td>Today, we’re celebrating an important milestone for the Apache Spark project -- it’s now been five years since Spark was <a href=\"https://github.com/apache/spark/commit/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9\">first open sourced</a>. When we first decided to release our research code at UC Berkeley, none of us knew how far Spark would make it, but we believed we had built some really neat technology that we wanted to share with the world. In the five years since, we’ve been simply awed by the numerous contributors and users that have made Spark the leading-edge computing framework it is today. Indeed, to our knowledge, Spark has now become the most active open source project in big data (looking at either contributors per month or commits per month). In addition to contributors, it has built up an array of <a href=\"https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html\">hundreds of production use cases</a> from batch analytics to stream processing.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-31-at-7.51.18-AM.png\"><img class=\"aligncenter wp-image-3221\" src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-31-at-7.51.18-AM-1024x411.png\" alt=\"Screen Shot 2015-03-31 at 7.51.18 AM\" width=\"600\" height=\"241\" /></a>\n\nTo celebrate Spark’s fifth birthday, one thing I wanted to do was to highlight some of the key ideas behind how we built out the project that still apply today. To do this, I took another look at the <a href=\"https://github.com/apache/spark/tree/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9\">first public version of Spark</a>.\n\nThe first thing to notice is that this version was quite small: it weighed in at 3900 lines of code, of which 1300 were the Scala interpreter, 600 were examples and 300 were tests. Since March 2010, I’m happy to say that our test coverage has gone up substantially. However, the observation about size does reflect something important: since the beginning, we’ve sought to keep the Spark engine small and compact, making it easier for many developers to understand and for us to change and improve. Even today, the core Spark engine is only about 50,000 lines of code. The main additions since that first version have been support for “shuffle” operations, which required new networking code and a DAG scheduler, as well as support for multiple backend schedulers, such as YARN. Nonetheless, even today we can regularly make large changes to the core engine that improve the performance or stability of all Spark applications. For example, during our work last year on <a href=\"https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html\">large-scale sorting</a>, multiple developers at Databricks ended up rewriting almost all of Spark’s networking layer.\n\nThe second thing to notice about Spark from 2010 is what it can do: even this ~2000 line engine could handle two of the most important workloads for Spark today, iterative algorithms and interactive queries. Back in 2010, we were the only cluster computing engine to support interactive use, by modifying the Scala interpreter to submit code to a Spark cluster. We’ve constantly sought to improve this experience and enable truly interactive data science through features like Spark’s <a href=\"http://spark.apache.org/examples.html\">Python API</a> and <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrames</a>. In addition, even the 2010 version of Spark was able to run iterative algorithms like <a href=\"https://github.com/apache/spark/blob/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9/src/examples/SparkHdfsLR.scala\">logistic regression</a> 20-30x faster than MapReduce (subsequent improvements brought this up to 100x).\n\nA final important element in how we think about the project is our focus on simple, stable APIs. The code examples that ship with Spark from 2010, like <a href=\"https://github.com/apache/spark/blob/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9/src/examples/SparkHdfsLR.scala\">logistic regression</a> and <a href=\"https://github.com/apache/spark/blob/df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9/src/examples/SparkPi.scala\">computing pi</a>, are nearly identical to Spark code from today (see <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\">logistic regression</a>, <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\">pi</a>). We work very hard to define stable APIs that developers can build on years into the future, minimizing the work they must do to keep up with improvements in Spark. Starting in Apache Spark 1.0, these compatibility guarantees are now <a href=\"https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy\">formalized</a> for all major Spark components.\n\nThat’s enough about Spark in 2010. How has the project grown since then? While there has been tremendous activity in all areas of Spark, including support for more programming languages (Java, Python and soon R), data sources, and optimizations, the single biggest addition to Spark has been its standard libraries. Over the years, Spark has acquired four high-level libraries -- <a href=\"http://spark.apache.org/streaming\">Spark Streaming</a>, <a href=\"http://spark.apache.org/mllib\">MLlib</a>, <a href=\"http://spark.apache.org/graphx\">GraphX</a> and <a href=\"http://spark.apache.org/sql\">Spark SQL</a> -- that all run on top of the core engine, and interoperate easily and efficiently with each other. Today these libraries are the bulk of the code in Spark -- about 200,000 lines compared to 50,000 in the core engine. They also represent the single largest standard library available for big data, making it easy to write applications that span all stages of the data lifecycle. Nevertheless, these libraries are still quite new, the majority of them having been added in the last two years. In future years I expect these libraries to grow significantly, with the aim to build as rich a toolset for big data as the libraries available for small data. You can find some of the areas where Databricks is working on these libraries in my <a href=\"http://spark-summit.org/east/2015/talk/matei-zaharia\">slides from Spark Summit 2015</a>.\n\nFinally, like any five-year-old, Spark is still sometimes able to get into trouble without supervision and sometimes hard to understand. At Databricks, we’re working hard to make Spark easier to use and run than ever, through our efforts on both the Spark codebase and support materials around it. All of our work on Spark is open source and goes directly to Apache. In addition, we have put up a large array of free <a href=\"https://databricks.com/spark/developer-resources\">online training materials</a>, as well as <a href=\"https://databricks.com/spark/training\">training courses</a> and <a href=\"http://shop.oreilly.com/product/0636920028512.do\">books</a>. Finally, we have built a service to make it very easy to run Spark in a few clicks, <a href=\"https://databricks.com/product/databricks-cloud\">Databricks Cloud</a>. We hope that you enjoy using Spark, no matter which environment you run it in, as much as we enjoy building it.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-04-01, 2015-04-01, UTC)</td><td>Yesterday, to celebrate Apache Spark’s <a href=\"https://databricks.com/blog/2015/03/31/spark-turns-five-years-old.html\">5 year old</a> birthday, we looked back at the history of the project. Today, we are happy to announce the next major chapter of Spark development: an architectural overhaul designed to enable Spark on mobile devices. Mobile computing is quickly rising to dominance, and by the end of 2017, it is estimated that 90% of CPU cycles will be devoted to mobile hardware. Spark’s project goal can be accomplished only when Spark runs efficiently for the growing population of mobile users. Already today, 100% of Spark’s users have mobile phones.\n\nDesigned and optimized for modern data centers and Big Data applications, Spark is unfortunately not a good fit for mobile computing today. In the past few months, we have been prototyping the feasibility of a mobile-first Spark architecture, and today we would like to share with you our findings. This post outlines the technical design of Spark’s mobile support, and shares results from several early prototypes. See also <a href=\"https://issues.apache.org/jira/browse/SPARK-6646\">SPARK-6646</a> for community discussion on the issue.\n\n<center><img src=\"https://databricks.com/wp-content/uploads/2015/03/image00.png\" alt=\"Spark on iPhone\" width=\"300\" /></center><strong>Requirements</strong>\n\nMust Have:\n\n<ul>\n    <li>Support running Spark on Android and iOS</li>\n    <li>Facilitate the development of SoLoMo (social, local, mobile) applications</li>\n    <li>Maintain source backward compatibility</li>\n    <li>Support heterogeneous mobile phones within a single Spark cluster</li>\n</ul>\n\nNice to Have:\n\n<ul>\n    <li>Support running Spark on Windows phones</li>\n    <li>Support feature phones through J2ME</li>\n</ul>\n\n<h2>Compilation and Runtime</h2>\n\nThe Android Runtime (ART) already supports developing applications using Scala, as evidenced by this <a href=\"http://scala-ide.org/docs/tutorials/play/index.html\">Scala IDE article on Android development</a>.\n\niOS, however, has no built-in JVM support. Luckily, we can leverage multiple community efforts here:\n\n<ol>\n    <li>Use <a href=\"http://robovm.com/\">RoboVM</a>, an ahead-of-time (AOT) compiler and library for developing iOS applications using Java.</li>\n    <li>Use <a href=\"http://greedy.github.io/scala-llvm/\">Scala+LLVM</a> to compile Spark’s Scala code into LLVM bytecode. Given Apple’s strong stand behind the LLVM project, we believe this can achieve the highest level of performance.</li>\n    <li>Since <a href=\"https://leverich.github.io/swiftislikescala/\">Swift looks very similar to Scala</a>, we can create a project to automatically translate Scala source code into Swift, and then build the iOS package using XCode.</li>\n    <li>Use <a href=\"http://www.scala-js.org/\">Scala.js</a> to compile Spark into JavaScript, and execute Spark in the Nitro JavaScript engine (Safari).</li>\n</ol>\n\nOption 4 is preferred because not only does it support iOS, but also all platforms using a single package. On iOS and Android, Spark can be executed by the JavaScript engines. On servers, Spark can be executed in Node.js.\n\n<h2>Performance Optimizations</h2>\n\nJavaScript engines are one of the hottest areas of innovation, and thus we fully expect JavaScript engines to improve their performance rapidly. However, mobile JavaScript engines seem to lag behind their desktop variants. In particular, there appears to be no SIMD support for mobile JavaScript.\n\nFor parts of Spark that might benefit tremendously from SIMD, such as low level matrix operations, we can selectively rewrite those parts to generate LLVM bytecode.\n\n<h2>Networking and Wire Protocol</h2>\n\nSpark’s network transport builds on Netty, which in turn relies on java.nio or Linux epoll. Android ART appears to support java.nio out of the box, but we might need to rewrite Netty to use kqueue on iOS. Additionally, it is unclear whether low-level networking primitives such as zero-copy can be exposed in JavaScript. We will need to work more closely with Apple and Google to improve networking support in mobile JavaScript.\n\nA viable alternative is to leverage <a href=\"http://www.grpc.io/\">grpc</a>, an open-source high performance RPC library developed by Google. grpc provides out of the box support for all common platforms (Java, Objective C, etc) using HTTP/2.\n\nGiven the focus on debuggability, JSON should be the preferred wire serialization protocol over any existing binary formats.\n\n<h2>True Locality Scheduling and DAGScheduler</h2>\n\nTo better support local, social, and mobile features of Spark, we change the locality field of RDDs to GPS coordinates, and locality scheduling can be refactored to support true locality that was never possible on servers.\n\nIn order to maintain source compatibility, we keep the old interface, and introduce a new true locality interface.\n\n[scala]\nclass RDD {\n  @deprecate(&quot;2.0&quot;, &quot;use getPreferredTrueLocations&quot;)\n  def getPreferredLocations(p: Partition): Seq[String]\n\n/**\n   * Returns the preferred locations for executing task on partition\n   * <code>p</code>. Concrete implementations of RDD can use this to enable\n   * locality scheduling.\n   */\n  def getPreferredTrueLocations(p: Partition): Seq[LatLong]\n}\n[/scala]\n\nThe DAGScheduler needs to be updated to compute the geographical proximity of executors.\n\n<h2>Extending TaskContext to the Mobile Platform</h2>\n\nThe TaskContext provides the contextual information for Spark tasks (e.g., job IDs, attempt IDs, etc.). While this was sufficient for running on server, the mobile platform has many other contextual information like GPS location, ongoing calls, etc. Such contextual information may be useful to optimize the processing of tasks without affecting the user experience of the smartphone/tablet user. For example, if a phone call is active, a new task may be paused until the call is over so that the call quality is not affected.\n\n<h2>Basic Engine on iPhone and Androids</h2>\n\nWe have already built a few proof-of-concepts using iPhones to better understand the complexities of the mobile platform. Here are some screenshots from our prototype.\n\nThe screenshot at the beginning of the blog post shows a Spark Streaming NetworkWordCount example running on an iPhone. It is receiving data using sockets from a server running on Amazon EC2. We are also prototyping it on Android. Here is some early screenshots on the Android device emulator.\n\n<center><img src=\"https://databricks.com/wp-content/uploads/2015/03/image01-808x1024.png\" alt=\"Spark on Android\" width=\"200\" /></center>\n\n<h2>Prototyping Machine Learning Application</h2>\n\nIn real-world machine learning, labels are always hard and expensive to obtain. This won’t be an issue with Spark on Mobile. In the pipeline API for machine learning, we will provide a transformer that produces human tags.\n\n[scala]\ntagger = HumanTagger(tags=[“spam”, “ham”], retries=10)\nlabeled = tagger.transform(images)\nmodel = LogisticRegression().fit(labeled)\n[/scala]\n\nDuring transformation, images are displayed along with possible tags and users choose the correct tag for each image. To make sure the transformation is fault tolerant, we will randomize the ordering and retry 10 times for each record. Yes, RDDs are always deterministic. And of course, we support tagging with multi-labels, even on a wearable device:\n\n<center><img src=\"https://databricks.com/wp-content/uploads/2015/03/Screen-Shot-2015-03-31-at-11.37.26-PM.png\" width=\"400\" /></center>In conclusion, this design doc proposes architectural changes to Spark to allow efficient execution on mobile platforms. Whether it is an octa-core x86 processor or a quad-core ARM processor, Spark should be the one unifying computing platform. We look forward to collaborating with the community to realize this effort, and hearing your feedback on the JIRA ticket: <a href=\"https://issues.apache.org/jira/browse/SPARK-6646\">SPARK-6646</a>.\n\n[<em>Just in case you didn't realize - it's April 1st!</em>]</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Customers)</td><td>List(2015-04-03, 2015-04-03, UTC)</td><td>We are thrilled to announce that Timeful chose Databricks to enable intelligent time management with data analytics.\n\nPress release: <a href=\"http://www.marketwired.com/press-release/timeful-chooses-databricks-cloud-enable-intelligent-time-management-with-data-2006609.htm\" target=\"_blank\">http://www.marketwired.com/press-release/timeful-chooses-databricks-cloud-enable-intelligent-time-management-with-data-2006609.htm</a>\n\nTimeful helps its users manage their time better by tracking commitments, categorizing to-do list items and assisting in the development of good lifestyle habits. Deployed as an application on smart phones devices, Timeful utilizes machine learning to recommend a personalized scheduled based on previous behavior, availability, and preferences.\n\nThe Timeful team relies heavily on data analytics to improve product design and monitor the personalized schedule recommendations. Both of these data analytics tasks proved to be challenging in Timeful’s environment because Timeful stores its data in multiple Postgres databases, which proved to be too slow and cumbersome for Timeful's needs.\n\nUtilizing Databricks as a centralized, high-performance data processing platform, Timeful was able to overcome the limitations imposed by multiple Postgres databases. The speed and simplicity of Databricks enabled the continuous monitoring of Timeful's production systems while providing an easy way to access and explore terabyte scale production data for non-engineers.\n\nAs a result of deploying Databricks, Timeful gained a number of benefits:\n<ul>\n \t<li>Improved key metrics monitoring by processing the entire production data set instead of sampling subsets</li>\n \t<li>More effective data-driven product design in a much shorter cycle</li>\n \t<li>Redirect one FTE in data analytics to focus on problem solving instead of data analysis for other teams</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/resources/customer-case-studies\" target=\"_blank\">case study</a> learn more about how Timeful is using Databricks.</td></tr><tr><td>null</td><td>List(Company Blog, Events)</td><td>List(2015-04-08, 2015-04-08, UTC)</td><td><a href=\"https://databricks.com/wp-content/uploads/2015/04/Spark-Summit.jpg\"><img class=\"aligncenter wp-image-3364\" src=\"https://databricks.com/wp-content/uploads/2015/04/Spark-Summit-1024x683.jpg\" alt=\"Spark summit keynote\" width=\"600\" height=\"400\" /></a>\n\nWe are delighted about the success of the first <a href=\"http://spark-summit.org/east/2015\">Spark Summit East</a>, held in New York City on March 18th. The summit was attended by a sold-out crowd of over 900 people from more than 300 organizations.\n\nDatabricks is proud to make all talk videos, slides, training talk videos, and training materials available online for free as a service to the Apache Spark community. Slides are already available on the <a href=\"http://spark-summit.org/east/2015\">Spark Summit East agenda page</a> and videos will be published there too as soon as we finish editing them.\n<h2>Keynotes</h2>\nMatei Zaharia, the creator of Spark, opened the summit with a talk titled <a href=\"http://www.slideshare.net/databricks/new-direction-for-spark-in-2015-spark-summit-east\">New Directions for Spark in 2015</a>. In it he presented work on higher level interfaces for machine learning, work enabling new external data sources to connect to Spark, and more.\n\nDuring the next keynote Ion Stoica, CEO of Databricks, highlighted two companies that are using Spark in Databricks Cloud to create innovative data products. MyFitnessPal is utilizing Spark in a variety of ways to understanding the behavior of their customers. Automatic Labs is using Spark to analyze data collected from sensors in vehicles.\n\nNext Brian Schimpf who is Director of Engineering at Palantir spoke about some of the data challenges Palantir faces related to trader oversight in the financial industry and the technology they’ve built in response with Spark at the core.\n\nIn the final two keynotes Matthew Glickman, managing director at Goldman Sachs, spoke of Spark as the “lingua franca” platform for scalable Big Data computation and Peter Wang from Continuum, a creator of the PyData Conference, spoke about the advantages of using Python and Spark together.\n<h2>Community Talks</h2>\nBeyond the keynotes, the summit showcased dozens of community talks across three parallel tracks covering a broad array of use cases from startups and large enterprises. Gauged by the diversity of talk topics, Spark is being used in a wide range of industries and organizations. Some highlights include:\n<ul>\n \t<li><b>Finance:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-24-Levans-Kuipers.pdf\">Tresata is revolutionizing the detection of money laundering</a></li>\n \t<li><b>Retail:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-27-ZacharyCohn.pdf\">Gilt</a> and <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-39-Solmaz-Shahalizadeh.pdf\">Shopify</a> are using Spark to improve overall customer experience through better personalization</li>\n \t<li><b>Media: </b><a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-18-Neumann-Alla.pdf\">Comcast is offering real-time recommendation to their viewers</a></li>\n \t<li><b>Health and wellness:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-36-Hesamoddin-Salehian.pdf\">MyFitnessPal is providing more accurate food information to its 80 million users</a></li>\n \t<li><b>Pharma:</b> <a href=\"http://spark-summit.org/wp-content/uploads/2015/03/SSE15-30-David-Tester.pdf\">Norvartis is accelerating Genomics research with Spark</a></li>\n</ul>\n<h2>Training</h2>\nThe day following the Summit we trained over 500 students to use Spark in three parallel classes. You can download the course material for free via the links below:\n<ul>\n \t<li><a href=\"http://training.databricks.com/workshop/sparkcamp.pdf\">Intro to Apache Spark</a></li>\n \t<li><a href=\"http://training.databricks.com/workshop/datasci.pdf\">Data Science of Apache Spark</a></li>\n \t<li><a href=\"http://www.slideshare.net/databricks/spark-summit-east-2015-advdevopsstudentslides\">DevOps with Apache Spark Workshop</a></li>\n</ul>\nLearn more about Spark training classes run by Databricks on the <a href=\"https://databricks.com/services/spark-training\">training portion</a> of our website.\n<h2>Learn More</h2>\nFor Spark enthusiasts on the West Coast, the next <a href=\"http://spark-summit.org/2015\">Spark Summit</a> will be in San Francisco from June 15th to 17th. <a href=\"http://prevalentdesignevents.com/sparksummit2015/registration.aspx\">Register now</a> before it sells out.\n\nTo keep up with Spark and Databricks news, sign up for <a href=\"https://databricks.com/resources/newsletters\">our monthly newsletter</a>.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-04-13, 2015-04-13, UTC)</td><td>Spark SQL is one of the newest and most technically involved components of Spark. It powers both SQL queries and the new <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">DataFrame API</a>. At the core of Spark SQL is the Catalyst optimizer, which leverages advanced programming language features (e.g. Scala's <a href=\"http://docs.scala-lang.org/tutorials/tour/pattern-matching.html\">pattern matching</a> and <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\">quasiquotes</a>) in a novel way to build an extensible query optimizer.\n\nWe recently published a <a href=\"http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf\">paper</a> on Spark SQL that will appear in <a href=\"http://www.sigmod2015.org/\">SIGMOD 2015</a> (co-authored with Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, and Ali Ghodsi). In this blog post we are republishing a section in the paper that explains the internals of the Catalyst optimizer for broader consumption.\n\nTo implement Spark SQL, we designed a new extensible optimizer, Catalyst, based on functional programming constructs in Scala. Catalyst’s extensible design had two purposes. First, we wanted to make it easy to add new optimization techniques and features to Spark SQL, especially for the purpose of tackling various problems we were seeing with big data (e.g., semistructured data and advanced analytics). Second, we wanted to enable external developers to extend the optimizer -- for example, by adding data source specific rules that can push filtering or aggregation into external storage systems, or support for new data types. Catalyst supports both rule-based and cost-based optimization.\n\nAt its core, Catalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, we have built libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode. For the latter, we use another Scala feature, <a href=\"http://docs.scala-lang.org/overviews/quasiquotes/intro.html\">quasiquotes</a>, that makes it easy to generate code at runtime from composable expressions. Finally, Catalyst offers several public extension points, including external data sources and user-defined types.\n<h2>Trees</h2>\nThe main data type in Catalyst is a tree composed of node objects. Each node has a node type and zero or more children. New node types are defined in Scala as subclasses of the TreeNode class. These objects are immutable and can be manipulated using functional transformations, as discussed in the next subsection.\n\nAs a simple example, suppose we have the following three node classes for a very simple expression language:\n<ul>\n\t<li><code>Literal(value: Int)</code>: a constant value</li>\n\t<li><code>Attribute(name: String):</code> an attribute from an input row, e.g.,“x”</li>\n\t<li><code>Add(left: TreeNode, right: TreeNode):</code> sum of two expressions.</li>\n</ul>\nThese classes can be used to build up trees; for example, the tree for the expression <code>x+(1+2)</code>, would be represented in Scala code as follows:\n\n[scala]\nAdd(Attribute(x), Add(Literal(1), Literal(2)))\n[/scala]\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.11.11-AM.png\"><img class=\"aligncenter wp-image-3382 size-medium\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.11.11-AM-300x174.png\" alt=\"Catalyst blog figure 1\" width=\"300\" height=\"174\" /></a>\n\n&nbsp;\n<h2>Rules</h2>\nTrees can be manipulated using rules, which are functions from a tree to another tree. While a rule can run arbitrary code on its input tree (given that this tree is just a Scala object), the most common approach is to use a set of pattern matching functions that find and replace subtrees with a specific structure.\n\nPattern matching is a feature of many functional languages that allows extracting values from potentially nested structures of algebraic data types. In Catalyst, trees offer a transform method that applies a pattern matching function recursively on all nodes of the tree, transforming the ones that match each pattern to a result. For example, we could implement a rule that folds Add operations between constants as follows:\n\n[scala]\ntree.transform {\n  case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2)\n}\n[/scala]\n\nApplying this to the tree for <code>x+(1+2)</code> would yield the new tree <code>x+3</code>. The <code>case</code> keyword here is Scala’s standard pattern matching syntax, and can be used to match on the type of an object as well as give names to extracted values (<code>c1</code> and <code>c2</code> here).\n\nThe pattern matching expression that is passed to transform is a partial function, meaning that it only needs to match to a subset of all possible input trees. Catalyst will tests which parts of a tree a given rule applies to, automatically skipping over and descending into subtrees that do not match. This ability means that rules only need to reason about the trees where a given optimization applies and not those that do not match. Thus, rules do not need to be modified as new types of operators are added to the system.\n\nRules (and Scala pattern matching in general) can match multiple patterns in the same transform call, making it very concise to implement multiple transformations at once:\n\n[scala]\ntree.transform {\n  case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2)\n  case Add(left, Literal(0)) =&gt; left\n  case Add(Literal(0), right) =&gt; right\n}\n[/scala]\n\nIn practice, rules may need to execute multiple times to fully transform a tree. Catalyst groups rules into batches, and executes each batch until it reaches a fixed point, that is, until the tree stops changing after applying its rules. Running rules to fixed point means that each rule can be simple and self-contained, and yet still eventually have larger global effects on a tree. In the example above, repeated application would constant-fold larger trees, such as <code>(x+0)+(3+3)</code>. As another example, a first batch might analyze an expression to assign types to all of the attributes, while a second batch might use these types to do constant folding. After each batch, developers can also run sanity checks on the new tree (e.g., to see that all attributes were assigned types), often also written via recursive matching.\n\nFinally, rule conditions and their bodies can contain arbitrary Scala code. This gives Catalyst more power than domain specific languages for optimizers, while keeping it concise for simple rules.\n\nIn our experience, functional transformations on immutable trees make the whole optimizer very easy to reason about and debug. They also enable parallelization in the optimizer, although we do not yet exploit this.\n<h2>Using Catalyst in Spark SQL</h2>\n<strong>\n</strong>We use Catalyst’s general tree transformation framework in four phases, as shown below: (1) analyzing a logical plan to resolve references, (2) logical plan optimization, (3) physical planning, and (4) code generation to compile parts of the query to Java bytecode. In the physical planning phase, Catalyst may generate multiple plans and compare them based on cost. All other phases are purely rule-based. Each phase uses different types of tree nodes; Catalyst includes libraries of nodes for expressions, data types, and logical and physical operators. We now describe each of these phases.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.41.26-AM.png\"><img class=\"aligncenter wp-image-3385\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.41.26-AM-1024x235.png\" alt=\"Catalyst blog figure 2\" width=\"800\" height=\"184\" /></a>\n<h2>Analysis</h2>\nSpark SQL begins with a relation to be computed, either from an abstract syntax tree (AST) returned by a SQL parser, or from a DataFrame object constructed using the API. In both cases, the relation may contain unresolved attribute references or relations: for example, in the SQL query <code>SELECT col FROM sales</code>, the type of col, or even whether it is a valid column name, is not known until we look up the table sales. An attribute is called unresolved if we do not know its type or have not matched it to an input table (or an alias). Spark SQL uses Catalyst rules and a Catalog object that tracks the tables in all data sources to resolve these attributes. It starts by building an “unresolved logical plan” tree with unbound attributes and data types, then applies rules that do the following:\n<ul>\n\t<li>Looking up relations by name from the catalog.</li>\n\t<li>Mapping named attributes, such as col, to the input provided given operator’s children.</li>\n\t<li>Determining which attributes refer to the same value to give them a unique ID (which later allows optimization of expressions such as <code>col = col</code>).</li>\n\t<li>Propagating and coercing types through expressions: for example, we cannot know the return type of <code>1 + col</code> until we have resolved col and possibly casted its subexpressions to a compatible types.</li>\n</ul>\nIn total, the rules for the analyzer are about <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala\">1000 lines of code</a>.\n<h2>Logical Optimizations</h2>\nThe logical optimization phase applies standard rule-based optimizations to the logical plan. (Cost-based optimization is performed by generating multiple plans using rules, and then computing their costs.) These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, and other rules. In general, we have found it extremely simple to add rules for a wide variety of situations. For example, when we added the fixed-precision DECIMAL type to Spark SQL, we wanted to optimize aggregations such as sums and averages on DECIMALs with small precisions; it took 12 lines of code to write a rule that finds such decimals in SUM and AVG expressions, and casts them to unscaled 64-bit LONGs, does the aggregation on that, then converts the result back. A simplified version of this rule that only optimizes SUM expressions is reproduced below:\n\n[scala]\nobject DecimalAggregates extends Rule[LogicalPlan] {\n  /** Maximum number of decimal digits in a Long */\n  val MAX_LONG_DIGITS = 18\n  def apply(plan: LogicalPlan): LogicalPlan = {\n    plan transformAllExpressions {\n      case Sum(e @ DecimalType.Expression(prec, scale))\n          if prec + 10 &lt;= MAX_LONG_DIGITS =&gt;\n        MakeDecimal(Sum(UnscaledValue(e)), prec + 10, scale) }\n}\n[/scala]\n\nAs another example, a 12-line rule optimizes LIKE expressions with simple regular expressions into String.startsWith or String.contains calls. The freedom to use arbitrary Scala code in rules made these kinds of optimizations, which go beyond pattern-matching the structure of a subtree, easy to express.\n\nIn total, the logical optimization rules are <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala\">800 lines of code</a>.\n<h2>Physical Planning</h2>\nIn the physical planning phase, Spark SQL takes a logical plan and generates one or more physical plans, using physical operators that match the Spark execution engine. It then selects a plan using a cost model. At the moment, cost-based optimization is only used to select join algorithms: for relations that are known to be small, Spark SQL uses a broadcast join, using a peer-to-peer broadcast facility available in Spark. The framework supports broader use of cost-based optimization, however, as costs can be estimated recursively for a whole tree using a rule. We thus intend to implement richer cost-based optimization in the future.\n\nThe physical planner also performs rule-based physical optimizations, such as pipelining projections or filters into one Spark map operation. In addition, it can push operations from the logical plan into data sources that support predicate or projection pushdown. We will describe the API for these data sources in a later section.\n\nIn total, the physical planning rules are about <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala\">500 lines of code</a>.\n<h2>Code Generation</h2>\nThe final phase of query optimization involves generating Java bytecode to run on each machine. Because Spark SQL often operates on in-memory datasets, where processing is CPU-bound, we wanted to support code generation to speed up execution. Nonetheless, code generation engines are often complicated to build, amounting essentially to a compiler. Catalyst relies on a special feature of the Scala language, quasiquotes, to make code generation simpler. Quasiquotes allow the programmatic construction of abstract syntax trees (ASTs) in the Scala language, which can then be fed to the Scala compiler at runtime to generate bytecode. We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.\n\nAs a simple example, consider the Add, Attribute and Literal tree nodes introduced in Section 4.2, which allowed us to write expressions such as <code>(x+y)+1</code>. Without code generation, such expressions would have to be interpreted for each row of data, by walking down a tree of Add, Attribute and Literal nodes. This introduces large amounts of branches and virtual function calls that slow down execution. With code generation, we can write a function to translate a specific expression tree to a Scala AST as follows:\n\n[scala]\ndef compile(node: Node): AST = node match {\n  case Literal(value) =&gt; q&quot;$value&quot;\n  case Attribute(name) =&gt; q&quot;row.get($name)&quot;\n  case Add(left, right) =&gt; q&quot;${compile(left)} + ${compile(right)}&quot;\n}\n[/scala]\n\nThe strings beginning with <code>q</code> are quasiquotes, meaning that although they look like strings, they are parsed by the Scala compiler at compile time and represent ASTs for the code within. Quasiquotes can have variables or other ASTs spliced into them, indicated using <code>$</code> notation. For example, <code>Literal(1)</code> would become the Scala AST for 1, while <code>Attribute(\"x\")</code> becomes <code>row.get(\"x\")</code>. In the end, a tree like <code>Add(Literal(1), Attribute(\"x\"))</code> becomes an AST for a Scala expression like <code>1+row.get(\"x\")</code>.\n\nQuasiquotes are type-checked at compile time to ensure that only appropriate ASTs or literals are substituted in, making them significantly more useable than string concatenation, and they result directly in a Scala AST instead of running the Scala parser at runtime. Moreover, they are highly composable, as the code generation rule for each node does not need to know how the trees returned by its children are constructed. Finally, the resulting code is further optimized by the Scala compiler in case there are expression-level optimizations that Catalyst missed. The following figure shows that quasiquotes let us generate code with performance similar to hand-tuned programs.\n<img class=\"aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.45.27-AM-300x129.png\" alt=\"\" width=\"400\" />\n\nWe have found quasiquotes very straightforward to use for code generation, and we observed that even new contributors to Spark SQL could quickly add rules for new types of expressions. Quasiquotes also work well with our goal of running on native Java objects: when accessing fields from these objects, we can code-generate a direct access to the required field, instead of having to copy the object into a Spark SQL Row and use the Row’s accessor methods. Finally, it was straightforward to combine code-generated evalua- tion with interpreted evaluation for expressions we do not yet generate code for, since the Scala code we compile can directly call into our expression interpreter.\n\nIn total, Catalyst’s code generator is about <a href=\"https://github.com/apache/spark/blob/fedbfc7074dd6d38dc5301d66d1ca097bc2a21e0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala\">700 lines of code</a>.\n\nThis blog post covered the internals of Spark SQL’s Catalyst optimizer. It’s novel, simple design has enabled the Spark community to rapidly prototype, implement, and extend the engine. You can read through rest of the <a href=\"http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf\">paper here</a>. If you are attending SIGMOD this year, please drop by our session!<strong><strong>\n</strong></strong>\n\nYou can also find more information about Spark SQL from the following:\n<ul>\n\t<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">Spark SQL and DataFrame Programming Guide</a> from Apache Spark</li>\n\t<li><a href=\"http://www.slideshare.net/databricks/yin-huai-20150325meetupwithdemos\">Data Source API in Spark</a> presentation by Yin Huai</li>\n\t<li><a href=\"http://www.slideshare.net/databricks/introducing-dataframes-in-spark-for-large-scale-data-science\">Introducing DataFrames in Spark for Large Scale Data Science</a> by Reynold Xin</li>\n\t<li><a href=\"http://www.slideshare.net/databricks/spark-sqlsse2015public\">Beyond SQL: Speeding up Spark with DataFrames</a> by Michael Armbrust</li>\n</ul></td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-04-14, 2015-04-14, UTC)</td><td><div class=\"asset-body\">This is a guest post from Bob DuCharme.</div>\n<div class=\"asset-body\">Original article appeared in: <a href=\"http://www.snee.com/bobdc.blog/2015/04/running-spark-graphx-algorithm.html\" target=\"_blank\">http://www.snee.com/bobdc.blog/2015/04/running-spark-graphx-algorithm.html</a></div>\n<div class=\"asset-body\">\n\n<hr />\n\n</div>\n<div class=\"asset-body\"><b>Well, one algorithm, but a very cool one.</b></div>\n<div id=\"more\" class=\"asset-more\">\n<div id=\"id116252\">\n\n<img id=\"id116247\" src=\"http://www.snee.com/bobdc.blog/img/GraphXLoCSKOS.png\" alt=\"GraphX LoC SKOS logos\" width=\"160\" align=\"right\" border=\"0\" hspace=\"30px\" vspace=\"30px\" />\n<p id=\"id116269\">Last month, in <a id=\"id116271\" href=\"http://www.snee.com/bobdc.blog/2015/03/spark-and-sparql-rdf-graphs-an.html\">Apache Spark and SPARQL; RDF Graphs and GraphX</a>, I described how Apache Spark has emerged as a more efficient alternative to MapReduce for distributing computing jobs across clusters. I also described how Spark's GraphX library lets you do this kind of computing on graph data structures and how I had some ideas for using it with RDF data. My goal was to use RDF technology on GraphX data and vice versa to demonstrate how they could help each other, and I demonstrated the former with a Scala program that output some GraphX data as RDF and then showed some SPARQL queries to run on that RDF.</p>\n<p id=\"id116277\">Today I'm demonstrating the latter by reading in a well-known RDF dataset and executing GraphX's Connected Components algorithm on it. This algorithm collects nodes into groupings that connect to each other but not to any other nodes. In classic Big Data scenarios, this helps applications perform tasks such as the identification of subnetworks of people within larger networks, giving clues about which products or cat videos to suggest to those people based on what their friends liked.</p>\n<p id=\"id116281\">The US Library of Congress has been working on their <a id=\"id116283\" href=\"http://id.loc.gov/authorities/subjects.html\">Subject Headings</a> metadata since 1898, and it's available in SKOS RDF. Many of the subjects include \"related\" values; for example, you can see that the subject <a id=\"id116288\" href=\"http://id.loc.gov/authorities/subjects/sh85027617.html\">Cocktails</a> has related values of <a id=\"id116293\" href=\"http://id.loc.gov/authorities/subjects/sh85027615.html\">Cocktail parties</a> and <a id=\"id116298\" href=\"http://id.loc.gov/authorities/subjects/sh2009010761.html\">Happy hours</a>, and that Happy hours has related values of <a id=\"id116304\" href=\"http://id.loc.gov/authorities/subjects/sh93000452.html\">Bars (Drinking establishments)</a>, <a id=\"id116139\" href=\"http://id.loc.gov/authorities/subjects/sh85113249.html\">Restaurants</a>, and Cocktails. So, while it includes skos:related triples that indirectly link Cocktails to Restaurants, it has none that link these to the subject of <a id=\"id116126\" href=\"http://id.loc.gov/authorities/subjects/sh85125961.html\">Space stations</a>, so the Space stations subject is not part of the same Connected Components subgraph as the Cocktails subject.</p>\n<p id=\"id116111\">After reading the Library of Congress Subject Header RDF into a GraphX graph and running the Connected Components algorithm on the skos:related connections, here are some of the groupings I found near the beginning of the output:</p>\n\n<pre id=\"id116102\">\"Hiding places\" \n\"Secrecy\" \n\"Loneliness\" \n\"Solitude\" \n\"Privacy\" \n--------------------------\n\"Cocktails\" \n\"Bars (Drinking establishments)\" \n\"Cocktail parties\" \n\"Restaurants\" \n\"Happy hours\" \n--------------------------\n\"Space stations\" \n\"Space colonies\" \n\"Large space structures (Astronautics)\" \n\"Extraterrestrial bases\" \n--------------------------\n\"Inanna (Sumerian deity)\" \n\"Ishtar (Assyro-Babylonian deity)\" \n\"Astarte (Phoenician deity)\" \n--------------------------\n\"Cross-cultural orientation\" \n\"Cultural competence\" \n\"Multilingual communication\" \n\"Intercultural communication\" \n\"Technical assistance--Anthropological aspects\" \n--------------------------\n</pre>\n<p id=\"id116094\">(You can find the <a id=\"id116090\" href=\"http://snee.com/bobdc.blog/files/readLoCSH.out\">complete output here</a>, a 565K file.) People working with RDF-based applications already know that this kind of data can help to enhance search. For example, someone searching for media about \"Space stations\" will probably also be interested in media filed under \"Space colonies\" and \"Extraterrestrial bases\". This data can also help other applications, and now, it can help distributed applications that use Spark.</p>\n\n<div id=\"id116082\">\n<h2 id=\"id116235\">Storing RDF in GraphX data structures</h2>\n<p id=\"id116227\">First, as I mentioned in the earlier blog entry, GraphX development currently means coding with the Scala programming language, so I have been learning Scala. My old friend from XML days <a id=\"id116222\" href=\"http://www.contakt.org/\">Tony Coates</a> wrote <a id=\"id116208\" href=\"http://www.contakt.org/Blog/Post/13/A-Scala-API-for-RDF-Processing\">A Scala API for RDF Processing</a>, which takes better advantage of native Scala data structures than I ever could, and the <a id=\"id116198\" href=\"https://github.com/w3c/banana-rdf\">banana-rdf Scala library</a> also looks interesting, but although I was using Scala my main interest was storing RDF in Spark GraphX data structures, not in Scala particularly.</p>\n<p id=\"id116186\">The basic Spark data structure is the Resilient Distributed Dataset, or RDD. The graph data structure used by GraphX is a combination of an RDD for vertices and one for edges. Each of these RDDs can have additional information; the Spark website's <a id=\"id116176\" href=\"https://spark.apache.org/docs/1.1.1/graphx-programming-guide.html#example-property-graph\">Example Property Graph</a> includes (name, role) pairs with its vertices and descriptive property strings with its edges. The obvious first step for storing RDF in a GraphX graph would be to store predicates in the edges RDD, subjects and resource objects in the vertices RDD, and literal properties as extra information in these RDDs like the (name, role) pairs and edge description strings in the Spark website's Example Property Graph.</p>\n\n<div id=\"id116082\">\n<p id=\"id118585\">But, as I also wrote last time, a hardcore RDF person would ask <a id=\"id118588\" href=\"http://www.snee.com/bobdc.blog/2015/03/spark-and-sparql-rdf-graphs-an.html#id106263\">these questions</a>:</p>\n\n<ul id=\"id118594\">\n \t<li id=\"id118596\">\n<p id=\"id118597\">What about properties of edges? For example, what if I wanted to say that an <tt id=\"id118600\">xp:advisor</tt>property was an <tt id=\"id118603\">rdfs:subPropertyOf</tt> the Dublin Core property <tt id=\"id118607\">dc:contributor</tt>?</p>\n</li>\n \t<li id=\"id118612\">\n<p id=\"id118613\">The ability to assign properties such as a name of \"rxin\" and a role of \"student\" to a node like 3L is nice, but what if I don't have a consistent set of properties that will be assigned to every node—for example, if I've aggregated person data from two different sources that don't use all the same properties to describe these persons?</p>\n</li>\n</ul>\n<p id=\"id118618\">The Example Property Graph can store these (name, role) pairs with the vertices because that RDD is declared as <tt id=\"id118620\">RDD[(VertexId, (String, String))]</tt>. Each vertex will have two strings stored with it; no more and no less. It's a data structure, but you can also think of it as a proscriptive schema, and the second bullet above is asking how to get around that.</p>\n<p id=\"id118625\">I got around both issues by storing the data in three data structures—the two RDDs described above and one more:</p>\n\n<ul id=\"id118628\">\n \t<li id=\"id118631\">\n<p id=\"id118632\">For the vertex RDD, along with the required long integer that must be stored as each vertex's identifier, I only stored one extra piece of information: the URI associated with that RDF resource. I did this for the subjects, the predicates (which may not be \"vertices\" in the GraphX sense of the word, but damn it, they're resources that can be the subjects or objects of triples if I want them to), and the relevant objects. After reading the triple {<tt id=\"id118634\">&lt;http://id.loc.gov/authorities/subjects/sh85027617&gt; &lt;http://www.w3.org/2004/02/skos/core#related&gt; &lt;http://id.loc.gov/authorities/subjects/sh2009010761&gt;</tt>} from the Library of Congress data, the program will create three vertices in this RDD whose node identifiers might be 1L, 2L, and 3L, with each of the triple's URIs stored with one of these RDD vertices.</p>\n</li>\n \t<li id=\"id118639\">\n<p id=\"id118640\">For the edge RDD, along with the required two long integers identifying the vertices at the start and end of the edge, each of my edges also stores the URI of the relevant predicate as the \"description\" of the edge. The edge for the triple above would be (1L, 3L, http://www.w3.org/2004/02/skos/core#related).</p>\n</li>\n \t<li id=\"id118644\">\n<p id=\"id118645\">To augment the graph data structure created from the two RDDs above, I created a third RDD to store literal property values. Each entry stores the long integer representing the vertex of the resource that has the property, a long integer representing the property (the integer assigned to that property in the vertex RDD), and a string representing the property value. For the triple { <tt id=\"id118648\">&lt;http://id.loc.gov/authorities/subjects/sh2009010761&gt; &lt;http://www.w3.org/2004/02/skos/core#prefLabel&gt; \"Happy hours\"</tt>} it might store (3L, 4L, \"Happy hours\"), assuming that 4L had been stored as the internal identifier for the skos:prefLabel property. To run the Connected Components algorithm and then output the preferred label of each member of each subgraph, I didn't need this RDD, but it does open up many possibilities for what you can do with RDF in an a Spark GraphX program.</p>\n</li>\n</ul>\n</div>\n<div id=\"id118734\">\n<h2 id=\"id118737\">Creating a report on Library of Congress Subject Heading connecting components</h2>\n<p id=\"id118740\">After loading up these data structures (plus another one that allows quick lookups of preferred labels) my program below applies the GraphX Connected Components algorithm to the subset of the graph that uses the skos:related property to connect vertices such as \"Cocktails\" and \"Happy hours\". Iterating through the results, it uses them to load a hash map with a list for each subgraph of connected components. Then, it goes through each of these lists, printing the label associated with each member of each subgraph and a string of hyphens to show where each list ends, as you can see in the excerpt above.</p>\n<p id=\"id118744\">I won't go into more detail about what's in my program because I commented it pretty heavily. (I do have to thank my friend Tony, mentioned above, for helping me past one point where I was stuck on a Scala scoping issue. Also, as I've warned before, my coding style will probably make experienced Scala programmers choke on their Red Bull. I'd be happy to hear about suggested improvements.)</p>\n<p id=\"id118748\">After getting the program to run properly with a small subset of the data, I ran it on the 1 GB subjects-skos-2014-0306.nt file that I downloaded from the Library of Congress with its 7,705,147 triples. Spark lets applications scale up by giving you an infrastructure to distribute program execution across multiple machines, but the 8GB on my single machine wasn't enough to run this, so I used two grep commands to create a version of the data that only had the skos:related and skos:prefLabel triples. At this point I had a total of 439,430 triples. Because my code didn't account for blank nodes, I removed the 385 triples that used them, leaving 439,045 to work with in a 60MB file. This ran successfully and you can follow the link shown earlier to see the complete output.</p>\n\n</div>\n<div id=\"id118752\">\n<h2 id=\"id118755\">Other GraphX algorithms to run on your RDF data</h2>\n<p id=\"id118758\"><a id=\"id118760\" href=\"https://spark.apache.org/docs/latest/graphx-programming-guide.html#graph-algorithms\">Other GraphX algorithms</a> besides Connected Components include Page Rank and Triangle Counting. <a id=\"id118764\" href=\"http://en.wikipedia.org/wiki/Graph_theory\">Graph theory</a> is an interesting world, in which my favorite phrase so far is \"<a id=\"id118769\" href=\"http://en.wikipedia.org/wiki/Strangulated_graph\">strangulated graph</a>\".</p>\n<p id=\"id118775\">One of the greatest things about RDF and Linked Data technology is the <a id=\"id118778\" href=\"http://linkeddata.org/\">growing amount</a> of interesting data being made publicly available, and with new tools such as these algorithms to work with this data—tools that can be run on inexpensive, scalable clusters faster than typical Hadoop MapReduce jobs—there are a lot of great possibilities.</p>\n\n\n[scala]\n//////////////////////////////////////////////////////////////////\n// readLoCSH.scala: read Library of Congress Subject Headings into\n// Spark GraphX graph and apply connectedComponents algorithm to those\n// connected by skos:related property.\n\nimport scala.io.Source \nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\nimport scala.collection.mutable.ListBuffer\nimport scala.collection.mutable.HashMap\n\nobject readLoCSH {\n\nval componentLists = HashMap[VertexId, ListBuffer[VertexId]]()\nval prefLabelMap =  HashMap[VertexId, String]()\n\ndef main(args: Array[String]) {\nval sc = new SparkContext(&quot;local&quot;, &quot;readLoCSH&quot;, &quot;127.0.0.1&quot;)\n\n// regex pattern for end of triple\nval tripleEndingPattern = &quot;&quot;&quot;\\s*\\.\\s*$&quot;&quot;&quot;.r    \n// regex pattern for language tag\nval languageTagPattern = &quot;@[\\\\w-]+&quot;.r    \n\n// Parameters of GraphX Edge are subject, object, and predicate\n// identifiers. RDF traditionally does (s, p, o) order but in GraphX\n// it's (edge start node, edge end node, edge description).\n\n// Scala beginner hack: I couldn't figure out how to declare an empty\n// array of Edges and then append Edges to it (or how to declare it\n// as a mutable ArrayBuffer, which would have been even better), but I\n// can append to an array started like the following, and will remove\n// the first Edge when creating the RDD.\n\nvar edgeArray = Array(Edge(0L,0L,&quot;http://dummy/URI&quot;))\nvar literalPropsTriplesArray = new Array[(Long,Long,String)](0)\nvar vertexArray = new Array[(Long,String)](0)\n\n// Read the Library of Congress n-triples file\n//val source = Source.fromFile(&quot;sampleSubjects.nt&quot;,&quot;UTF-8&quot;)  // shorter for testing\nval source = Source.fromFile(&quot;PrefLabelAndRelatedMinusBlankNodes.nt&quot;,&quot;UTF-8&quot;)\n\nval lines = source.getLines.toArray\n\n// When parsing the data we read, use this map to check whether each\n// URI has come up before.\nvar vertexURIMap = new HashMap[String, Long];\n\n// Parse the data into triples.\nvar triple = new Array[String](3)\nvar nextVertexNum = 0L\nfor (i &lt;- 0 until lines.length) {\n    // Space in next line needed for line after that. \n    lines(i) = tripleEndingPattern.replaceFirstIn(lines(i),&quot; &quot;)  \n    triple = lines(i).mkString.split(&quot;&gt;\\\\s+&quot;)       // split on &quot;&gt; &quot;\n    // Variables have the word &quot;triple&quot; in them because &quot;object&quot; \n    // by itself is a Scala keyword.\n    val tripleSubject = triple(0).substring(1)   // substring() call\n    val triplePredicate = triple(1).substring(1) // to remove &quot;&lt;&quot;\n    if (!(vertexURIMap.contains(tripleSubject))) {\n        vertexURIMap(tripleSubject) = nextVertexNum\n        nextVertexNum += 1\n    }\n    if (!(vertexURIMap.contains(triplePredicate))) {\n        vertexURIMap(triplePredicate) = nextVertexNum\n        nextVertexNum += 1\n    }\n    val subjectVertexNumber = vertexURIMap(tripleSubject)\n    val predicateVertexNumber = vertexURIMap(triplePredicate)\n\n    // If the first character of the third part is a &lt;, it's a URI;\n    // otherwise, a literal value. (Needs more code to account for\n    // blank nodes.)\n    if (triple(2)(0) == '&lt;') { \n        val tripleObject = triple(2).substring(1)   // Lose that &lt;.\n        if (!(vertexURIMap.contains(tripleObject))) {\n            vertexURIMap(tripleObject) = nextVertexNum\n            nextVertexNum += 1\n        }\n        val objectVertexNumber = vertexURIMap(tripleObject)\n        edgeArray = edgeArray :+\n            Edge(subjectVertexNumber,objectVertexNumber,triplePredicate)\n    }\n    else {\n        literalPropsTriplesArray = literalPropsTriplesArray :+\n            (subjectVertexNumber,predicateVertexNumber,triple(2))\n    }\n}\n\n// Switch value and key for vertexArray that we'll use to create the\n// GraphX graph.\nfor ((k, v) &lt;- vertexURIMap) vertexArray = vertexArray :+  (v, k)   \n\n// We'll be looking up a lot of prefLabels, so create a hashmap for them. \nfor (i &lt;- 0 until literalPropsTriplesArray.length) {\n    if (literalPropsTriplesArray(i)._2 ==\n        vertexURIMap(&quot;http://www.w3.org/2004/02/skos/core#prefLabel&quot;)) {\n        // Lose the language tag.\n        val prefLabel =\n            languageTagPattern.replaceFirstIn(literalPropsTriplesArray(i)._3,&quot;&quot;)\n        prefLabelMap(literalPropsTriplesArray(i)._1) = prefLabel;\n    }\n}\n\n// Create RDDs and Graph from the parsed data.\n\n// vertexRDD Long: the GraphX longint identifier. String: the URI.\nval vertexRDD: RDD[(Long, String)] = sc.parallelize(vertexArray)\n\n// edgeRDD String: the URI of the triple predicate. Trimming off the\n// first Edge in the array because it was only used to initialize it.\nval edgeRDD: RDD[Edge[(String)]] =\n    sc.parallelize(edgeArray.slice(1,edgeArray.length))\n\n// literalPropsTriples Long, Long, and String: the subject and predicate\n// vertex numbers and the the literal value that the predicate is\n// associating with the subject.\nval literalPropsTriplesRDD: RDD[(Long,Long,String)] =\n    sc.parallelize(literalPropsTriplesArray)\n\nval graph: Graph[String, String] = Graph(vertexRDD, edgeRDD)\n\n// Create a subgraph based on the vertices connected by SKOS &quot;related&quot;\n// property.\nval skosRelatedSubgraph =\n    graph.subgraph(t =&gt; t.attr ==\n                   &quot;http://www.w3.org/2004/02/skos/core#related&quot;)\n\n// Find connected components  of skosRelatedSubgraph.\nval ccGraph = skosRelatedSubgraph.connectedComponents() \n\n// Fill the componentLists hashmap.\nskosRelatedSubgraph.vertices.leftJoin(ccGraph.vertices) {\ncase (id, u, comp) =&gt; comp.get\n}.foreach\n{ case (id, startingNode) =&gt; \n  {\n      // Add id to the list of components with a key of comp.get\n      if (!(componentLists.contains(startingNode))) {\n          componentLists(startingNode) = new ListBuffer[VertexId]\n      }\n      componentLists(startingNode) += id\n  }\n}\n\n// Output a report on the connected components. \nprintln(&quot;------  connected components in SKOS \\&quot;related\\&quot; triples ------\\n&quot;)\nfor ((component, componentList) &lt;- componentLists){\n    if (componentList.size &gt; 1) { // don't bother with lists of only 1\n        for(c &lt;- componentList) {\n            println(prefLabelMap(c));\n        }\n        println(&quot;--------------------------&quot;)\n    }\n}\n\nsc.stop\n}\n[/scala]\n\n</div>\n</div>\n</div>\n</div></td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Customers, Product)</td><td>List(2015-04-15, 2015-04-15, UTC)</td><td>We are thrilled to announce that Celtra selected Databricks to scale its big data analysis projects, increasing the amount of ad-hoc analysis done, six-fold.\n\nPress release: <a href=\"http://www.marketwired.com/press-release/celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud-2009995.htm\" target=\"_blank\">http://www.marketwired.com/press-release/celtra-scales-big-data-analysis-projects-six-fold-with-databricks-cloud-2009995.htm</a>\n\n<a href=\"http://www.celtra.com/\">Celtra</a> provides agencies, media suppliers and brand leaders alike with an integrated, scalable HTML5 technology for brand advertising on smartphones, tablets and desktop. The platform, <i>AdCreator 4</i>, gives clients such as MEC, Kargo, Pepsi and Macy’s the ability to easily create, manage, and traffic sophisticated data-driven dynamic ads, optimize them on the go, and track their performance with insightful analytics.\n\nA wide variety of data is collected by Celtra, including data related to internal company processes, data based on the usage of the product by clients and, most importantly, data focused on the engagements of consumers with their clients’ ads. In addition to providing analytics to its clients, Celtra is constantly exploring new ways to leverage this information to improve their offering.\n\nAs Celtra’s business grew, it was challenged to meet the corresponding increase in demand for analytics because of its diverse data sources, terabyte scale data, and small analytics team. It needed a powerful data platform that was capable of integrating data from disparate data sources while being fast enough to support interactive analysis at terabyte scale. This platform must also be user-friendly enough to empower teams outside of analytics to perform analysis themselves, and to remove the bottleneck created by their small analytics team.\n\nWith the adoption of Databricks, Celtra has enabled teams from Engineering, Product Management, and QA to perform complex data analysis on their own, leveraging the massive production data to improve product design, address anomalies rapidly, and fine-tune the performance of production systems.\n\nDatabricks provided Celtra with a number of critical business benefits:\n<ul>\n \t<li>Increased the amount of ad-hoc analysis done six-fold, leading to better informed product design and quicker issue detection and resolution.</li>\n \t<li>Reduced the load on the analytics engineering team by expanding access to the number of people able to work with the data directly by a factor of four.</li>\n \t<li>Increased collaboration and improved reproducibility and repeatability of analyses.</li>\n \t<li>Reduced the cost of cloud infrastructure through faster and easier management of Apache Spark clusters.</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/resources/customer-case-studies\" target=\"_blank\">case study</a> learn more about how Celtra is using Databricks.</td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-04-16, 2015-04-16, UTC)</td><td>Recently, Databricks added a new feature, Jobs, to our cloud service. You can find a detailed overview of this feature <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\">here</a>.\n\nThis feature allows one to programmatically run Apache Spark jobs on Amazon’s EC2 easier than ever before. In this blog, I will provide a quick tour of this feature.\n<h2>What is a Job?</h2>\nThe job feature is very flexible. A user can run a job not only as any Spark JAR, but also notebooks you have created with Databricks Cloud. In addition, notebooks can be used as scripts to create sophisticated pipelines.\n<h2>How to run a Job?</h2>\nAs shown below, Databricks Cloud offers an intuitive, easy to use interface to create a job.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.16.29-PM.png\"><img class=\"aligncenter wp-image-3472\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.16.29-PM-1024x293.png\" alt=\"Jobs how-to blog figure 1\" width=\"600\" height=\"171\" /></a>\n\nWhen creating a job, you will need to specify the name and the size of the cluster which will run the job. Since typically with Spark the amount of memory determines its performance, you will then be asked to enter the memory capacity of the cluster. Databricks Cloud will automatically instantiate a cluster of the specified capacity when running the job either by reusing your existing cluster or by creating a new one, and then subsequently tears down the cluster, once the job completes.\n\nNext, you need to specify the notebook or the JAR you intend to run as a job, the input arguments of the job (both JARs and notebooks can take input arguments), and the job’s configuration parameters: <i>schedule</i>, <i>timeout</i>, <i>alerts</i>, and the <i>type of EC2 instances</i> you would like the job to use. Next, we consider each of these configuration parameters in turn.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.18.17-PM.png\"><img class=\"aligncenter wp-image-3473\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.18.17-PM-300x216.png\" alt=\"Jobs howto blog figure 2\" width=\"250\" height=\"180\" /></a>\n\n<b>Scheduling</b>: The user can run any job periodically, by simply specifying the starting time and the interval, as shown below.\n\n<strong><strong><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.21.47-PM.png\"><img class=\"aligncenter wp-image-3475\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.21.47-PM-300x88.png\" alt=\"Jobs how-to blog figure 3\" width=\"400\" height=\"118\" /></a></strong></strong>\n\n<b>Timeout</b>: Optionally the user can set a timeout which specifies the time the job is allowed to run before being terminated. This feature is especially useful when handling runaway jobs, and to make sure that an instance of a periodic job terminates before the next instance begins. If no timeout is specified and a job instance takes more than the scheduling period, no new instances are started before the current one terminates.\n\n<strong><strong><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.25.57-PM.png\"><img class=\"aligncenter size-medium wp-image-3476\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.25.57-PM-300x126.png\" alt=\"Jobs how-to blog figure 4\" width=\"300\" height=\"126\" /></a></strong></strong><strong><strong> </strong></strong>\n\n<b>Alerts</b>: When running production jobs, it is critical to alert the user when any significant event occurs. Databricks Cloud allows a user to specify the events they would like to be alerted on via e-mail: when <i>job starts</i>, when it <i>successfully finishes</i>, or when it <i>finishes with error</i>.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.27.39-PM.png\"><img class=\"aligncenter size-medium wp-image-3477\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.27.39-PM-300x193.png\" alt=\"Jobs how-to figure 5\" width=\"300\" height=\"193\" /></a>\n\n<b>Resource type</b>: Finally, the user can specify whether they would want to use spot or on-demand instances to run the job.\n\n<strong><strong> <a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.28.54-PM.png\"><img class=\"aligncenter size-medium wp-image-3478\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.28.54-PM-300x151.png\" alt=\"Jobs how-to figure 6\" width=\"300\" height=\"151\" /></a> </strong></strong>\n<h2>History and Results<strong><strong> </strong></strong></h2>\nThe Job UI provides an easy way to inspect the status of each run of a given job. The figure below shows the status of multiple runs of the same job. i.e., when each run starts, how long it takes, and if it has terminated successfully.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.33.59-PM.png\"><img class=\"aligncenter size-medium wp-image-3481\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-6.33.59-PM-300x152.png\" alt=\"Jobs how-to figure 6\" width=\"300\" height=\"152\" /></a>\n\nBy clicking on any of the “Run x” links you can immediately see the output of the corresponding run including its output logs and errors, if any. The picture below shows the output of “Run 6” above.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.36.50-PM.png\"><img class=\"aligncenter size-medium wp-image-3484\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.36.50-PM-300x234.png\" alt=\"Jobs how-to blog figure 7\" width=\"300\" height=\"234\" /></a>\n\nSimilarly, the figure below shows the output of running a notebook as a job. Incidentally, the output is the same as running the notebook manually.\n\n<strong><strong><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.37.30-PM.png\"><img class=\"aligncenter size-medium wp-image-3485\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-14-at-7.37.30-PM-300x296.png\" alt=\"Jobs how-to figure 8\" width=\"300\" height=\"296\" /></a></strong></strong><strong><strong> </strong></strong>\n<h2>Summary</h2>\nAs I hope this short tour has convinced you, Databricks Cloud provides a powerful, yet easy to use feature to run not only arbitrary Spark jobs, but also notebooks created with Databricks Cloud. If you’d like to run your own jobs with Databricks Cloud, you can <a href=\"https://databricks.com/registration\">register</a> here for an account.\n<h2>Additional Resources</h2>\n<p style=\"text-align: left;\">Other Databricks Cloud how-tos can be found at:</p>\n\n<ul>\n \t<li style=\"text-align: left;\"><a href=\"https://databricks.com/?p=3558\" target=\"_blank\">Analyzing Apache Access Logs with Databricks Cloud</a></li>\n</ul></td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-04-17, 2015-04-17, UTC)</td><td>This is a guest blog post from Huawei’s big data global team.\n\nHuawei, a Fortune Global 500 private company, has put together a global team since 2013 to work on Apache Spark community projects and contribute back to the community. This blog post describes two new MLlib algorithms contributed from Huawei in Apache Spark 1.3 and their use cases: FP-growth for frequent pattern mining and Power Iteration Clustering for graph clustering.\n\n<hr />\n\n&nbsp;\n<h2>FP-Growth: Scalable Frequent Itemset Mining</h2>\nAs smartphones and the mobile internet become more and more popular, a huge amount of data traffic is transmitted each second on the global internet. In a typical network of millions subscribers, traffic rates can reach terabytes per second, which drives gigabytes of event logs generated per second from the underneath network equipment. At Huawei, we are often interested in analyzing traffic patterns from these logs, so we can leverage usage information to make the network more efficient. A common technique for analyzing network data is <i>frequent pattern mining</i>. Frequent pattern mining can reveal the most frequently visited site in a particular period or find popular routing paths that generate most traffic in a particular region. Finding these patterns allows us to improve utilization of the network; for instance, information on routing hotspots can influence the placement of gateway and routers in the network.\n<h2>FP-Growth</h2>\nThe FP-growth mining problem models its input as a set of <i>transactions</i>. Each transaction is simply a set of <i>items</i> and the algorithm looks for common subsets of items that appear across transactions. For a subset to be considered a pattern, it must appear in some minimum proportion of all transactions, termed the <i>support</i>. In the case of a telco network, items would be individual network nodes, and a transaction could represent one path of nodes. Then the algorithm would return sub-paths of the network that are frequently traversed.\n\nA naive way to do this is to generate all possible itemsets and count their occurrence, which is not scalable because it quickly becomes a combinatorial explosion problem as the input data size increases. To solve this problem, we chose FP-growth, a classic algorithm that finds all frequent itemsets without generating and testing all candidates. And to make FP-growth work on large-scale datasets, we at Huawei has implemented a parallel version of FP-growth, as described in <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-growth for query recommendation</a>, and contributed it to Apache Spark 1.3.\n\nHere is a brief description of the algorithm. The algorithm takes an RDD of transactions from user, and works in two steps to output frequent itemsets. In the first step, item frequency is calculated and infrequent items are filtered (because frequent itemsets must consist of frequent items). In the second step, suffix trees (FP-trees) are constructed and grown from the filtered transactions, and then frequent itemsets can be extracted from the suffix trees. The work is distributed based on the suffixes of the filtered transactions, and <code>combineByKey</code> is used to reduce the amount of shuffle data.\n<h2>Scalability</h2>\nWe have compared MLlib’s FP-growth implementation against Mahout on our production datasets. The results are plotted as below.\n<p style=\"text-align: center;\"><img class=\"wp-image-3526\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.16.01-AM1.png\" alt=\"Experiment 1: Running times for different support levels using a 1.5GB data set.\" width=\"500\" height=\"305\" /></p>\n<p style=\"text-align: center;\">Experiment 1: Running times for different support levels using a 1.5GB data set.</p>\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.20.13-AM.png\"><img class=\"aligncenter wp-image-3529\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.20.13-AM.png\" alt=\"Huawei guest blog figure 2\" width=\"500\" height=\"305\" /></a></p>\n<p style=\"text-align: center;\">Experiment 2: Running times for different data sizes (GB).</p>\nAs shown in the figures, MLlib is about 7~9 times faster than Mahout on a 1.5GB dataset, and MLlib achieves good scalability as the dataset grows 10 times and 100 times. In the largest test, MLlib is about 11 times faster than Mahout.\n<h2>Examples</h2>\n<p style=\"text-align: left;\">MLlib’s FP-growth is available in Scala/Java in Apache Spark 1.3. Its Python API was merged recently and it will be available in 1.4. Following example code demonstrates its API usage:</p>\n\n\n[scala]\nimport org.apache.spark.mllib.fpm.FPGrowth\n\n// the input data set containing all transactions\nval transactions = sc.textFile(&quot;...&quot;).map(_.split(&quot; &quot;)).cache()\n\n// run the FP-growth algorithm\nval model = new FPGrowth()\n  .setMinSupport(0.5)\n  .setNumPartitions(10)\n  .run(transactions)\n\n// print the frequent itemset result\nmodel.freqItemsets.collect().foreach { itemset =&gt;\n  println(itemset.items.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;) + &quot;, &quot; + itemset.freq)\n}\n[/scala]\n\nFor more information about MLlib’s FP-growth, please visit its <a href=\"https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html\">user guide</a> and check out full examples in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\">Scala</a> and in <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/mllib/JavaFPGrowthExample.java\">Java</a> on GitHub.\n<h2>Power Iteration Clustering: Spectral Clustering on GraphX</h2>\nCommunication service providers like Huawei must manage, operate, and optimize increasingly dynamic traffic workloads on heterogeneous networks. Among various algorithms being used in this effort, unsupervised learning including clustering plays an important role, for example, in identifying similar behaviors among users or network clusters. Graph clustering algorithms are commonly used in the telecom industry for this purpose, and can be applied to data center management and operation.\n<h2>Power Iteration Clustering</h2>\nWe have implemented Power Iteration Clustering (PIC) in MLlib, a simple and scalable graph clustering method described in <a href=\"http://www.icml2010.org/papers/387.pdf\">Lin and Cohen, Power Iteration Clustering</a>. PIC takes an undirected graph with similarities defined on edges and outputs clustering assignment on nodes. PIC uses truncated <a href=\"http://en.wikipedia.org/wiki/Power_iteration\">power iteration</a> to find a very low-dimensional embedding of the nodes, and this embedding leads to effective graph clustering.\n\nPIC is a graph algorithm and it can be easily described in a graph language. So it was natural to implement PIC using GraphX in Spark and take advantage of GraphX’ graph processing APIs and optimization. MLlib’s PIC is among the first MLlib algorithms built upon GraphX. In particular, we store the normalized similarity matrix as a graph with normalized similarities defined as edge properties. The edge properties are cached and remain static during the power iterations. The embedding of nodes is defined as node properties on the same graph topology. We update the embedding through power iterations, where aggregateMessages is used to compute matrix-vector multiplications, the essential operation in a power iteration method. Finally, k-means is used to cluster nodes using the embedding.\n<h2>Examples</h2>\nMLlib’s PIC is available in Scala/Java in Apache Spark 1.3. Its Python support will be added in a future release. The following example code demonstrates its API usage:\n\n[scala]\nimport org.apache.spark.mllib.clustering.PowerIterationClustering\n\n\n// pairwise similarities\nval similarities: RDD[(Long, Long, Double)] = ...\n\nval pic = new PowerIteartionClustering()\n  .setK(3)\n  .setMaxIterations(20)\nval model = pic.run(similarities)\n\nmodel.assignments.collect().foreach { a =&gt;\n  println(s&quot;${a.id} -&gt; ${a.cluster}&quot;)\n}\n[/scala]\n\nA more concrete example can be found at <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\">PowerIterationClusteringExample</a>, and the following is a clustering assignment produced by it with five circles:\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.25.54-AM.png\"><img class=\"aligncenter wp-image-3533\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-17-at-8.25.54-AM.png\" alt=\"Huawei guest blog figure 3\" width=\"500\" height=\"356\" /></a>\n\nWhat we notice is that PIC is able to distinguish clearly the degree of similarity – as represented by the Euclidean distance among the points – even though their relationship is non-linear. For more information about PIC in MLlib, please visit its <a href=\"https://spark.apache.org/docs/latest/mllib-clustering.html#power-iteration-clustering-pic\">user guide</a>.\n<h2>Summary</h2>\nBoth FP-growth and PIC are included in Apache Spark 1.3. So you can <a href=\"http://spark.apache.org/downloads.html\">download it</a> now and try them out. At Huawei, our team is working on improving MLlib’s FP-growth implementation further and exploring possible enhancements to PIC. In addition, we plan to work on MLlib’s pipeline API, such as model persistence and re-deployment of the models, and share use cases of MLlib algorithms from our customers.\n<h2>Acknowledgement</h2>\nXiangrui Meng at Databricks provided tremendous help to us, including design discussions and code reviews. We also want to thank all community members who helped code reviews and expanded the work, e.g., adding Python support and model import/export.</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-04-21, 2015-04-21, UTC)</td><td>Databricks provides a powerful platform to process, analyze, and visualize big and small data in one place. In this blog, we will illustrate how to analyze access logs of an Apache HTTP web server using Notebooks. Notebooks allow users to write and run arbitrary Apache Spark code and interactively visualize the results. Currently, notebooks support three languages: Scala, Python, and SQL. In this blog, we will be using Python for illustration.\n\nThe analysis presented in this blog and much more is available in Databricks as part of the Databricks Guide. Find this notebook in your Databricks workspace at “<code>databricks_guide/Sample Applications/Log Analysis/Log Analysis in Python</code>” - it will also show you how to create a data frame of access logs with Python using the new Spark SQL 1.3 API.  Additionally, there are also Scala &amp; SQL notebooks in the same folder with similar analysis available.\n<h2>Getting Started</h2>\nFirst we need to locate the log file. In this example, we are using synthetically generated logs which are stored in the <code>“/dbguide/sample_log”</code> file. The command below (typed in the notebook) assigns the log file pathname to the <code>DBFS_SAMPLE_LOGS_FOLDER</code> variable, which will be used throughout the rest of this analysis.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.21.13-PM.png\"><img class=\"aligncenter wp-image-3559\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.21.13-PM-1024x84.png\" alt=\"Apache Log how-to figure 1\" width=\"700\" height=\"58\" /></a>\n<p style=\"text-align: center;\"><em>Figure 1: Location of the synthetically generated logs in your instance of Databricks Cloud</em></p>\n\n<h2>Parsing the Log File</h2>\nEach line in the log file corresponds to an Apache web server access request. To parse the log file, we define <code>parse_apache_log_line()</code>, a function that takes a log line as an argument and returns the main fields of the log line. The return type of this function is a PySpark SQL Row object which models the web log access request.  For this we use the “re” module which implements regular expression operations. The <code>APACHE_ACCESS_LOG_PATTERN</code> variable contains the regular expression used to match an access log line. In particular, <code>APACHE_ACCESS_LOG_PATTERN</code> matches client IP address (<code>ipAddress</code>) and identity (<code>clientIdentd</code>), user name as defined by HTTP authentication (<code>userId</code>), time when the server has finished processing the request (<code>dateTime</code>), the HTTP command issued by the client, e.g., GET (<code>method</code>), protocol, e.g., HTTP/1.0 (<code>protocol</code>), response code (<code>responseCode</code>), and the size of the response in bytes (<code>contentSize</code>).\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.26.06-PM.png\"><img class=\"aligncenter wp-image-3562\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.26.06-PM-1024x528.png\" alt=\"Apache log how-to blog figure 2\" width=\"700\" height=\"361\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 2: Example function to parse the log file in a Databricks Cloud notebook</em></p>\n\n<h2>Loading the Log File</h2>\nNow we are ready to load the logs into a <a href=\"https://spark.apache.org/docs/latest/quick-start.html#basics\">Resilient Distributed Dataset (RDD)</a>. An RDD is a partitioned collection of tuples (rows), and is the primary data structure in Spark. Once the data is stored in an RDD, we can easily analyze and process it in parallel. To do so, we launch a Spark job that reads and parses each line in the log file using the <code>parse_apache_log_line()</code> function defined earlier, and then creates an RDD, called access_logs. Each tuple in access_logs contains the fields of a corresponding line (request) in the log file, <code>DBFS_SAMPLE_LOGS_FOLDER</code>. Note that once we create the <code>access_logs</code> RDD, we cache it into memory, by invoking the <code>cache()</code> method. This will dramatically speed up subsequent operations we will perform on <code>access_logs</code>.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.29.26-PM.png\"><img class=\"aligncenter wp-image-3564\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.29.26-PM-1024x229.png\" alt=\"Apache log how-to figure 3\" width=\"700\" height=\"157\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 3: Example code to load the log file in Databricks Cloud notebook</em></p>\nAt the end of the above code snippet, notice that we count the number of tuples in <code>access_logs</code> (which returns 100,000 as a result).\n<h2>Log Analysis</h2>\nNow we are ready to analyze the logs stored in the <code>access_logs</code> RDD. Below we give two simple examples:\n<ol>\n \t<li>Computing the average content size</li>\n \t<li>Computing and plotting the frequency of each response code</li>\n</ol>\n<h3 style=\"text-align: left;\">1. Average Content Size</h3>\n<p style=\"text-align: left;\">We compute the average content size in two steps. First, we create another RDD, <code>content_sizes</code>, that contains only the “<code>contentSize</code>” field from <code>access_logs</code>, and cache this RDD:</p>\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.14-PM.png\"><img class=\"aligncenter wp-image-3567\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.14-PM-1024x133.png\" alt=\"Apache log how-to figure 4\" width=\"700\" height=\"91\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 4: Create the content size RDD in Databricks notebook</em></p>\nSecond, we use the <code>reduce()</code> operator to compute the sum of all content sizes and then divide it into the total number of tuples to obtain the average:\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.21-PM.png\"><img class=\"aligncenter wp-image-3568\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-2.34.21-PM-1024x87.png\" alt=\"Apache log how-to figure 5\" width=\"700\" height=\"59\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 5: Computing the average content size with the <code>reduce()</code> operator</em></p>\nThe result is 249 bytes. Similarly we can easily compute the min and max, as well as other statistics of the content size distribution.\n<p style=\"text-align: left;\">An important point to note is that both commands above run in parallel. Each RDD is partitioned across a set of workers, and each operation invoked on an RDD is shipped and executed in parallel at each worker on the corresponding RDD partition. For example the lambda function passed as the argument of <code>reduce()</code> will be executed in parallel at workers on each partition of the <code>content_sizes</code> RDD. This will result in computing the partial sums for each partition. Next, these partial sums are aggregated at the driver to obtain the total sum. The ability to cache RDDs and process them in parallel are the two of the main features of Spark that allows us to perform large scale, sophisticated analysis.</p>\n\n<h3 style=\"text-align: left;\">2. Computing and Plotting the Frequency of Each Response Code</h3>\n<p style=\"text-align: left;\">We compute these counts using a map-reduce pattern. In particular, the code snippet returns an RDD (<code>response_code_to_count_pair_rdd</code>) of tuples, where each tuple associates a response code with its count.</p>\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.07.31-PM.png\"><img class=\"aligncenter wp-image-3570\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.07.31-PM-1024x186.png\" alt=\"Apache how-to blog figure 6\" width=\"700\" height=\"127\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 6: Counting the response codes using a map-reduce pattern</em></p>\nNext, we take the first 100 tuples from <code>response_code_to_count_pair_rdd</code> to filter out possible bad data, and store the result in another RDD, <code>response_code_to_count_array</code>.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.11.20-PM.png\"><img class=\"aligncenter wp-image-3571\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.11.20-PM-1024x163.png\" alt=\"Apache log how-to figure 7\" width=\"700\" height=\"112\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 7: Filter out possible bad data with take()</em></p>\nTo plot data we convert the <code>response_code_to_count_array</code> RDD into a DataFrame. A DataFrame is basically a table, and it is very similar to the DataFrame abstraction in the popular <a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.html\">Python’s pandas package</a>. The resulting DataFrame (<code>response_code_to_count_data_frame</code>) has two columns “response code” and “count”.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.13.41-PM.png\"><img class=\"aligncenter wp-image-3572\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.13.41-PM-1024x133.png\" alt=\"Apache log how-to figure 8\" width=\"700\" height=\"91\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 8: Converting RDD to DataFrame for easy data manipulation and visualization</em></p>\nNow we can plot the count of response codes by simply invoking<code> display()</code> on our data frame.\n<p style=\"text-align: left;\"><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.15.56-PM.png\"><img class=\"aligncenter wp-image-3573\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.15.56-PM-1024x484.png\" alt=\"Apache log how-to figure 9\" width=\"700\" height=\"331\" /></a></p>\n<p style=\"text-align: center;\"><em>Figure 9: Visualizing response codes with display()</em></p>\nIf you want to change the plot size you can do so interactively by just clicking on the down arrow below the plot, and select another plot type. To illustrate this capability, below we show the same data using a pie-chart.\n<p style=\"text-align: left;\"><em><a href=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.17.25-PM.png\"><img class=\"aligncenter wp-image-3574\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-20-at-8.17.25-PM-1024x505.png\" alt=\"Apache log how-to figure 10.1\" width=\"700\" height=\"345\" /></a></em></p>\n<p style=\"text-align: center;\"><em>Figure 10: Changing the visualization of response codes to a pie chart</em></p>\n\n<h2>Additional Resources</h2>\n<p style=\"text-align: left;\">If you’d like to analyze your Apache access logs with Databricks Cloud, you can<a href=\"https://databricks.com/registration\"> register</a> here for an account. You can also find the source code on Github <a href=\"https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/python/databricks/apps/logs\" target=\"_blank\">here</a>.</p>\n<p style=\"text-align: left;\">Other Databricks Cloud how-tos can be found at:</p>\n\n<ul>\n \t<li style=\"text-align: left;\"><a title=\"The Easiest Way to Run Spark Jobs\" href=\"https://databricks.com/blog/2015/04/16/the-easiest-way-to-run-spark-jobs.html\" target=\"_blank\">Easiest way to run Spark jobs</a></li>\n</ul></td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-04-23, 2015-04-23, UTC)</td><td>This is a guest blog from our one of our partners: <a href=\"http://www.lynxanalytics.com/\" target=\"_blank\">Lynx Analytics</a>\n\n<hr />\n\n<h2> About Lynx Analytics</h2>\nLynx Analytics is a data analytics consultancy firm with a focus on graph analytics and proprietary big graph analytics software development. We augment classical data mining methods with our expertise in graph analytics, and apply these methods against large datasets such as call data records, bank transactions, and cell tower usage.\n\nApplying the graph analytics can reveal unexpected patterns about human behavior, emergent properties of customer interactions, untapped market opportunities, to name just a few examples. Graph analysis is also often the only way to establish relationships between diverse datasets, leading to complex insights that were unattainable by analyzing standalone datasets.\n\nOur clients are large multinational telecommunications and financial corporations. Due to the enormous volume of data we needed a scalable solution to perform exploratory, interactive graph data analysis. The existing solutions did not fulfill the needs of our analysts and clients, so we leveraged the power of Apache Spark to develop the LynxKite graph analytics platform.\n<h2>Why choose Spark?</h2>\nThe LynxKite graph analytics platform is a web application with a rich, clean UI for exploring and manipulating graphs. One critical requirement is to allow the users to work with extremely large dataset sizes interactively in real-time. After evaluating several distributed computation frameworks we found Apache Spark to best fulfill our needs for low latency, ease of use, and production readiness.\n\nLeveraging the power of Spark, LynxKite takes just a few clicks to bucket the customers by age and gender, and visualize the number of calls within and between the buckets. Within a minute, the overlapping communities can be identified in the graph, and for each customer we can find the average age of the most homogeneous community they belong to.\n<h2>The Architecture &amp; Benefits to Users</h2>\nThe LynxKite frontend is an AngularJS web application running in the browser. It is served by a Play Framework web server, which also receives the AJAX requests from the frontend. The web server process is also the Apache Spark driver application and is connected to our Apache Spark cluster. When the frontend requests new data, such as an aggregate view of the graph, the computations run on the Apache Spark cluster.\n<h2>The highlights of this technical solution are:</h2>\n<ul>\n \t<li><b>Latency is low. </b>Many computations complete in less than a second, which is a dream come true for our users. The more computationally intensive operations can be sped up by increasing the cluster size. When the cluster is hosted by a cloud provider, its size can be easily adjusted to fit the needs of the moment.</li>\n \t<li><b>Simple is easy. </b>The clean and flexible Apache Spark Scala API allows us to implement graph analytics methods in a very simple and natural way. To quantify this, we compared our solution to open ­source implementations (on other frameworks) of common graph algorithms, to see a ten­fold advantage of the Spark solution with respect to code complexity.</li>\n \t<li><b>Complex is possible. </b>The ease of developing with the Apache Spark Scala API has enabled us to create a great variety of more complex analytic methods. Viral modeling can estimate unobserved properties based on a small proportion of nodes with observed properties, using the link structure between them. Time and space can be visualized to explore the diffusion of product usage or understand geographic data on a map.</li>\n \t<li><b>Deployments are smooth.</b> We are deploying LynxKite into the private Hadoop clusters of a number of clients. This is a surprisingly straightforward process thanks to the integration of Apache Spark into the Hadoop ecosystem.</li>\n</ul>\n<h2>What’s Next</h2>\nBetting big on a pioneering technology such as Apache Spark, we really relied on its developer community. We found the Spark community to be extremely smart, professional, and responsive to our questions, tickets, and pull requests. We are very thankful and hope to contribute more.\n\nWe look forward to our presence  at<a href=\"http://spark-summit.org/2015\" target=\"_blank\"> Spark Summit West 2015</a> (San Francisco, June 15–17) where we will talk about the technical challenges of running Apache Spark in an interactive setting.\n\nFind out more at <a href=\"http://www.lynxanalytics.com/\" target=\"_blank\">www.lynxanalytics.com</a>\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-04-24, 2015-04-24, UTC)</td><td>In this post, we look back and cover recent performance efforts in Apache Spark. In a follow-up blog post next week, we will look forward and share with you our thoughts on the future evolution of Spark's performance.\n\n2014 was the most active year of Spark development to date, with major improvements across the entire engine. One particular area where it made great strides was performance: Spark <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">set a new world record in 100TB sorting</a>, beating the previous record held by Hadoop MapReduce by three times, using only one-tenth of the resources; it received a new <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">SQL query engine</a> with a state-of-the-art optimizer; and many of its built-in algorithms became <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">five times faster</a>.\n\nBack in 2010, we at the AMPLab at UC Berkeley designed Spark for interactive queries and iterative algorithms, as these were two major use cases not well served by batch frameworks like MapReduce. As a result, early users were drawn to Spark because of the significant performance improvements in these workloads. However, performance optimization is a never-ending process, and as Spark’s use cases have grown, so have the areas looked at for further improvement. User feedback and detailed measurements helped the Apache Spark developer community to prioritize areas to work in. Starting with the core engine, I will cover some of the recent optimizations that have been made.\n\n[caption id=\"attachment_2822\" align=\"aligncenter\" width=\"600\"]<a href=\"https://databricks.com/wp-content/uploads/2014/11/Spark_Ecosystem_Chart11.jpg\"><img class=\"wp-image-2822\" src=\"https://databricks.com/wp-content/uploads/2014/11/Spark_Ecosystem_Chart11-1024x414.jpg\" alt=\"The Spark ecosystem\" width=\"600\" height=\"243\" /></a> The Spark ecosystem[/caption]\n<h2>Core engine</h2>\nOne unique thing about Spark is its user-facing APIs (SQL, streaming, machine learning, etc.) run over a common core execution engine. Whenever possible, specific workloads are sped up by making optimizations in the core engine. As a result, these optimizations speed up <em>all</em> components. We’ve often seen very surprising results this way: for example, when core developers decreased latency to introduce Spark Streaming, we also saw SQL queries become two times faster.\n\nIn the core engine, the major improvements in 2014 were in communication. First, <em>shuffle</em> is the operation that moves data point-to-point across machines. It underpins almost all workloads. For example, a SQL query joining two data sources uses shuffle to move tuples that should be joined together onto the same machine, and product recommendation algorithms such as ALS use shuffle to send user/product weights across the network.\n\nThe last two releases of Spark featured a new sort-based shuffle layer and a new network layer based on <a href=\"http://en.wikipedia.org/wiki/Netty_%28software%29\">Netty</a> with zero-copy and explicit memory management. These two make Spark more robust in very large-scale workloads. In our own experiments at Databricks, we have used this to run petabyte shuffles on 250,000 tasks. These two changes were also the key to Spark <a href=\"https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\">setting the current world record in large-scale sorting</a>, beating the previous Hadoop-based record by 30 times in per-node performance.\n\nIn addition to shuffle, core developers rewrote Spark’s <em>broadcast</em> primitive to use a BitTorrent-like protocol to reduce network traffic. This speeds up workloads that need to send a large parameter to multiple machines, including SQL queries and many machine learning algorithms. We have seen more than <a href=\"https://github.com/apache/spark/pull/3417\">five times performance improvements</a> for these workloads.\n<h2>Python API (PySpark)</h2>\nPython is perhaps the most popular programming language used by data scientists. The Spark community views Python as a first-class citizen of the Spark ecosystem. When it comes to performance, Python programs historically lag behind their JVM counterparts due to the more dynamic nature of the language.\n\nSpark’s core developers have worked extensively to bridge the performance gap between JVM languages and Python. In particular, PySpark can now run on <em>PyPy</em> to leverage the just-in-time compiler, in some cases <a href=\"https://github.com/apache/spark/pull/2144\">improving performance by a factor of 50</a>. The way Python processes communicate with the main Spark JVM programs have also been redesigned to enable <em>worker reuse</em>. In addition, broadcasts are handled via a more optimized serialization framework, enabling PySpark to broadcast data larger than 2GB. The latter two have made general Python program performance two to 10 times faster.\n<h2>SQL</h2>\nOne year ago, Shark, an earlier SQL on Spark engine based on Hive, was deprecated and we at Databricks built a new query engine based on a new query optimizer, <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">Catalyst</a>, designed to run natively on Spark. It was a controversial decision, within the Apache Spark developer community as well as internally within Databricks, because building a brand new query engine necessitates astronomical engineering investments. One year later, more than 115 open source contributors have joined the project, making it one of the most active open source query engines.\n\n[caption id=\"attachment_3624\" align=\"aligncenter\" width=\"570\"]<img class=\"size-full wp-image-3624\" src=\"https://databricks.com/wp-content/uploads/2015/04/sparksql-tpcds-perf.jpg\" alt=\"Shark vs. Spark SQL\" width=\"570\" height=\"197\" /> Shark vs. Spark SQL[/caption]\n\nDespite being less than a year old, Spark SQL is outperforming Shark on almost all benchmarked queries. In TPC-DS, a decision-support benchmark, Spark SQL is outperforming Shark often by an order of magnitude, due to <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">better optimizations and code generation</a>.\n<h2>Machine learning (MLlib) and Graph Computation (GraphX)</h2>\nFrom early on, Spark was packaged with powerful standard libraries that can be optimized along with the core engine. This has allowed for a number of rich optimizations to these libraries. For instance, Spark 1.1 featured a new communication pattern for aggregating machine learning models using <a href=\"https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html\">multi-level aggregation trees</a>. This has reduced the model aggregation time by an order of magnitude. This new communication pattern, coupled with the more efficient broadcast implementation in core, results in speeds 1.5 to five times faster across all algorithms.\n\n<img class=\"aligncenter size-full wp-image-3623\" src=\"https://databricks.com/wp-content/uploads/2015/04/mllib-perf.jpg\" alt=\"mllib-perf\" width=\"570\" height=\"258\" />\n\nIn addition to optimizations in communication, <em>Alternative Least Squares (ALS)</em>, a common collaborative filtering algorithm, was also re-implemented 1.3, which provided another factor of two speedup for ALS over what the above chart shows. In addition, all the built-in algorithms in GraphX have also seen 20% to 50% runtime performance improvements, due to a new optimized API.\n<h2>DataFrames: Leveling the Field for Python and JVM</h2>\nIn Spark 1.3, we introduced a <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">new <em>DataFrame</em> API</a>. This new API makes Spark programs more concise and easier to understand, and at the same time exposes more application semantics to the engine. As a result, Spark can use Catalyst to optimize these programs.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\"><img class=\"aligncenter wp-image-2767\" src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM-1024x457.png\" alt=\"DataFrame performance\" width=\"600\" height=\"268\" /></a>\n\nThrough the new DataFrame API, Python programs can achieve the same level of performance as JVM programs because the Catalyst optimizer compiles DataFrame operations into JVM bytecode. Indeed, performance sometimes beats hand-written Scala code.\n\nThe Catalyst optimizer will also become smarter over time, picking better logical optimizations and physical execution optimizations. For example, in the future, Spark will be able to leverage schema information to create a custom physical layout of data, improving cache locality and reducing garbage collection. This will benefit both Spark SQL and DataFrame programs. As more libraries are converting to use this new DataFrame API, they will also automatically benefit from these optimizations.\n\nThe goal of Spark is to offer a single platform where users can get the best distributed algorithms for any data processing task. We will continue to push the boundaries of performance, making Spark faster and more powerful for more users.\n\nNote: An earlier version of this blog post appeared on <a href=\"http://radar.oreilly.com/2015/02/recent-performance-improvements-in-apache-spark.html\">O'Reilly Radar</a>.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-04-28, 2015-04-28, UTC)</td><td>In a previous <a href=\"https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html\">blog post</a>, we looked back and surveyed performance improvements made to Apache Spark in the past year. In this post, we look forward and share with you the next chapter, which we are calling <i>Project Tungsten. </i>2014 witnessed Spark setting the world record in large-scale sorting and saw major improvements across the entire engine from Python to SQL to machine learning. Performance optimization, however, is a never ending process.\n\nProject Tungsten will be the largest change to Spark’s execution engine since the project’s inception. It focuses on substantially improving the efficiency of <i>memory and CPU</i> for Spark applications, to push performance closer to the limits of modern hardware. This effort includes three initiatives:\n<ol>\n \t<li><i>Memory Management and Binary Processing:</i> leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n \t<li><i>Cache-aware computation</i>: algorithms and data structures to exploit memory hierarchy</li>\n \t<li><i>Code generation</i>: using code generation to exploit modern compilers and CPUs</li>\n</ol>\nThe focus on CPU efficiency is motivated by the fact that Spark workloads are increasingly bottlenecked by CPU and memory use rather than IO and network communication. This trend is shown by recent research on the performance of big data workloads (<a href=\"https://kayousterhout.github.io/trace-analysis/\">Ousterhout et al</a>) and we’ve arrived at similar findings as part of our ongoing tuning and optimization efforts for <a href=\"https://databricks.com/product/databricks-cloud\">Databricks Cloud</a> customers.\n\nWhy is CPU the new bottleneck? There are many reasons for this. One is that hardware configurations offer increasingly large aggregate IO bandwidth, such as 10Gbps links in networks and high bandwidth SSD’s or striped HDD arrays for storage. From a software perspective, Spark’s optimizer now allows many workloads to avoid significant disk IO by pruning input data that is not needed in a given job. In Spark’s shuffle subsystem, serialization and hashing (which are CPU bound) have been shown to be key bottlenecks, rather than raw network throughput of underlying hardware. All these trends mean that Spark today is often constrained by CPU efficiency and memory pressure rather than IO.\n<h2>1. Memory Management and Binary Processing</h2>\nApplications on the JVM typically rely on the JVM’s garbage collector to manage memory. The JVM is an impressive engineering feat, designed as a general runtime for many workloads. However, as Spark applications push the boundary of performance, the overhead of JVM objects and GC becomes non-negligible.\n\nJava objects have a large inherent memory overhead. Consider a simple string “abcd” that would take 4 bytes to store using UTF-8 encoding. JVM’s native String implementation, however, stores this differently to facilitate more common workloads. It encodes each character using 2 bytes with UTF-16 encoding, and each String object also contains a 12 byte header and 8 byte hash code, as illustrated by the following output from the the <a href=\"http://openjdk.java.net/projects/code-tools/jol/\">Java Object Layout</a> tool.\n<pre>java.lang.String object internals:\nOFFSET  SIZE   TYPE DESCRIPTION                    VALUE\n     0     4        (object header)                ...\n     4     4        (object header)                ...\n     8     4        (object header)                ...\n    12     4 char[] String.value                   []\n    16     4    int String.hash                    0\n    20     4    int String.hash32                  0\nInstance size: 24 bytes (reported by Instrumentation API)</pre>\nA simple 4 byte string becomes over 48 bytes in total in the JVM object model!\n\nThe other problem with the JVM object model is the overhead of garbage collection. At a high level, generational garbage collection divides objects into two categories: ones that have a high rate of allocation/deallocation (the young generation) ones that are kept around (the old generation). Garbage collectors exploit the transient nature of young generation objects to manage them efficiently. This works well when GC can reliably estimate the life cycle of objects, but falls short if the estimation is off (i.e. some transient objects spill into the old generation). Since this approach is ultimately based on heuristics and estimation, eeking out performance can require the “black magic” of GC tuning, with <a href=\"http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html\">dozens of parameters</a> to give the JVM more information about the life cycle of objects.\n\nSpark, however, is not just a general-purpose application. Spark understands how data flows through various stages of computation and the scope of jobs and tasks. As a result, Spark knows much more information than the JVM garbage collector about the life cycle of memory blocks, and thus should be able to manage memory more efficiently than the JVM.\n\nTo tackle both object overhead and GC’s inefficiency, we are introducing an explicit memory manager to convert most Spark operations to operate directly against binary data rather than Java objects. This builds on <code>sun.misc.Unsafe</code>, an advanced functionality provided by the JVM that exposes C-style memory access (e.g. explicit allocation, deallocation, pointer arithmetics). Furthermore, Unsafe methods are <i>intrinsic</i>, meaning each method call is compiled by JIT into a single machine instruction.\n\nIn certain areas, Spark has already started using explicitly managed memory. Last year, Databricks contributed a new Netty-based network transport that explicitly manages all network buffers using a jemalloc like memory manager. That was critical in scaling up Spark’s shuffle operation and winning the Sort Benchmark.\n\nThe first pieces of this will appear in Spark 1.4, which includes a hash table that operates directly against binary data with memory explicitly managed by Spark. Compared with the standard Java <code>HashMap</code>, this new implementation much less indirection overhead and is invisible to the garbage collector.\n\n<img class=\" wp-image-3639 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-27-at-6.08.39-PM-1024x684.png\" alt=\"Screen Shot 2015-04-27 at 6.08.39 PM\" width=\"800\" height=\"534\" />\n\nThis is still work-in-progress, but initial performance results are encouraging. As shown above, we compare the throughput of aggregation operations using different hash map: one with our new hash map’s heap mode, one with offheap, and one with java.util.HashMap. The new hash table supports over 1 million aggregation operations per second in a single thread, about 2X the throughput of java.util.HashMap. More importantly, without tuning any parameters, it has almost no performance degradation as memory utilization increases, while the JVM default one eventually thrashes due to GC.\n\nIn Spark 1.4, this hash map will be used for aggregations for DataFrames and SQL, and in 1.5 we will have data structures ready for most other operations, such as sorting and joins. This will in many cases eliminating the need to tune GC to achieve high performance.\n<h2>2. Cache-aware Computation</h2>\nBefore we explain cache-aware computation, let’s revisit “in-memory” computation. Spark is widely known as an in-memory computation engine. What that term really means is that Spark can leverage the memory resources on a cluster efficiently, processing data at a rate much higher than disk-based solutions. However, Spark can also process data orders magnitude larger than the available memory, transparently spill to disk and perform external operations such as sorting and hashing.\n\nSimilarly, cache-aware computation improves the speed of data processing through more effective use of L1/ L2/L3 CPU caches, as they are orders of magnitude faster than main memory. When profiling Spark user applications, we’ve found that a large fraction of the CPU time is spent waiting for data to be fetched from main memory. As part of Project Tungsten, we are designing cache-friendly algorithms and data structures so Spark applications will spend less time waiting to fetch data from memory and more time doing useful work.\n\nConsider sorting of records as an example. A standard sorting procedure would store an array of pointers to records and use quicksort to swap pointers until all records are sorted. Sorting in general has good cache hit rate due to the sequential scan access pattern. Sorting a list of pointers, however, has a poor cache hit rate because each comparison operation requires dereferencing two pointers that point to randomly located records in memory.\n\n<img class=\"aligncenter wp-image-3640\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-27-at-6.12.51-PM.png\" alt=\"\" width=\"400\" height=\"249\" />\n\nSo how do we improve the cache locality of sorting? A very simple approach is to store the sort key of each record side by side with the pointer. For example, if the sort key is a 64-bit integer, then we use 128-bit (64-bit pointer and 64-bit key) to store each record in the pointers array. This way, each quicksort comparison operation only looks up the pointer-key pairs in a linear fashion and requires no random memory lookup. Hopefully the above illustration gives you some idea about how we can redesign basic operations to achieve higher cache locality.\n\nHow does this apply to Spark? Most distributed data processing can be boiled down to a small list of operations such as aggregations, sorting, and join. By improving the efficiency of these operations, we can improve the efficiency of Spark applications as a whole. We have already built a version of sort that is cache-aware that is 3X faster than the previous version. This new sort will be used in sort-based shuffle, high cardinality aggregations, and sort-merge join operator. By the end of this year, most Spark’s lowest level algorithms will be upgraded to be cache-aware, increasing the efficiency of all applications from machine learning to SQL.\n<h2>3. Code Generation</h2>\nAbout a year ago Spark introduced <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">code generation for expression evaluation</a> in SQL and DataFrames. Expression evaluation is the process of computing the value of an expression (say “<code>age &gt; 35 &amp;&amp; age &lt; 40</code>”) on a particular record. At runtime, Spark dynamically generates bytecode for evaluating these expressions, rather than stepping through a slower interpreter for each row. Compared with interpretation, code generation reduces the boxing of primitive data types and, more importantly, avoids expensive <a href=\"http://shipilev.net/blog/2015/black-magic-method-dispatch/\">polymorphic function dispatches</a>.\n\nIn an <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\">earlier blog post</a>, we demonstrated that code generation could speed up many TPC-DS queries by almost an order of magnitude. We are now broadening the code generation coverage to most built-in expressions. In addition, we plan to increase the level of code generation from <i>record-at-a-time</i> expression evaluation to <i>vectorized</i> expression evaluation, leveraging JIT’s capabilities to exploit better instruction pipelining in modern CPUs so we can process multiple records at once.\n\nWe’re also applying code generation in areas beyond expression evaluations to optimize the CPU efficiency of internal components. One area that we are very excited about applying code generation is to speed up the conversion of data from in-memory binary format to wire-protocol for shuffle. As mentioned earlier, shuffle is often bottlenecked by data serialization rather than the underlying network. With code generation, we can increase the throughput of serialization, and in turn increase shuffle network throughput.\n\n<img class=\" wp-image-3641 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-27-at-6.15.12-PM-1024x383.png\" alt=\"\" width=\"400\" />\n\nThe above chart compares the performance of shuffling 8 million complex rows in one thread using the Kryo serializer and a code generated custom serializer. The code generated serializer exploits the fact that all rows in a single shuffle have the same schema and generates specialized code for that. This made the generated version over 2X faster to shuffle than the Kryo version.\n<h2>Tungsten and Beyond</h2>\nProject Tungsten is a broad initiative that will influence the design of Spark’s core engine over the next several releases. The first pieces will land in Spark 1.4, which includes explicitly managed memory for <a href=\"https://github.com/apache/spark/pull/5725\">aggregation operations</a> in Spark’s DataFrame API as well as <a href=\"https://github.com/apache/spark/pull/5497\">customized serializers</a>. Expanded coverage of binary memory management and cache-aware data structures will appear in Spark 1.5. Several parts of project Tungsten leverage the DataFrame model. We will also retrofit the improvements onto Spark’s RDD API whenever possible.\n\nThere are also a handful of longer term possibilities for Tungsten. In particular, we plan to investigate compilation to LLVM or OpenCL, so Spark applications can leverage SSE/SIMD instructions out of modern CPUs and the wide parallelism in GPUs to speed up operations in machine learning and graph computation.\n\nThe goal of Spark has always been to offer a single platform where users can get the best distributed algorithms for any data processing task. Performance is a key part of that goal, and Project Tungsten aims to let Spark applications run at the speed offered by bare metal. Stay tuned for the Databricks blog for longer term articles on the components of Project Tungsten as they ship. We’ll also be reporting details about the project at the upcoming <a href=\"http://spark-summit.org/2015\">Spark Summit in San Francisco in June</a>.\n\n* Note: hand-drawing diagrams inspired by our friends at Confluent (Martin Kleppmann)</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Events)</td><td>List(2015-05-11, 2015-05-11, UTC)</td><td><a href=\"https://spark-summit.org/\"><img class=\"alignnone wp-image-3755\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-11-at-9.50.26-AM-1024x488.png\" alt=\"Spark Summit - new website\" width=\"800\" height=\"381\" /></a>\n\n&nbsp;\n\nWe’re proud to announce that the new <a href=\"http://spark-summit.org\">Spark Summit website</a> is live! This includes the full list of <a href=\"https://spark-summit.org/2015/schedule/\">community talks</a> along with the first set of <a href=\"https://spark-summit.org/2015/speakers/\">keynotes</a>. With over 260 submissions this year, the Program Committee had its work cut out narrowing the list to 54 talks. We would like to thank everyone who submitted and invite everyone to submit a presentation for a future Spark Summit (just in case you have not heard as yet, we are taking the Spark Summit to Amsterdam this fall.).\n\nThis year, join us at the <a href=\"https://spark-summit.org/2015/venue\">Hilton Union Square</a> where you will hear from thought leaders and practitioners in Data Science, the Spark community and the enterprise. Those looking to get hands-on with Spark are encouraged to sign up for one of our <a href=\"https://spark-summit.org/2015/training/\">workshops</a>.\n\nThe tracks for Spark Summit 2015 are organized by the following three themes:\n<ul>\n\t<li>Developer</li>\n\t<li>Use Cases</li>\n\t<li>Data Science</li>\n</ul>\nTraining tracks include:\n<ul>\n\t<li>Intro to Apache Spark</li>\n\t<li>Advanced: DevOps with Spark</li>\n\t<li>Data Science with Spark</li>\n</ul>\n<a href=\"http://prevalentdesignevents.com/sparksummit2015/registration.aspx\" target=\"_blank\">Register now</a> before the tickets sell out! Use promo code \"<strong>Databricks20</strong>\" to receive 20% off your registration fee.\n<h3>Our stellar keynotes line-up</h3>\n<ul>\n\t<li>Ben Horowitz, Co-founder of Andreessen Horowitz</li>\n\t<li>Doug Wolfe, CIO of the CIA</li>\n\t<li>Ion Stoica, CEO of Databricks</li>\n\t<li>Tim O'Reilly, Founder of O'Reilly Media</li>\n\t<li>Mike Olson, Chief Strategy Officer of Cloudera</li>\n\t<li>Matt Wood, Data Scientist at Amazon Web Services</li>\n\t<li>Matei Zaharia, CTO of Databricks</li>\n\t<li>Beth Smith, General Manager, Analytics Platform at IBM</li>\n\t<li>Brian Kursar, Data Scientist at Toyota</li>\n\t<li>Anil Gadre, SVP of Product Management at MapR</li>\n\t<li>Gloria lau, Data Science at Timeful</li>\n\t<li>Reynold Xin, Co-founder of Databricks</li>\n</ul>\n<h3>Community Talk Highlights</h3>\n<ul>\n\t<li><a href=\"https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/\">SparkR: The Past, the Present and the Future</a> - Shivaram Venkataraman (UC Berkeley AMPLAB), Rui Sun (Intel Asia Pacific R&amp;D)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/Solving-low-latency-query-over-big-data-with-Spark-SQL\">Solving Low Latency Query Over Big Data with Spark SQL</a> - Julien Pierre (Microsoft)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/tagging-and-processing-data-in-real-time-using-spark-streaming/\">Tagging and Processing Data in Real-Time Using Spark Streaming</a> - Hari Shreedharan (Cloudera Inc.), Siddhartha Jain (Salesforce Inc.)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix\">Spark and Spark Streaming at Netflix</a> - Kedar Sadekar (Netflix), Monal Daxini (Netflix)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/hybrid-community-detection-for-web-scale-e-commerce-using-spark-streaming-and-graphx/\">Hybrid Community Detection for Web-scale E-commerce Using Spark Streaming and GraphX</a> - Ming Huang (Taobao)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/use-of-spark-mllib-for-predicting-the-offlining-of-digital-media/\">Use of Spark MLlib for Predicting the Offlining of Digital Media</a> - Christopher Burdorf (NBC Universal)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/appraiser-how-airbnb-generates-complex-models-in-spark-for-demand-prediction/\">Appraiser : How Airbnb Generates Complex Models in Spark for Demand Prediction</a> - Hector Yee (Airbnb)</li>\n\t<li><a href=\"https://spark-summit.org/2015/events/lessons-learned-with-spark-at-the-us-patent-trademark-office/\">Lessons Learned with Spark at the US Patent &amp; Trademark Office</a> - Christopher Bradford (OpenSource Connections)</li>\n</ul>\n<h3>Sponsors</h3>\nThank you to all of our wonderful <a href=\"http://spark-summit.org/2015/sponsors\">sponsors</a> who are helping in numerous ways to bring Spark Summit 2015 to life. You’ve heard this before but without our sponsors, the Summits wouldn’t happen.</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-05-14, 2015-05-14, UTC)</td><td>This is a guest blog from our one of our partners: <a href=\"http://www.nttdata.com/global/en/\" target=\"_blank\">NTT DATA Corporation</a>\n\n<hr />\n\n<h2>About NTT DATA Corporation</h2>\nNTT DATA Corporation is a Japanese IT solution provider and the global IT services arm of NTT (Nippon Telegraph and Telephone Corporation), which ranks among the top 10 telecommunication companies in the world by revenue.\n\nAt NTT DATA, the OSS (Open Source Software) Professional Services Team has the responsibility of providing our customers with consulting, designing, integrating, and supporting services for various OSS products including Apache Hadoop. For these 7+ years we've been integrating dozens of Hadoop systems, which include 1000+ nodes cluster for production use of our customer.\n\nRecently, Apache Spark has become a core component of our development platform and been included in the support service we provide.\n<h2>Why Spark</h2>\nThere are several reasons we make use of Spark. The first is that Spark can be effectively integrated with our existing Hadoop ecosystem, such as HDFS and YARN.  The companies who are well-experienced with Hadoop are able to use Spark on the same Hadoop cluster. The second is that Spark has features useful for data analysts, which traditional Hadoop doesn’t have, such as interactive shell for on demand analysis and high level APIs for complex data analysis.\n\nIn order to validate that Spark has a good balance of high throughput and low latency, we ran some tests on a large cluster and found that Spark could scale to processing tens of TBs of data without unpredictable decrease of performance or stoppages. The detailed result of this validation is shown in the footnoted URL.\n<h2>Our Spark use cases</h2>\nThe followings are some examples of our Spark use cases in production:\n\n<b>Use case 1: The first case is for the analysis of system infrastructures of a Telecom company.</b>\n\n5 years ago, we implemented an on-premises Hadoop cluster consisting of 1000 nodes with NTT DOCOMO, the leading mobile carrier company in Japan. Our emphasis was on achieving high fault tolerance and scalability while computing vast amount of system operation data in the mobile carrier. We were able to achieve our goals without any data loss for 4+ years nevertheless it was difficult to operate Hadoop at this scale at that time.\n\nHowever, we still needed more speed and flexibility for present day requirements. In other words, the demand for a newer parallel distributed processing framework, based on the computational model other than MapReduce was steadily increasing.  To satisfy this condition, we have launched a Spark on YARN cluster with 4000+ cores, evaluated Spark's features and successfully migrated some data processing models to Spark environment.\n\n<b>Use case 2: The second case is for the numerical analysis of massive IoT data gathered from machineries and public infrastructures.</b>\n\nWe are supporting this customer to establish the platform for data analysis, using a statistical approach. One important requirement of this project is to iterate trial-and-error workflows rapidly in order to find algorithms optimized for dynamically changing situations. In general, Spark excels in this type of use case.\n\nIn this project, Spark applications are combined with Hadoop HDFS, the stable data storage, and YARN, the resource management service for distributed computing. One important advantage of using YARN is that we can use multiple versions of Spark on the same cluster at the same time. For example, we can try an application built for the latest version of Spark, which has many useful new features, while another application built for older version of Spark is already running on the cluster.\n<h2>Our future with Spark</h2>\nNTT DATA has become the No 1 Spark contributing company in Japan. Based on the above mentioned production cases and our long experiences as a Hadoop integrator, we regularly provide input and feedback to the Spark development community. Our main focus of contribution is to improve usability. For example, we are developing a Web-based debugging tool \"Timeline Viewer\", which has been contributed to the community. We can easily understand when/where tasks are assigned in chronological order and how long it took with Timeline Viewer.\n<h2>Conclusion</h2>\nWe found that Spark has flexible features and good performance for production use from our deployments. When technical issues arise, NTT DATA has been able to resolve them quickly and contribute the solutions back to the Spark community.  Based on our experiences for distributed computing and open source software, NTT DATA has begun to actively deploy Spark for our customers.\n\nBy taking advantage of open source software, we provide capability to establish variety of data processing systems, such as strict data management, batch processing, data analysis, stream processing, visualization, etc. We believe that using open source software actively triggers “the open innovation” and Spark is one core component for this concept.\n<h2>Additional Resources</h2>\nFor more information of our validation of Spark, please refer to Masaru Dobashi's <a href=\"http://spark-summit.org/2014/talk/spark-on-large-hadoop-cluster-and-evaluation-from-the-view-point-of-enterprise-hadoop-user-and-developer\" target=\"_blank\">recent presentation video and slides</a> at the Spark Summit 2014.\n\nAnd also please refer to Satoshi Tanaka's (from NTT DOCOMO) <a href=\"http://www.slideshare.net/hadoopconf/hadoopspark-hadoop-conference-japan-2014\" target=\"_blank\">slides</a> in Japanese at Hadoop Conference Japan 2014.\n\nTimeline Viewer is proposed in <a href=\"https://issues.apache.org/jira/browse/SPARK-3468\" target=\"_blank\">this ticket</a>.</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-05-28, 2015-05-28, UTC)</td><td>This is a guest post from our friends in the SSG STO Big Data Technology group at Intel.\n\n<a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Join us at the Spark Summit</a> to hear from Intel and other companies deploying Apache Spark in production.  Use the code <em>Databricks20</em> to receive a 20% discount!\n\n<hr />\n\nApache Spark is gaining wide industry adoption due to its superior performance, simple interfaces, and a rich library for analysis and calculation. Like many projects in the big data ecosystem, Spark runs on the Java Virtual Machine (JVM). Because Spark can store large amounts of data in memory, it has a major reliance on Java’s memory management and garbage collection (GC). New initiatives like <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Project Tungsten</a> will simplify and optimize memory management in future Spark versions. But today, users who understand Java’s GC options and parameters can tune them to eek out the best the performance of their Spark applications. This article describes how to configure the JVM's garbage collector for Spark, and gives actual use cases that explain how to tune GC in order to improve Spark’s performance. We look at key considerations when tuning GC, such as collection throughput and latency.\n<h2>Introduction to Spark and Garbage Collection</h2>\nWith Spark being widely used in industry, Spark applications’ stability and performance tuning issues are increasingly a topic of interest. Due to Spark’s memory-centric approach, it is common to use 100GB or more memory as heap space, which is rarely seen in traditional Java applications. In working with large companies using Spark, we receive plenty of concerns about the various challenges surrounding GC during execution of Spark applications. For example, garbage collection takes a long time, causing program to experience long delays, or even crash in severe cases. In this article, we use real examples, combined with the specific issues, to discuss GC tuning methods for Spark applications that can alleviate these problems.\n\nJava applications typically use one of two garbage collection strategies: Concurrent Mark Sweep (CMS) garbage collection and ParallelOld garbage collection. The former aims at lower latency, while the latter is targeted for higher throughput. Both strategies have performance bottlenecks: CMS GC does not do compaction[1], while Parallel GC performs only whole-heap compaction, which results in considerable pause times. At Intel, we advise our customers to choose the strategy which best suits a given application’s requirements. For applications with real-time response, we generally recommend CMS GC; for off-line analysis programs, we use Parallel GC.\n\nSo for a computing framework such as Spark that supports both streaming computing and traditional batch processing, can we find an optimal collector? The Hotspot JVM version 1.6 introduced a third option for garbage collections: the Garbage-First GC (G1 GC). The G1 collector is planned by Oracle as the long term replacement for the CMS GC. Most importantly, the G1 collector aims to achieve both high throughput and low latency. Before we go into details on using the G1 collector with Spark, let’s go over some background on Java GC fundamentals.\n<h2>How Java’s Garbage Collectors Work</h2>\nIn traditional JVM memory management, heap space is divided into Young and Old generations. The young generation consists of an area called Eden along with two smaller survivor spaces, as shown in Figure 1. Newly created objects are initially allocated in Eden. Each time a <i>minor GC</i> occurs, the JVM copies live objects in Eden to an empty survivor space and also copies live objects in the other survivor space that is being used to that empty survivor space. This approach leaves one of the survivor spaces holding objects, and the other empty for the next collection. Objects that have survived some number of minor collections will be copied to the old generation. When the old generation fills up, a <i>major GC</i> will suspend all threads to perform full GC, namely organizing or removing objects in the old generation. This execution pause when all threads are suspended is called Stop-The-World (STW), which sacrifices performance in most GC algorithms. [2]\n\n<em>Figure 1 Generational Hotspot Heap Structure [2] **</em>\n\n<img class=\"aligncenter wp-image-3815\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-11.35.50-AM-1024x302.png\" alt=\"Figure 1Generational Hotspot Heap Structure [2] ** \" width=\"600\" height=\"177\" />\n\nJava’s newer G1 GC completely changes the traditional approach. The heap is partitioned into a set of equal-sized heap regions, each a contiguous range of virtual memory (Figure 2). Certain region sets are assigned the same roles (Eden, survivor, old) as in the older collectors, but there is not a fixed size for them. This provides greater flexibility in memory usage. When an object is created, it is initially allocated in an available region. When the region fills up, JVM creates new regions to store objects. When minor GC occurs, G1 copies live objects from one or more regions of the heap to a single region on the heap, and select a few free new regions as Eden regions. Full GC occurs only when all regions hold live objects and no full-empty region can be found. G1 uses the Remembered Sets (RSets) concept when marking live objects. RSets track object references into a given region by external regions. There is one RSet per region in the heap. The RSet avoids whole-heap scan, and enables the parallel and independent collection of a region. In this context, we can see that G1 GC not only greatly improves heap occupancy rate when full GC is triggered, but also makes the minor GC pause times more controllable, thereby is very friendly for large memory environment. How do these disruptive improvements change GC performance? Here we use the easiest way to observe the performance changes, i.e. by migrating from old GC settings to G1 GC settings. [3]\n\n<em>Figure 2 Illustration for G1 Heap Structure [3]**</em>\n\n<img class=\"aligncenter wp-image-3817\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-11.38.37-AM.png\" alt=\"Figure 2Illustration for G1 Heap Structure [3]** \" width=\"400\" height=\"218\" />\n\nSince G1 gives up the approach of using fixed heap partitions for young/aged objects, we have to adjust GC configuration options accordingly to safeguard the smooth running of the application with G1 collector. Unlike with older garbage collectors, we’ve generally found that a good starting point with the G1 collector is not to perform <i>any</i> tuning. So we recommend beginning only with the default settings and simply enabling G1 via the <code>-XX:+UseG1GC </code>option. One tweak we have found sometimes useful is that, when an application uses multiple threads, it is best to use <code>-XX: -ResizePLAB</code> to close <code>PLAB()</code> resize and avoid performance degradation caused by a large number of thread communications.\n\nFor a complete list of GC parameters supported by Hotspot JVM, you can use the parameter <code>-XX: +PrintFlagsFinal</code> to print out the list, or refer to the Oracle official documentation for explanations on part of the parameters.\n<h2>Understanding Memory Management in Spark</h2>\nA Resilient Distributed Dataset (RDD) is the core abstraction in Spark. Creation and caching of RDD’s closely related to memory consumption. Spark allows users to persistently cache data for reuse in applications, thereby avoid the overhead caused by repeated computing. One form of persisting RDD is to cache all or part of the data in JVM heap. Spark’s executors divide JVM heap space into two fractions: one fraction is used to store data persistently cached into memory by Spark application; the remaining fraction is used as JVM heap space, responsible for memory consumption during RDD transformation. We can adjust the ratio of these two fractions using the <code>spark.storage.memoryFraction</code> parameter to let Spark control the total size of the cached RDD by making sure it doesn’t exceed RDD heap space volume multiplied by this parameter’s value. The unused portion of the RDD cache fraction can also be used by JVM. Therefore, GC analysis for Spark applications should cover memory usage of both memory fractions.\n\nWhen an efficiency decline caused by GC latency is observed, we should first check and make sure the Spark application uses the limited memory space in an effective way. The less memory space RDD takes up, the more heap space is left for program execution, which increases GC efficiency; on the contrary, excessive memory consumption by RDDs leads to significant performance loss due to a large number of buffered objects in the old generation. Here we expand on this point with a use case:\n\nFor example, the user has an application based on the Bagel component of Spark, which performs simple iterative computing. The result of one superstep (iteration) depends on that of the previous superstep, so the result of each superstep will be persisted in memory space. During program execution, we observed that when the number of iterations increase, the memory space used by progress grows rapidly, causing GC to get worse. When we looked closely at Bagel, we discovered that it caches the RDDs of each superstep in memory without freeing them up over time, even though they are not used after a single iteration. This leads to a memory consumption growth that triggers more GC attempts. We removed this unnecessary caching in <a href=\"https://issues.apache.org/jira/i#browse/SPARK-2661?issueKey=SPARK-2661&amp;serverRenderedViewIssue=true\">SPARK-2661</a>. After this modification cache, RDD size stabilizes after three iterations and cache space is now effectively controlled (as shown in Table 1). As a result, GC efficiency has been greatly improved, with the total running time of the program shortened by 10%~20%.\n\n<em>Table 1: Comparison of Bagel Application’s RDD Cache Sizes before and after Optimization</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<th>Iteration Number</th>\n<th>Cache Size of each Iteration</th>\n<th>Total Cache Size (Before Optimization)</th>\n<th>Total Cache Size (After Optimization)</th>\n</tr>\n<tr>\n<td>Initialization</td>\n<td>4.3GB</td>\n<td>4.3GB</td>\n<td>4.3GB</td>\n</tr>\n<tr>\n<td>1</td>\n<td>8.2GB</td>\n<td>12.5GB</td>\n<td>8.2GB</td>\n</tr>\n<tr>\n<td>2</td>\n<td>98.8GB</td>\n<td>111.3 GB</td>\n<td>98.8GB</td>\n</tr>\n<tr>\n<td>3</td>\n<td>90.8GB</td>\n<td>202.1 GB</td>\n<td>90.8GB</td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n\n<b><i>Conclusion:</i></b>\n\nWhen GC is observed as too frequent or long lasting, it may indicate that memory space is not used efficiently by Spark process or application. You can improve performance by explicitly cleaning up cached RDD’s after they are no longer needed.\n<h2>Choosing a Garbage Collector</h2>\nIf our application is using memory as efficiently as possible, the next step is to tune our choice of garbage collector. After implementing <a href=\"https://issues.apache.org/jira/i#browse/SPARK-2661?issueKey=SPARK-2661&amp;serverRenderedViewIssue=true\">SPARK-2661</a>, we set up a four-node cluster, assigned an 88GB heap to each executor, and launched Spark in Standalone mode to conduct our experiments. We started with the default Spark Parallel GC, and found that because the Spark application’s memory overhead is relatively large and most of the objects cannot be reclaimed in a reasonably short life cycle, the Parallel GC is often trapped in full GC, which brings a decline to performance every time it occurs. To make it worse, Parallel GC provides very limited options for performance tuning, so we can only use some basic parameters to adjust performance, such as the size ratio of each generation, and the number of copies before objects are promoted to the old generation. Since these tuning strategies only postpone full GC, the Parallel GC tuning helps little to long-running applications. Therefore, in this article we do not proceed with the Parallel GC tuning. Table 2 shows the operation of the Parallel GC, and obviously when the full GC is executed the lowest CPU utilization rates occur.\n\n<em>Table 2: Parallel GC Running Status (Before Tuning)</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Configuration Options </b></td>\n<td><code>-XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -Xms88g -Xmx88g</code></td>\n</tr>\n<tr>\n<td><b>Stage*</b></td>\n<td> <img class=\"alignnone wp-image-3833\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-1.42.13-PM1.png\" alt=\"Screen Shot 2015-05-26 at 1.42.13 PM\" width=\"500\" height=\"218\" /></td>\n</tr>\n<tr>\n<td><b>Task*</b></td>\n<td> <img class=\"alignnone wp-image-3837\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-1.56.05-PM.png\" alt=\"Screen Shot 2015-05-26 at 1.56.05 PM\" width=\"502\" height=\"220\" /></td>\n</tr>\n<tr>\n<td><b>CPU*</b></td>\n<td> <img class=\"alignnone wp-image-3838\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-1.57.56-PM.png\" alt=\"Screen Shot 2015-05-26 at 1.57.56 PM\" width=\"493\" height=\"215\" /></td>\n</tr>\n<tr>\n<td><b>Mem*</b></td>\n<td> <img class=\"alignnone wp-image-3839\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-2.00.31-PM.png\" alt=\"Screen Shot 2015-05-26 at 2.00.31 PM\" width=\"493\" height=\"218\" /></td>\n</tr>\n</tbody>\n</table>\nCMS GC cannot do anything to eliminate full GC in this Spark application. Moreover, CMS GC has much longer full GC pause times than Parallel GC, taking a big bite out of the application’s throughput.\n\nNext, we ran our application with default G1 GC configuration. To our surprise, G1 GC also gave unacceptable full GC (see “CPU Utilization” in Table 3, obviously Job 3 paused nearly 100 seconds), and a long pause time significantly dragged down the entire application operation. As shown in Table 4, although the total running time is slightly longer than the Parallel GC, G1 GC’s performance was slightly better than the CMS GC.\n\n<em>Table 3: G1 GC Running Status (Before Tuning)</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Configuration Options</b></td>\n<td>-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms88g -Xmx88g</td>\n</tr>\n<tr>\n<td><b>Stage*</b></td>\n<td> <img class=\"alignnone wp-image-3842\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.02.59-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.02.59 PM\" width=\"500\" height=\"223\" /></td>\n</tr>\n<tr>\n<td><b>Task*</b></td>\n<td> <img class=\"alignnone wp-image-3843\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.03.42-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.03.42 PM\" width=\"500\" height=\"222\" /></td>\n</tr>\n<tr>\n<td><b>CPU*</b></td>\n<td> <img class=\"alignnone wp-image-3844\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.04.24-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.04.24 PM\" width=\"509\" height=\"222\" /></td>\n</tr>\n<tr>\n<td><b>Mem*</b></td>\n<td> <img class=\"alignnone wp-image-3845\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.05.59-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.05.59 PM\" width=\"505\" height=\"215\" /></td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n\n<em>Table 4 Comparison of Three Garbage Collectors’ Program Running Time (88GB Heap before tuning)</em>\n<table class=\"table\">\n<tbody>\n<tr>\n<th>Garbage Collector</th>\n<th>Running Time for 88GB Heap</th>\n</tr>\n<tr>\n<td>Parallel GC</td>\n<td>6.5min</td>\n</tr>\n<tr>\n<td>CMS GC</td>\n<td>9min</td>\n</tr>\n<tr>\n<td>G1 GC</td>\n<td>7.6min</td>\n</tr>\n</tbody>\n</table>\n<h2>Tuning The G1 Collector Based on Logs[4][5]</h2>\nAfter we set up G1 GC, the next step is to further tune the collector performance based on GC log.\n\nFirst of all, we want JVM to record more details in GC log. So for Spark, we set “spark.executor.extraJavaOptions” to include additional flags. In general, we need to set such options:\n\n<code>-XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark</code>\n\nWith these options defined, we keep track of detailed GC log and effective GC options in Spark's executer log (output to <code>$SPARK_HOME/work/$ app_id/$executor_id/stdout</code> at each <i>worker</i> node). Next, we can analyze root cause of the problems according to GC log and learn how to improve the program performance.\n\nLet's take a look at the structure of a G1 GC log as follows, which takes a mixed GC in G1 GC for example.\n<pre>251.354: [G1Ergonomics (Mixed GCs) continue mixed GCs, reason: candidate old regions available, candidate old regions: 363 regions, reclaimable: 9830652576 bytes (10.40 %), threshold: 10.00 %]\n\n[Parallel Time: 145.1 ms, GC Workers: 23]\n\n[GC Worker Start (ms): Min: 251176.0, Avg: 251176.4, Max: 251176.7, Diff: 0.7]\n\n[Ext Root Scanning (ms): Min: 0.8, Avg: 1.2, Max: 1.7, Diff: 0.9, Sum: 28.1]\n\n[Update RS (ms): Min: 0.0, Avg: 0.3, Max: 0.6, Diff: 0.6, Sum: 5.8]\n\n[Processed Buffers: Min: 0, Avg: 1.6, Max: 9, Diff: 9, Sum: 37]\n\n[Scan RS (ms): Min: 6.0, Avg: 6.2, Max: 6.3, Diff: 0.3, Sum: 143.0]\n\n[Object Copy (ms): Min: 136.2, Avg: 136.3, Max: 136.4, Diff: 0.3, Sum: 3133.9]\n\n[Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.3]\n\n[GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 1.9]\n\n[GC Worker Total (ms): Min: 143.7, Avg: 144.0, Max: 144.5, Diff: 0.8, Sum: 3313.0]\n\n[GC Worker End (ms): Min: 251320.4, Avg: 251320.5, Max: 251320.6, Diff: 0.2]\n\n[Code Root Fixup: 0.0 ms]\n\n[Clear CT: 6.6 ms]\n\n[Other: 26.8 ms]\n\n[Choose CSet: 0.2 ms]\n\n[Ref Proc: 16.6 ms]\n\n[Ref Enq: 0.9 ms]\n\n[Free CSet: 2.0 ms]\n\n[Eden: 3904.0M(3904.0M)-&gt;0.0B(4448.0M) Survivors: 576.0M-&gt;32.0M Heap: 63.7G(88.0G)-&gt;58.3G(88.0G)]\n\n[Times: user=3.43 sys=0.01, real=0.18 secs]\n</pre>\nFrom this log, we can see that G1 GC log has a very clear hierarchy. The log lists when and why the pause occurs, and grades time consumption of various threads as well as average and maximum CPU time. Finally, G1 GC lists the cleanup results after this pause, and the total time consumption.\n\nIn our current G1 GC running log, we find a special block like this:\n<pre>(to-space exhausted), 1.0552680 secs]\n\n[Parallel Time: 958.8 ms, GC Workers: 23]\n\n[GC Worker Start (ms): Min: 759925.0, Avg: 759925.1, Max: 759925.3, Diff: 0.3]\n\n[Ext Root Scanning (ms): Min: 1.1, Avg: 1.4, Max: 1.8, Diff: 0.6, Sum: 33.0]\n\n[SATB Filtering (ms): Min: 0.0, Avg: 0.0, Max: 0.3, Diff: 0.3, Sum: 0.3]\n\n[Update RS (ms): Min: 0.0, Avg: 1.2, Max: 2.1, Diff: 2.1, Sum: 26.9]\n\n[Processed Buffers: Min: 0, Avg: 2.8, Max: 11, Diff: 11, Sum: 65]\n\n[Scan RS (ms): Min: 1.6, Avg: 2.5, Max: 3.0, Diff: 1.4, Sum: 58.0]\n\n[Object Copy (ms): Min: 952.5, Avg: 953.0, Max: 954.3, Diff: 1.7, Sum: 21919.4]\n\n[Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 2.2]\n\n[GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.6]\n\n[GC Worker Total (ms): Min: 958.1, Avg: 958.3, Max: 958.4, Diff: 0.3, Sum: 22040.4]\n\n[GC Worker End (ms): Min: 760883.4, Avg: 760883.4, Max: 760883.4, Diff: 0.0]\n\n[Code Root Fixup: 0.0 ms]\n\n[Clear CT: 0.4 ms]\n\n[Other: 96.0 ms]\n\n[Choose CSet: 0.0 ms]\n\n[Ref Proc: 0.4 ms]\n\n[Ref Enq: 0.0 ms]\n\n[Free CSet: 0.1 ms]\n\n[Eden: 160.0M(3904.0M)-&gt;0.0B(4480.0M) Survivors: 576.0M-&gt;0.0B Heap: 87.7G(88.0G)-&gt;87.7G(88.0G)]\n\n[Times: user=1.69 sys=0.24, real=1.05 secs]\n\n760.981: [G1Ergonomics (Heap Sizing) attempt heap expansion, reason: allocation request failed, allocation request: 90128 bytes]\n\n760.981: [G1Ergonomics (Heap Sizing) expand the heap, requested expansion amount: 33554432 bytes, attempted expansion amount: 33554432 bytes]\n\n760.981: [G1Ergonomics (Heap Sizing) did not expand the heap, reason: heap expansion operation failed]\n\n760.981: [Full GC 87G-&gt;36G(88G), 67.4381220 secs]\n</pre>\nAs we can see, the largest performance degradation was caused by such a full GC, and was output in the log as To-space Exhausted, To-space Overflow or the similar (for various JVM versions, the output may look slightly different). The cause is that when the G1 GC collector tries to collect garbage for certain regions, it fails to find free regions which it can copy the live objects to. This situation is called Evacuation Failure and often leads to full GC. And apparently, full GC in G1 GC is even worse than in Parallel GC, so we must try to avoid full GC in order to achieve better performance. To avoid full GC in G1 GC, there are two commonly-used approaches:\n<ol>\n \t<li>Decrease the <code>InitiatingHeapOccupancyPercent</code> option’s value (the default value is 45), to let G1 GC starts initial concurrent marking at an earlier time, so that we are more likely to avoid full GC.</li>\n \t<li>Increase the <code>ConcGCThreads</code> option’s value, to have more threads for concurrent marking, thus we can speed up the concurrent marking phase. Take caution that this option could also take up some effective worker thread resources, depending on your workload CPU utilization.</li>\n</ol>\nTuning these two options minimized the possibility of a full GC occurrences. After full GC was eliminated, performance was increased dramatically. However, we still found long pauses during GC. On further investigation, we found the following occurrence in our logs:\n<pre>280.008: [G1Ergonomics (Concurrent Cycles) request concurrent cycle initiation, reason: occupancy higher than threshold, occupancy: 62344134656 bytes, allocation request: 46137368 bytes, threshold: 42520176225 bytes (45.00 %), source: concurrent humongous allocation]\n</pre>\nHere we see <i>humongous objects</i> (objects that are 50% the size of a standard region or larger). G1 GC would put each of these objects in contiguous set of regions. And since copying these objects would consume a lot of resources, humongous objects are directly allocated out of the old generation (bypassing all young GCs) and then categorized into humongous regions [4]. Before 1.8.0_u40, a complete heap liveness analysis is required to reclaim humongous regions [<a href=\"https://bugs.openjdk.java.net/browse/JDK-8027959\">JDK-8027959</a>]. If there are many objects of this kind, the heap would be filled up very quickly, and to reclaim them is too expensive. Even with the fixes (they do increase the efficiency of reclaiming humongous objects greatly), the allocation of contiguous regions is still more expensive (especially when meeting serious heap fragmentations), so we want to avoid creating objects of this size. We can increase the value of <code>G1HeapRegionSize</code> to reduce possibility of creating humongous regions, but the default value is already at its maximum size of 32M if we are using a comparatively large heap. This means we can only analyze the program to find these objects and to minimize their creation. Otherwise, it likely leads to more concurrent marking phase, and after that, you need to carefully tune mix GC related knobs (e.g., <code>-XX:G1HeapWastePercent -XX:G1MixedGCLiveThresholdPercent</code>) to avoid long mix GC pauses (caused by lots of humongous objects).\n\nNext, we can analyze the interval of a single GC cycle from cycle start until end of mixed GC. If the time is too long, you can consider increasing the value of <code>ConcGCThreads</code>, but note that this will take up more CPU resources.\n\nThe G1 GC also has ways to decrease STW pause length in return for doing more work in the concurrent stage of garbage collection. As mentioned above, G1 GC maintains an Remembered Set(RSet) for each region to track object references into a given region by external regions, and G1 collector updates RSets both at the STW stage and at the concurrent stage. If you are seeking to decrease the length of STW pauses with the G1 GC, you can decrease the value of <code>G1RSetUpdatingPauseTimePercent</code> while increasing the value of <code>G1ConcRefinementThreads</code>. The option <code>G1RSetUpdatingPauseTimePercent</code> is used to specify a desired ratio of RSets update time in total STW time, which is 10-percent by default, and <code>G1ConcRefinementThreads</code> is used to define the number of threads for maintaining RSets during program running. With these two options tuned, we can shift more workloads of RSets updating from STW stage to concurrent stage.\n\nIn addition, for long-running applications, we use the <code>AlwaysPreTouch</code> option, so JVM applies all the memory needed to OS at startup and avoids dynamic applications. This improves runtime performance at the cost of extending the start time.\n\nEventually, after several rounds of GC parameters tuning, we arrived at the results in Table 5. Compared with the previous results, we finally obtained a more satisfactory running efficiency.\n\n<em>Table 5 G1 GC Running Status (after Tuning)</em>\n<table>\n<tbody>\n<tr>\n<td><b>Configuration Options</b></td>\n<td><code>-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms88g -Xmx88g -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThread=20</code></td>\n</tr>\n<tr>\n<td><b>Stage*</b></td>\n<td> <img class=\"alignnone wp-image-3852\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.20.31-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.20.31 PM\" width=\"500\" height=\"188\" /></td>\n</tr>\n<tr>\n<td><b>Task*</b></td>\n<td> <img class=\"alignnone wp-image-3853\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.21.36-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.21.36 PM\" width=\"497\" height=\"193\" /></td>\n</tr>\n<tr>\n<td><b>CPU*</b></td>\n<td> <img class=\"alignnone wp-image-3854\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.22.13-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.22.13 PM\" width=\"500\" height=\"196\" /></td>\n</tr>\n<tr>\n<td><b>Mem*</b></td>\n<td> <img class=\"alignnone wp-image-3855\" src=\"https://databricks.com/wp-content/uploads/2015/05/Screen-Shot-2015-05-26-at-3.22.45-PM.png\" alt=\"Screen Shot 2015-05-26 at 3.22.45 PM\" width=\"500\" height=\"188\" /></td>\n</tr>\n</tbody>\n</table>\n<h4></h4>\n<b><i>Conclusion:</i></b>\n\nWe recommend trying the G1 GC compared with alternatives for Spark applications. Finer-grained optimizations can be obtained through GC log analysis. After tuning, we successfully shortened the running time of the application to 4.3 minutes. Compared with the running time before tuning, we achieved a performance increase of 1.7 times; compared with Parallel GC, an increase by 1.5 times more or less.\n<h2>Summary</h2>\nFor Spark applications which rely heavily on memory computing, GC tuning is particularly important. When problems emerge with GC, do not rush into debugging the GC itself. First consider inefficiency in Spark program’s memory management, such as persisting and freeing up RDD in cache. When tuning garbage collectors, we first recommend using G1 GC to run Spark applications. The G1 collector is well poised to handle growing heap sizes often seen with Spark. With G1, fewer options will be needed to provide both higher throughput and lower latency. Of course, there is no fixed pattern for GC tuning. The various applications have different characteristics, and in order to tackle unpredictable situations, one must master the art of GC tuning according to logs and other forensics. Finally, we cannot forget optimizing through program’s logic and code, such as reducing intermediate object creation or replication, controlling creation of large objects, storing long-lived objects in off-heap, and so on.\n\nBy using the G1 GC we achieved major performance improvements in Spark applications. <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">Future work in Spark</a> will move memory management responsibility away from Java’s garbage collectors and into Spark itself. This will alleviate much of the tuning requirements for Spark applications. Nonetheless, today garbage collector choice can increase performance for mission critical Spark applications.\n<h2>Acknowledgement</h2>\nDuring the tuning practice and writing of this article, we received guidance and assistance from Ms. Yanping Wang, senior engineer from Intel's Java Runtime team.\n\n<b>* Indicates graphs generated using internal Performance Analysis tools developed by Intel Big Data team.</b>\n\n<b>** Indicates images from Oracle documentation. For details, see reference [2] [3]</b>\n<h2>References</h2>\n[1] <a href=\"https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/garbage_collect.html#wp1086917\">https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/garbage_collect.html#wp1086917</a>\n\n[2] <a href=\"http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html\">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html</a>\n\n[3] <a href=\"http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html\" target=\"_blank\">http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html</a>\n\n[4] <a href=\"http://www.infoq.com/articles/tuning-tips-G1-GC\" target=\"_blank\">http://www.infoq.com/articles/tuning-tips-G1-GC</a>\n\n[5] <a href=\"https://blogs.oracle.com/poonam/entry/understanding_g1_gc_logs\">https://blogs.oracle.com/poonam/entry/understanding_g1_gc_logs</a>\n<h2>About the Authors:</h2>\nDaoyuan Wang, Software Engineer from SSG STO Big Data Technology, Intel Asia-Pacific Research &amp; Development Ltd., who is also an active Spark contributor in the Apache community.\n\nJie Huang, engineering manager of SSG STO Big Data Technology, Intel Asia-Pacific Research &amp; Development Ltd.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog)</td><td>List(2015-06-01, 2015-06-01, UTC)</td><td>For the past several months, we have been working in collaboration with professors from the University of California Berkeley and University of California Los Angeles to produce two freely available Massive Open Online Courses (MOOCs). We are proud to announce that both MOOCs will launch in June on the edX platform!\n\nThe first course, called Introduction to Big Data with Apache Spark, begins today and teaches students about Apache Spark and performing data analysis. The second course, called Scalable Machine Learning, will begin on June 29th and will introduce the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using Spark.  Both courses will be freely available on the edX MOOC platform, and edX Verified Certificates are also available for a fee.\n\nStudents have shown an overwhelming interest in these courses, as exhibited by the following enrollment numbers (as of 5/28/15):\n<ul>\n \t<li>over 80K enrolled students</li>\n \t<li>over 4K new enrollments in the past week</li>\n \t<li>nearly 1K Verified Certificate enrollments</li>\n</ul>\nWe would also like to thank the Spark community for their support.  Several community members are serving as teaching assistants and beta testers, and multiple study groups have been organized by community members in anticipation of these courses.\n\nBoth courses are available for free on the edX website, and you can sign up for them today:\n<ol>\n \t<li><a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\">Introduction to Big Data with Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\">Scalable Machine Learning</a></li>\n</ol>\nIt is our mission to enable data scientists and engineers around the world to leverage the power of Big Data, and an important part of this mission is to educate the next generation.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-06-02, 2015-06-02, UTC)</td><td>We <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">introduced DataFrames</a> in Apache Spark 1.3 to make Apache Spark much easier to use. Inspired by data frames in R and Python, DataFrames in Spark expose an API that’s similar to the single-node data tools that data scientists are already familiar with. Statistics is an important part of everyday data science. We are happy to announce improved support for statistical and mathematical functions in the upcoming 1.4 release.\n\nIn this blog post, we walk through some of the important functions, including:\n<ol>\n \t<li>Random data generation</li>\n \t<li>Summary and descriptive statistics</li>\n \t<li>Sample covariance and correlation</li>\n \t<li>Cross tabulation (a.k.a. contingency table)</li>\n \t<li>Frequent items</li>\n \t<li>Mathematical functions</li>\n</ol>\nWe use Python in our examples. However, similar APIs exist for Scala and Java users as well.\n<h2>1. Random Data Generation</h2>\nRandom data generation is useful for testing of existing algorithms and implementing randomized algorithms, such as random projection. We provide methods under sql.functions for generating columns that contains i.i.d. values drawn from a distribution, e.g., uniform (<code>rand</code>),  and standard normal (<code>randn</code>).\n<pre>In [1]: from pyspark.sql.functions import rand, randn\nIn [2]: # Create a DataFrame with one int column and 10 rows.\nIn [3]: df = sqlContext.range(0, 10)\nIn [4]: df.show()\n+--+\n|id|\n+--+\n| 0|\n| 1|\n| 2|\n| 3|\n| 4|\n| 5|\n| 6|\n| 7|\n| 8|\n| 9|\n+--+\n\nIn [4]: # Generate two other columns using uniform distribution and normal distribution.\nIn [5]: df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()\n+--+-------------------+--------------------+\n|id|            uniform|              normal|\n+--+-------------------+--------------------+\n| 0| 0.7224977951905031| -0.1875348803463305|\n| 1| 0.2953174992603351|-0.26525647952450265|\n| 2| 0.4536856090041318| -0.7195024130068081|\n| 3| 0.9970412477032209|  0.5181478766595276|\n| 4|0.19657711634539565|  0.7316273979766378|\n| 5|0.48533720635534006| 0.07724879367590629|\n| 6| 0.7369825278894753| -0.5462256961278941|\n| 7| 0.5241113627472694| -0.2542275002421211|\n| 8| 0.2977697066654349| -0.5752237580095868|\n| 9| 0.5060159582230856|  1.0900096472044518|\n+--+-------------------+--------------------+\n</pre>\n<h2>2. Summary and Descriptive Statistics</h2>\nThe first operation to perform after importing data is to get some sense of what it looks like. For numerical columns, knowing the descriptive summary statistics can help a lot in understanding the distribution of your data. The function <code>describe</code> returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column.\n<pre>In [1]: from pyspark.sql.functions import rand, randn\nIn [2]: # A slightly different way to generate the two random columns\nIn [3]: df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27))\n\nIn [4]: df.describe().show()\n+-------+------------------+-------------------+--------------------+\n|summary|                id|            uniform|              normal|\n+-------+------------------+-------------------+--------------------+\n|  count|                10|                 10|                  10|\n|   mean|               4.5| 0.5215336029384192|-0.01309370117407197|\n| stddev|2.8722813232690143|  0.229328162820653|  0.5756058014772729|\n|    min|                 0|0.19657711634539565| -0.7195024130068081|\n|    max|                 9| 0.9970412477032209|  1.0900096472044518|\n+-------+------------------+-------------------+--------------------+\n</pre>\nIf you have a DataFrame with a large number of columns, you can also run describe on a subset of the columns:\n<pre>In [4]: df.describe('uniform', 'normal').show()\n+-------+-------------------+--------------------+\n|summary|            uniform|              normal|\n+-------+-------------------+--------------------+\n|  count|                 10|                  10|\n|   mean| 0.5215336029384192|-0.01309370117407197|\n| stddev|  0.229328162820653|  0.5756058014772729|\n|    min|0.19657711634539565| -0.7195024130068081|\n|    max| 0.9970412477032209|  1.0900096472044518|\n+-------+-------------------+--------------------+\n</pre>\nOf course, while describe works well for quick exploratory data analysis, you can also control the list of descriptive statistics and the columns they apply to using the normal select on a DataFrame:\n<pre>In [5]: from pyspark.sql.functions import mean, min, max\nIn [6]: df.select([mean('uniform'), min('uniform'), max('uniform')]).show()\n+------------------+-------------------+------------------+\n|      AVG(uniform)|       MIN(uniform)|      MAX(uniform)|\n+------------------+-------------------+------------------+\n|0.5215336029384192|0.19657711634539565|0.9970412477032209|\n+------------------+-------------------+------------------+\n</pre>\n<h2>3. Sample covariance and correlation</h2>\n<a href=\"http://en.wikipedia.org/wiki/Covariance\">Covariance</a> is a measure of how two variables change with respect to each other. A positive number would mean that there is a tendency that as one variable increases, the other increases as well. A negative number would mean that as one variable increases, the other variable has a tendency to decrease. The sample covariance of two columns of a DataFrame can be calculated as follows:\n<pre>In [1]: from pyspark.sql.functions import rand\nIn [2]: df = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n\nIn [3]: df.stat.cov('rand1', 'rand2')\nOut[3]: 0.009908130446217347\n\nIn [4]: df.stat.cov('id', 'id')\nOut[4]: 9.166666666666666\n</pre>\nAs you can see from the above, the covariance of the two randomly generated columns is close to zero, while the covariance of the id column with itself is very high.\n\nThe covariance value of 9.17 might be hard to interpret. <a href=\"http://en.wikipedia.org/wiki/Correlation\">Correlation</a> is a normalized measure of covariance that is easier to understand, as it provides quantitative measurements of the statistical dependence between two random variables.\n<pre>In [5]: df.stat.corr('rand1', 'rand2')\nOut[5]: 0.14938694513735398\n\nIn [6]: df.stat.corr('id', 'id')\nOut[6]: 1.0\n</pre>\nIn the above example, id correlates perfectly with itself, while the two randomly generated columns have low correlation value.\n<h2>4. Cross Tabulation (Contingency Table)</h2>\n<a href=\"http://en.wikipedia.org/wiki/Contingency_table\">Cross Tabulation</a> provides a table of the frequency distribution for a set of variables. Cross-tabulation is a powerful tool in statistics that is used to observe the statistical significance (or independence) of variables. In Spark 1.4, users will be able to cross-tabulate two columns of a DataFrame in order to obtain the counts of the different pairs that are observed in those columns. Here is an example on how to use crosstab to obtain the contingency table.\n<pre>In [1]: # Create a DataFrame with two columns (name, item)\nIn [2]: names = [\"Alice\", \"Bob\", \"Mike\"]\nIn [3]: items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\nIn [4]: df = sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n\nIn [5]: # Take a look at the first 10 rows.\nIn [6]: df.show(10)\n+-----+-------+\n| name|   item|\n+-----+-------+\n|Alice|   milk|\n|  Bob|  bread|\n| Mike| butter|\n|Alice| apples|\n|  Bob|oranges|\n| Mike|   milk|\n|Alice|  bread|\n|  Bob| butter|\n| Mike| apples|\n|Alice|oranges|\n+-----+-------+\n\nIn [7]: df.stat.crosstab(\"name\", \"item\").show()\n+---------+----+-----+------+------+-------+\n|name_item|milk|bread|apples|butter|oranges|\n+---------+----+-----+------+------+-------+\n|      Bob|   6|    7|     7|     6|      7|\n|     Mike|   7|    6|     7|     7|      6|\n|    Alice|   7|    7|     6|     7|      7|\n+---------+----+-----+------+------+-------+\n</pre>\nOne important thing to keep in mind is that the cardinality of columns we run crosstab on cannot be too big. That is to say, the number of distinct “name” and “item” cannot be too large. Just imagine if “item” contains 1 billion distinct entries: how would you fit that table on your screen?!\n<h2>5. Frequent Items</h2>\nFiguring out which items are frequent in each column can be very useful to understand a dataset. In Spark 1.4, users will be able to find the frequent items for a set of columns using DataFrames. We have implemented an <a href=\"http://dx.doi.org/10.1145/762471.762473\">one-pass algorithm</a> proposed by Karp et al. This is a fast, approximate algorithm that always return all the frequent items that appear in a user-specified minimum proportion of rows. Note that the result might contain false positives, i.e. items that are not frequent.\n<pre>In [1]: df = sqlContext.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n\nIn [2]: df.show(10)\n+-+--+-+\n|a| b|c|\n+-+--+-+\n|1| 2|3|\n|1| 2|1|\n|1| 2|3|\n|3| 6|3|\n|1| 2|3|\n|5|10|1|\n|1| 2|3|\n|7|14|3|\n|1| 2|3|\n|9|18|1|\n+-+--+-+\n\nIn [3]: freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)\n</pre>\nGiven the above DataFrame, the following code finds the frequent items that show up 40% of the time for each column:\n<pre>In [4]: freq.collect()[0]\nOut[4]: Row(a_freqItems=[11, 1], b_freqItems=[2, 22], c_freqItems=[1, 3])\n</pre>\nAs you can see, “11” and “1” are the frequent values for column “a”. You can also find frequent items for column combinations, by creating a composite column using the struct function:\n<pre>In [5]: from pyspark.sql.functions import struct\n\nIn [6]: freq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n\nIn [7]: freq.collect()[0]\nOut[7]: Row(ab_freqItems=[Row(a=11, b=22), Row(a=1, b=2)])\n</pre>\nFrom the above example, the combination of “a=11 and b=22”, and “a=1 and b=2” appear frequently in this dataset. Note that “a=11 and b=22” is a false positive.\n<h2>6. Mathematical Functions</h2>\nSpark 1.4 also added a suite of mathematical functions. Users can apply these to their columns with ease. The list of math functions that are supported come from <a href=\"https://github.com/apache/spark/blob/efe3bfdf496aa6206ace2697e31dd4c0c3c824fb/python/pyspark/sql/functions.py#L109\" target=\"_blank\">this file</a> (we will also post pre-built documentation once 1.4 is released). The inputs need to be columns functions that take a single argument, such as <code>cos</code>, <code>sin</code>, <code>floor</code>, <code>ceil</code>. For functions that take two arguments as input, such as <code>pow</code>, <code>hypot</code>, either two columns or a combination of a double and column can be supplied.\n<pre>In [1]: from pyspark.sql.functions import *\nIn [2]: df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10) * 3.14)\n\nIn [3]: # you can reference a column or supply the column name\nIn [4]: df.select(\n   ...:   'uniform',\n   ...:   toDegrees('uniform'),\n   ...:   (pow(cos(df['uniform']), 2) + pow(sin(df.uniform), 2)). \\\n   ...:     alias(\"cos^2 + sin^2\")).show()\n\n+--------------------+------------------+------------------+\n|             uniform|  DEGREES(uniform)|     cos^2 + sin^2|\n+--------------------+------------------+------------------+\n|  0.7224977951905031| 41.39607437192317|               1.0|\n|  0.3312021111290707|18.976483133518624|0.9999999999999999|\n|  0.2953174992603351|16.920446323975014|               1.0|\n|0.018326130186194667| 1.050009914476252|0.9999999999999999|\n|  0.3163135293051941|18.123430232075304|               1.0|\n|  0.4536856090041318| 25.99427062175921|               1.0|\n|   0.873869321369476| 50.06902396043238|0.9999999999999999|\n|  0.9970412477032209| 57.12625549385224|               1.0|\n| 0.19657711634539565| 11.26303911544332|1.0000000000000002|\n|  0.9632338825504894| 55.18923615414307|               1.0|\n+--------------------+------------------+------------------+\n</pre>\n<h2>What’s Next?</h2>\nAll the features described in this blog post will be available in Spark 1.4 for Python, Scala, and Java, to be released in the next few days. If you can’t wait, you can also build Spark from the 1.4 release branch yourself: <a href=\"https://github.com/apache/spark/tree/branch-1.4\">https://github.com/apache/spark/tree/branch-1.4</a>\n\nStatistics support will continue to increase for DataFrames through better integration with Spark MLlib in future releases. Leveraging the existing Statistics package in MLlib, support for feature selection in pipelines, Spearman Correlation, ranking, and aggregate functions for covariance and correlation.\n\nAt the end of the blog post, we would also like to thank Davies Liu, Adrian Wang, and rest of the Spark community for implementing these functions.</td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-06-04, 2015-06-04, UTC)</td><td>Join us at <a href=\"https://spark-summit.org/2015/\">Spark Summit</a> to hear more about new functionalities of Apache Spark.  Use the code <em>Databricks20</em> to receive a 20% discount!\n\n<hr />\n\nAs many data scientists and engineers can attest, the majority of the time is spent not on the models themselves but on the supporting infrastructure.  Key issues include on the ability to easily visualize, share, deploy, and schedule jobs.  More disconcerting is the need for data engineers to re-implement the models developed by data scientists for production.  With Databricks, data scientists and engineers can simplify these logistical issues and spend more of their time focusing on their data problems.\n\n<h2>Simplify Visualization</h2>\n\nAn important perspective for data scientists and engineers is the ability to quickly visualize the data and the model that is generated.  For example, a common issue when working with linear regression is to determine the model’s goodness of fit.  While statistical evaluations such as Mean Squared Error are fundamental, the ability to view the data scatterplot in relation to the regression model is just as important.\n\n&nbsp;\n\n<h3>Training the models</h3>\n\nUsing a dataset comparing the population (x) with label data of median housing prices (y), we can build a linear regression model using Spark MLlib’s Linear Regression with Stochastic Gradient Descent (LinearRegressionWithSGD).   Spark MLlib is a core component of Apache Spark that allows data scientists and data engineers to quickly experiment and build data models - and bring them to production.  Because we are experimenting with SGD, we will need to try out different iterations and learning rates (i.e. alpha or step size).\n\nAn easy way to start experimenting with these models is to create a Databricks notebook in your language of choice (python, scala, Spark SQL) and provide contextual information via markdown text.  The screenshot below is two cells from an example DBC notebook where the top cell contains markdown comments while the bottom cell contains pyspark code to train two models.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-1a.png\"><img class=\"alignnone wp-image-4003\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-1a-1024x321.png\" alt=\"Figure 1a\" width=\"701\" height=\"220\" /></a>\n\n<em><strong>Figure 1:</strong> Screenshot of Databricks Notebook training two models with Linear Regression with SGD</em>\n\n&nbsp;\n\n<h3>Evaluating the models</h3>\n\nOnce the models are trained, with some additional pyspark code, you can quickly calculate the mean squared error of these two models:\n\n<pre>valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\nMSE = valuesAndPreds.(lambda (v, p): (v - p)**2).mean()\nprint(\"Mean Squared Error = \" + str(MSE))</pre>\n\n&nbsp;\n\nThe definition of the models and MSE results are in the table below.\n\n<table style=\"height: 112px;\" width=\"565\">\n<tbody>\n<tr>\n<td></td>\n<td><b># of iterations</b></td>\n<td><b>Step Size</b></td>\n<td><b>MSE</b></td>\n</tr>\n<tr>\n<td><b>Model A</b></td>\n<td>100</td>\n<td>0.01</td>\n<td>1.25095190484</td>\n</tr>\n<tr>\n<td><b>Model B</b></td>\n<td>1500</td>\n<td>0.1</td>\n<td>0.205298649734</td>\n</tr>\n</tbody>\n</table>\n\nWhile the evaluation of statistics most likely indicates that Model B has a better goodness of fit, the ability to visually inspect the data will make it easier to validate these results.\n\n&nbsp;\n\n<h3>Visualizing the models</h3>\n\nWith Databricks, there are numerous visualization options that you can use with your Databricks notebooks.  In addition to the default visualizations automatically available when working with Spark DataFrames, you can also use matplotlib, ggplot, and d3.js - all embedded with the same notebook.\n\nIn our example, we are using ggplot (the python code is below) so we can not only provide a scatter plot of the original dataset (in blue), but also graph line plots of the two models where Model A is in red and Model B is in green.\n\n<pre>p = ggplot(pydf, aes('x','y')) + \\\n    geom_point(color='blue') + \\\n    geom_line(pydf, aes('x','y2'), color='red') + \\\n    geom_line(pydf, aes('x','y3'), color='green')\ndisplay(p)</pre>\n\nEmbedded within the same notebook is the median housing prices ggplot scatterplot figure where the x-axis is the normalized population and y-axis is the normalized median housing price; Model A  is in red while Model B is in green.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-2a.png\"><img class=\"alignnone wp-image-4004\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-2a-1024x745.png\" alt=\"Figure 2a\" width=\"700\" height=\"509\" /></a>\n\n<em><strong>Figure 2:</strong> <i>Screenshot of a ggplot scatterplot embedded within a Databricks notebook</i></em>\n\nAs you can see from the above figure, the green line (Model B) has a better goodness of fit compared to the red line (Model A).  While the evaluation statistics pointed toward this direction, the ability to quickly visualize the data and the models within the same notebook allows the data scientist to spend more time understanding and optimizing their models.\n\n&nbsp;\n\n<h2>Simplify Sharing</h2>\n\nAnother crucial aspect of data sciences is the collaborative effort needed to solve data problems.  With many developers, engineers, and data scientists often working in different time zones, schedules, and/or locations, it is important to have an environment that is designed for collaboration.\n\n&nbsp;\n\n<h3>Portability</h3>\n\nWith Databricks, you can make it easier to collaborate with your team.  You can share your Databricks notebooks by sharing its URL so that any web browser on any device can view your notebooks.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/IMG_1596.png\"><img class=\"alignnone wp-image-4006\" src=\"https://databricks.com/wp-content/uploads/2015/06/IMG_1596-1024x576.png\" alt=\"IMG_1596\" width=\"600\" height=\"337\" /></a>\n\n<i><strong>Figure 3</strong>: Databricks notebook view of a the same linear regression SGD model via matplotlib on an iPhone 6.</i>\n\n&nbsp;\n\n<h3>Non-proprietary</h3>\n\nWhile these notebooks are optimized for Databricks, you can export these notebooks to python, scala, and SQL files so you can use them in your own environments.   A common use-case for this approach is that data scientists and engineers will collaborate and experiment in Databricks and then apply their resulting code into their on-premises environment.\n\n&nbsp;\n\n<h3>Share Definitions</h3>\n\nAs a data scientist or data engineer working with many different datasets, keeping up with all of the changes in schema and locations itself can be a full time job.  To help keep this under control, Databricks includes centralized table definitions. Instead of searching for include files that contain the schema, go the tables tab within Databricks  and you can define all of your tables in one place.   This way as a data engineer updates the schema or source location for these table, these changes are immediately available to all notebooks.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-41.png\"><img class=\"alignnone wp-image-3958\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-41-1024x512.png\" alt=\"Figure 4\" width=\"800\" height=\"400\" /></a>\n\n<i><strong>Figure 4:</strong> View of table definitions (schema and sample data) all from one place.</i>\n\n&nbsp;\n\n<h3>Collaborate</h3>\n\nAs notebooks are being created and shared, users can comment on the code or figures so they can provide input to the notebooks without making any changes to them.  This way you can lock the notebooks to prevent accidental changes and still accept feedback.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-5.png\"><img class=\"alignnone wp-image-3960\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-5-1024x595.png\" alt=\"Figure 5\" width=\"700\" height=\"407\" /></a>\n\n<i><strong>Figure 5:</strong> Users commenting on a Databricks notebook to more easily facilitate feedback</i>\n\n&nbsp;\n\n<h2>Simplify Deployment</h2>\n\nOne of the key advantages of Databricks is that the model developed by data scientists can be run in production. This is a huge advantage as it reduces the development cycle and tremendously simplifies the maintenance. In contrast, today data scientists develop the model using single machine tools such as R or Python and then have data engineers re-implement the model for production.\n\n&nbsp;\n\n<h3>Simplify Infrastructure</h3>\n\nAs a data engineer, there are many steps and configurations to deploy Apache Spark in production.  Some examples include (but are not limited to):\n\n<ul>\n    <li>Configuring High Availability and Disaster Recovery for your Spark clusters</li>\n    <li>Building the necessary manifests to spin up and down clusters</li>\n    <li>Configuring Spark to utilize local SSDs for fast retrieval</li>\n    <li>Upgrading or patching your Spark clusters to the latest version of the OS or Apache Spark</li>\n</ul>\n\nWith Databricks, the management of your Spark clusters are taken care by dedicated Databricks engineers who are supported by the developers and committers of the Apache Spark open source project.  These clusters are configured for optimal performance and balance the issues surrounding resource scheduling, caching, and garbage collection.\nOnce deployed, you can quickly view what clusters are available and their current state including the libraries and notebooks that are attached to the cluster(s).  Concerns around high availability, disaster recovery, manifests to build and deploy clusters, service management, configurations, patching, and upgrades are all managed on your behalf using your own (or your company’s) AWS account.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-6.png\"><img class=\"alignnone wp-image-3968\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-6-1024x640.png\" alt=\"Figure 6\" width=\"700\" height=\"438\" /></a>\n\n<em><strong>Figure 6:</strong> Databricks Cluster view for easier management of your Databricks infrastructure</em>\n\n&nbsp;\n\n<h3>Simplify Job Scheduling</h3>\n\nTraditionally, transitioning from code development to production is a complicated task.  It typically involves separate personnel and processes to build the code and push it into production.  But Databricks has a powerful Jobs feature for running applications in production.  You can take the notebook you had just created and run it as a periodic job - scheduling it minute, hourly, daily, weekly, or monthly intervals.  It also has a smart cluster allocation feature that allows you to run your notebook on an existing cluster or on an on-demand cluster.  You can also receive email notifications for your job as well as configure retries and timeouts.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/Figure-7.png\"><img class=\"alignnone wp-image-3962\" src=\"https://databricks.com/wp-content/uploads/2015/06/Figure-7-1024x640.png\" alt=\"Figure 7\" width=\"700\" height=\"438\" /></a>\n\n<strong>Figure 7</strong>: View of the Population vs. Price Multi-Chart Notebook Nightly Job\n\n&nbsp;\n\nAs well, you can upload and execute any Spark JAR compiled against any Spark installation within the Jobs feature.  Therefore any previous work can be used immediately instead of recreating and rebuilding the code-base.\n\n&nbsp;\n\n<h2>Try out Databricks</h2>\n\nWe created Databricks to make it easier for data scientists and data engineers to focus on experimenting and training their models, quickly deploy and schedule jobs against those models, easily collaborate and share their learnings, and easily share the schema and definitions for their datasets.   Let us manage the cluster, configure it for optimal performance, perform upgrades and patches, and ensure high availability and disaster recovery.\n\nMachine Learning with Spark MLlib is a lot more fun when you get to spend most of your time doing Machine Learning!</td></tr><tr><td>null</td><td>List(Company Blog, Events)</td><td>List(2015-06-03, 2015-06-03, UTC)</td><td><i>This post is about the upcoming <a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Spark Summit in San Francisco</a>. Tickets are selling fast, so register today to join us! Use the code <em>Databricks20</em> to receive a 20% discount!</i>\n\n<hr />\n\nWe’re happy to announce that the upcoming Spark Summit will feature open office hours with experts on both Apache Spark and Databricks. The committer leads of every major Spark component (MLlib, Core, SQL, and Streaming) will be hosting hours along with several core engineers on the Databricks platform.\n\nAll office hours are hosted at the Databricks booth (A1). There is no need to sign up, just show up and ask questions! We’re looking forward to seeing you there!\n\n<b>Day 1, June 15th </b>(at Databricks Booth, A1)\n<table class=\"table\">\n<tbody>\n<tr>\n<td></td>\n<td><b><i>Expert</i></b></td>\n<td><b><i>Topic Area</i></b></td>\n</tr>\n<tr>\n<td>1:00-1:45</td>\n<td>Andrew Or</td>\n<td>Spark Core, YARN</td>\n</tr>\n<tr>\n<td></td>\n<td>Tathagata Das</td>\n<td>Spark Streaming</td>\n</tr>\n<tr>\n<td>1:45-2:30</td>\n<td>Michael Armbrust</td>\n<td>Spark SQL</td>\n</tr>\n<tr>\n<td></td>\n<td>Hossein Falaki</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td><i>2:30-3:00 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3:00-3:40</td>\n<td>Parviz Deyhim</td>\n<td>Databricks, Spark Ops</td>\n</tr>\n<tr>\n<td></td>\n<td>Ahir Reddy</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td>3:40-4:15</td>\n<td>Vida Ha</td>\n<td>Databricks, Spark Ops</td>\n</tr>\n<tr>\n<td></td>\n<td>Yin Huai</td>\n<td>Spark SQL</td>\n</tr>\n<tr>\n<td><i>4:15-4:30 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>4:30-5:15</td>\n<td>Josh Rosen</td>\n<td>Spark Core, PySpark</td>\n</tr>\n<tr>\n<td></td>\n<td>Burak Yavuz</td>\n<td>Spark MLlib</td>\n</tr>\n<tr>\n<td>5:15-6:00</td>\n<td>Joseph Bradley</td>\n<td>Spark MLlib</td>\n</tr>\n</tbody>\n</table>\n&nbsp;\n\n<b>Day 2, June 16 </b>(at Databricks Booth, A1)\n<table class=\"table\">\n<tbody>\n<tr>\n<td></td>\n<td><b><i>Expert</i></b></td>\n<td><b><i>Topic Area</i></b></td>\n</tr>\n<tr>\n<td>1:00-1:45</td>\n<td>Andrew Or</td>\n<td>Spark Core, YARN</td>\n</tr>\n<tr>\n<td></td>\n<td>Pat McDonough</td>\n<td>Databricks, Spark Ops</td>\n</tr>\n<tr>\n<td>1:45-2:30</td>\n<td>Reynold Xin</td>\n<td>Spark SQL, Spark Core</td>\n</tr>\n<tr>\n<td></td>\n<td>Hossein Falaki</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td><i>2:30-3:00 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>3:00-3:40</td>\n<td>Tathagata Das</td>\n<td>Spark Streaming</td>\n</tr>\n<tr>\n<td></td>\n<td>Jeff Pang</td>\n<td>Databricks</td>\n</tr>\n<tr>\n<td>3:40-4:15</td>\n<td>Davies Liu</td>\n<td>PySpark, Spark Core, SparkR</td>\n</tr>\n<tr>\n<td></td>\n<td>Josh Rosen</td>\n<td>Spark Core, PySpark</td>\n</tr>\n<tr>\n<td><i>4:15-4:30 break</i></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>4:30-5:15</td>\n<td>Xiangrui Meng</td>\n<td>Spark MLlib</td>\n</tr>\n<tr>\n<td></td>\n<td>Burak Yavuz</td>\n<td>Spark MLlib</td>\n</tr>\n<tr>\n<td>5:15-6:00</td>\n<td>Matei Zaharia</td>\n<td>Creator of Spark</td>\n</tr>\n</tbody>\n</table>\n&nbsp;</td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-06-05, 2015-06-05, UTC)</td><td>We have been working hard at Databricks to make our product more user-friendly for developers. Recently, we have added two new features that will allow developers easily use external libraries - both their own and 3rd party packages - in Databricks. We will showcase these features in a two-part series. Here is part 1, introducing how to upload your own libraries to Databricks. Stay tuned for the second installment on how to upload Apache Spark Packages and other 3rd party libraries!\n<h2>Using your favorite IDE with Databricks</h2>\nSometimes you prefer to stick to the development environment you are most familiar with. However, you also want to harness the power of Apache Spark with Databricks. We now offer the option to upload the libraries you wrote in your favorite IDE to Databricks with a single click.\n\nTo provide this functionality, we have created an SBT plugin (for more information on SBT, see <a href=\"http://www.scala-sbt.org/\" target=\"_blank\">http://www.scala-sbt.org/</a>). This plugin, sbt-databricks, (<a href=\"https://github.com/databricks/sbt-databricks\" target=\"_blank\">https://github.com/databricks/sbt-databricks</a>) provides Databricks users the ability to upload their libraries to Databricks within an IDE, like IntelliJ IDEA, or from the terminal. This means that anyone who has SBT can seamlessly upload their custom libraries to Databricks in a single click. This greatly simplifies the iteration time during development and provides users the freedom to develop in the environment that they are most comfortable with.\n<h2>Uploading your own libraries to Databricks in 4 simple steps</h2>\nHere is a simple example of how this works with IntelliJ IDEA:\n\n<strong>0. Install the SBT plugin. (IntelliJ IDEA -&gt; Preferences)</strong>\n\n<img class=\"alignnone wp-image-3987\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.09.35-PM.png\" alt=\"Screen Shot 2015-06-03 at 2.09.35 PM\" width=\"600\" height=\"375\" />\n\n<strong>1. Import the sbt-databricks plugin </strong>\n\n<img class=\"alignnone wp-image-3989\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.09.48-PM-1024x537.png\" alt=\"Screen Shot 2015-06-03 at 2.09.48 PM\" width=\"600\" height=\"314\" />\n\n<strong>2. Set up configurations in your build file</strong>\n\n<img class=\"alignnone wp-image-3990\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.01-PM-1024x499.png\" alt=\"Screen Shot 2015-06-03 at 2.10.01 PM\" width=\"600\" height=\"292\" />\n\n<strong>3. Open up the SBT console (through IDE or terminal)</strong>\n\n<img class=\"alignnone wp-image-3991\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.12-PM-1024x624.png\" alt=\"Screen Shot 2015-06-03 at 2.10.12 PM\" width=\"600\" height=\"366\" />\n\n<strong>4. Execute \"dbcDeploy\" and hit \"Enter\"!</strong>\n\n<img class=\"alignnone wp-image-3994\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.24-PM-1024x644.png\" alt=\"Screen Shot 2015-06-03 at 2.10.24 PM\" width=\"600\" height=\"378\" />\n\n<strong>Congratulations! Your library is now in Databricks</strong>\n<h2>An example of using a custom library in Databricks</h2>\nNow your libraries are imported to Databricks,\n\nYou can use them in Notebooks during an interactive data exploration session...\n\n<img class=\"alignnone wp-image-3995\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.49-PM-1024x336.png\" alt=\"Screen Shot 2015-06-03 at 2.10.49 PM\" width=\"600\" height=\"197\" />\n\n...or you can also use them in a production setting with Jobs! (both in Notebook Jobs and Jar Jobs, <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\" target=\"_blank\">see our Jobs blog for more details on how this works</a>)\n\n<img class=\"alignnone wp-image-3996\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-03-at-2.10.58-PM-1024x347.png\" alt=\"Screen Shot 2015-06-03 at 2.10.58 PM\" width=\"600\" height=\"203\" />\n<h2>Summary</h2>\nIn this blog post we introduced <a href=\"https://github.com/databricks/sbt-databricks\" target=\"_blank\">sbt-databricks</a>, an SBT plugin that allows users to easily deploy their own libraries to Databricks straight from their IDEs (SBT support for different IDEs can be found <a href=\"http://www.scala-sbt.org/0.13/docs/Community-Plugins.html\" target=\"_blank\">here</a>). At Databricks, our goal is to keep simple things simple, and make complex things possible. This includes providing developers with the flexibility to work in the environments they prefer - IDEs or Notebooks. We are developers ourselves after all!\n\nIf you have more questions, please check out the additional resources for more detailed information on how to use this plugin.\n\nStay tuned for the next installment, where we will show how to search for, and import 3rd Party Libraries from Spark Packages and/or Maven Central!\n<h2>Additional resources</h2>\n<ul>\n \t<li>Download <a href=\"https://github.com/databricks/sbt-databricks\">sbt-databricks from github</a></li>\n \t<li><a href=\"https://github.com/databricks/sbt-databricks/blob/master/README.md\">Get Documentation and sbt tips and tricks</a></li>\n \t<li><a href=\"http://www.scala-sbt.org/0.13/docs/Community-Plugins.html\">Find SBT Plugins for other IDEs</a></li>\n \t<li><a href=\"https://databricks.com/product/databricks-cloud\" target=\"_blank\">Learn more about Databricks</a></li>\n \t<li><a href=\"http://go.databricks.com/register-for-dbc\" target=\"_blank\">Sign-up for a 14-day free trial of Databricks</a></li>\n</ul>\n&nbsp;</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-06-09, 2015-06-09, UTC)</td><td>I am excited to announce that the upcoming Apache Spark 1.4 release will include SparkR, an R package that allows data scientists to analyze large datasets and interactively run jobs on them from the R shell.\n\nR is a popular statistical programming language with a number of extensions that support data processing and machine learning tasks. However, interactive data analysis in R is usually limited as the runtime is single-threaded and can only process data sets that fit in a single machine’s memory.  SparkR, an R package initially developed at the AMPLab, provides an R frontend to Apache Spark and using Spark’s distributed computation engine allows us to run large scale data analysis from the R shell.\n<h2>Project History</h2>\nThe SparkR project was initially started in the <a href=\"https://amplab.cs.berkeley.edu/\" target=\"_blank\">AMPLab</a> as an effort to explore different techniques to integrate the usability of R with the scalability of Spark. Based on these efforts, an initial developer preview of SparkR was <a href=\"http://amplab-extras.github.io/SparkR-pkg\" target=\"_blank\">first open sourced in January 2014</a>. The project was then developed in the AMPLab for the next year and we made many performance and usability improvements through open source contributions to SparkR. SparkR was recently merged into the Apache Spark project and will be released as an alpha component of Apache Spark in the 1.4 release.\n<h2>SparkR DataFrames</h2>\nThe central component in the SparkR 1.4 release is the SparkR DataFrame, a distributed data frame implemented on top of <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\" target=\"_blank\">Spark</a>.  Data frames are a fundamental data structure used for data processing in R and the concept of data frames has been extended to other languages with libraries like Pandas etc. Projects like <a href=\"https://github.com/hadley/dplyr\" target=\"_blank\">dplyr</a> have further simplified expressing complex data manipulation tasks on data frames. SparkR DataFrames present an API similar to dplyr and local R data frames but can scale to large data sets using support for distributed computation in Spark.\n\nThe following example shows some of the aspects of the DataFrame API in SparkR. (You can see the full example at <a href=\"https://gist.github.com/shivaram/d0cd4aa5c4381edd6f85\">https://gist.github.com/shivaram/d0cd4aa5c4381edd6f85</a>)\n\n<pre># flights is a SparkR data frame. We can first print the column \n# names, types, flights\n#DataFrame[year:string, month:string, day:string, dep_time:string, dep_delay:string, #arr_time:string, arr_delay:string, carrier:string, tailnum:string, flight:string, origin:string, #dest:string, air_time:string, distance:string, hour:string, minute:string]\n# Print the first few rows using `head`\nhead(flights)\n# Filter all the flights leaving from JFK\njfk_flights <- filter(flights, flights$origin == \"JFK\")\n# Collect the DataFrame into a local R data frame (for plotting etc.)\nlocal_df <- collect(jfk_flights)\n</pre>\n\nFor a more comprehensive introduction to DataFrames you can see the SparkR programming guide at <a href=\"http://people.apache.org/~pwendell/spark-releases/latest/sparkr.html\" target=\"_blank\">http://people.apache.org/~pwendell/spark-releases/latest/sparkr.html</a>\n<h2>Benefits of Spark integration</h2>\nIn addition to having an easy to use API, SparkR inherits many benefits from being tightly integrated with Spark. These include:\n\n<b>Data Sources API</b>: By tying into Spark SQL’s <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\" target=\"_blank\">data sources API</a> SparkR can read in data from a variety of sources include Hive tables, JSON files, Parquet files etc.\n\n<b>Data Frame Optimizations</b>: SparkR DataFrames also inherit all of the optimizations made to the computation engine in terms of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\">code generation, memory management</a>. For example, the following chart compares the runtime performance of running group-by aggregation on 10 million integer pairs on a single machine in R, Python and Scala (it uses the same dataset as <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html</a>). From the graph we can see that using the optimizations in the computation engine makes SparkR performance similar to that of Scala / Python.\n\n<img class=\" wp-image-4099 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-08-at-7.11.27-PM2-1024x359.png\" alt=\"Screen-Shot-2015-06-08-at-7.11.27-PM2\" width=\"500\" height=\"175\" />\n\n<b>Scalability to many cores and machines: </b>Operations executed on SparkR DataFrames get automatically distributed across all the cores and machines available on the Spark cluster. As a result SparkR DataFrames <a href=\"http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html\" target=\"_blank\">can be used on terabytes of data</a> and run on clusters with thousands of machines.\n<h2>Looking forward</h2>\nWe have many other features planned for SparkR in upcoming releases: these include support for <a href=\"https://issues.apache.org/jira/browse/SPARK-6805\" target=\"_blank\">high level machine learning algorithms</a> and making SparkR DataFrames a stable component of Spark.\n\nThe SparkR package represents the work of many contributors from various organizations including AMPLab, Databricks, Alteryx and Intel. We’d like to thank our contributors and users who tried out early versions of SparkR and provided feedback.  If you are interested in SparkR, do check out our talks at the upcoming <a href=\"https://spark-summit.org/2015/schedule/\" target=\"_blank\">Spark Summit 2015</a>.</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-06-10, 2015-06-10, UTC)</td><td>This is a guest blog from one of our partners: Huawei\n\n<a href=\"http://spark-summit.org/2015\">Join us at the Spark Summit</a> to hear from Intel and other companies deploying Apache Spark in production.  Use the code <i>Databricks20</i> to receive a 20% discount!\n\n<hr />\n\nIt’s not unusual that one or more terabytes data flows in a telco network every second - this translates to roughly exabytes every month.  In fact, the challenges go beyond the speed and volume of network flow data.  For example, the location data is in original wireless coding format with complex nested structure, and leaves little room for compression; the signaling data, derived from multi-interfaces device of multi vendors in real-time and batch mode, requires complex association rules to make it meaningful and easily interpretable.  Finally, the dynamic relationships across those data layers and among data entities of each horizontal layer create an exceedingly complex analytical problem. An effective and inherently unified data processing framework is the key to address this set of challenges.\n\n<img class=\" wp-image-4119 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-09-at-10.32.31-PM.png\" alt=\"Screen Shot 2015-06-09 at 10.32.31 PM\" width=\"400\" height=\"359\" />\n\n<b><i>Why We Chose Spark </i></b>\n\nTo solve telco data issues and meet data analytics needs in a cost effective manner, two factors matter the most: First, a scale-out, parallel data flow model based platform that can simultaneously handle different processing modes while efficiently supporting diverse workloads on the same execution engine - from SQL to running machine learning algorithms, from streaming to graph computing.\n\nSecond, an open framework can support diverse complex data sources in a consistent way, support multiple APIs intuitively, and have rich libraries in easy extension.  In this way, the IT, business, data science and network users can continue using their existing skills without tackling steep learning curves.  It significantly shortens the lifecycle of application development and onsite deployment.\n\nApache Spark allows us to address both of these needs with a single powerful platform, without the burden of coding, managing and integrating with multiple processing frameworks.\n\n<b><i>How Huawei leverages Spark</i></b>\n\nSpark is core to the data processing and analytics platform of Huawei’s big data solution, FusionInsight, which is used by more than 100 enterprise customers globally.\n\nWith Spark, the raw data from multi-systems of multi-vendors (e.g.,  CRM, billing, OSS and network) can be easily loaded into a single data processing layer.  Data scientists and data engineers can also use Spark SQL to explore the data, extract and group features, and develop models by leveraging MLlib algorithms.  Application developers can leverage the output of these models or features to build specific applications (e.g. base station investment optimization), and publish dashboards or reports for subscriber profiling and network monitoring.  Finally, business users can use Spark SQL for ad-hoc query, or continue to use existing BI systems or tools like SAS, R or Python with Spark’s powerful APIs.\n\nAs Huawei continues to build cutting-edge telecom solutions, we will increasingly adopt Spark as the core framework of our solutions since it provides a robust programming framework, rich set of APIs and libraries, vibrant ecosystem, and unparalleled pace of technology innovation.\n\n<b>Business Value Realized</b>\n\nIn one of top 5 mobile carriers in the world (who has more than 300 million subscribers) Huawei deployed Spark in its operating branch across mission-critical business areas. The system supports near real time analysis, ad-hoc query, especially over multiple data sources of CRM, billing, OSS (Operational Support System), and wireless network.  It also allows analysts and data scientists to build models over large data set more effectively, in some cases improving the time to deliver a product from months to mere weeks.\n\nWe have also had success in leveraging Spark to plan recommendation and churn prediction.  The conversion rate from pre-paid to post-paid customers improved by 10-20% in each month after the project going live the prediction for top K churned customers enhanced by ~30%, and each month it helped retain over 30,000 subscribers.  It translates into multi-million dollars business benefit to this flagship branch.\n\nHuawei and this customer are working together to further expand Spark into other operating branches, and to unlock the potential of data in other new business areas (e.g. providing site recommendation to leading ads agencies and retailers).\n\n<b><i>Huawei’s Commitment to Spark </i></b>\n\nHuawei’s relationship with Spark can be traced back to 2011 when AMP Lab was founded.  Huawei was convinced by the vision of AMP Lab and became corporate sponsor in early stage. Over years, Huawei has put together a global team to actively participate in the community and contribute things back.  In Spark 1.2 release, there’re 10 contributors from Huawei and 11 contributors in 1.3 release.\n\nTo further the adoption of Spark in vertical industries, we have developed <a href=\"https://github.com/Huawei-Spark/hbase/tree/master\">Spark SQL on HBase</a>, a community package project, designed to accelerate online data query and analytics for large data sets, and contributed thousands of lines code back.  Huawei team has also contributed <a href=\"https://databricks.com/blog/2015/04/17/new-mllib-algorithms-in-spark-1-3-fp-growth-and-power-iteration-clustering.html\">two new features into Spark 1.3 release</a>: The FP-growth algorithm is utilized to solve the frequent pattern mining problem and Power Iteration Clustering algorithm to identify similar behaviors among subscribers, network clusters or other combinations.\n\nHuawei will continue to contribute to Spark and work on community projects, some of our planned efforts include: adding co-processor and custom filter into Spark SQL on HBase; participating on Project Tungsten while exploring the possibility to bring vectorized processing and compilation on LLVM; bringing business case driven new algorithms into MLlib under pipeline API and support MLlib feature transformer; planning to support CEP processing in Spark streaming.  In short, Huawei is deeply committed to Spark and intends participate extensively in joint community and industry efforts.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-06-11, 2015-06-11, UTC)</td><td>Today I’m excited to announce the general availability of Apache Spark 1.4! Spark 1.4 introduces SparkR, an R API targeted towards data scientists. It also evolves Spark’s DataFrame API with a large number of new features. Spark's ML pipelines API first introduced in Spark 1.3 graduates from an alpha component. Finally, Spark Streaming and Core add visualization and monitoring to aid in production debugging.  We’ll be publishing in-depth posts covering Spark’s new features over the coming weeks. Here I’ll briefly outline some of the major themes and features in this release.\n<h2>SparkR ships in Spark</h2>\nSpark 1.4 introduces <a href=\"http://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">SparkR, an R API for Spark</a> and Spark's first new language API since PySpark was added in 2012. SparkR is based on Spark’s <a href=\"https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html\">parallel DataFrame abstraction</a>. Users can create SparkR DataFrames from “local” R data frames, or from any Spark data source such as Hive, HDFS, Parquet or JSON. SparkR DataFrames support all Spark DataFrame operations including aggregation, filtering, grouping, summary statistics, and other analytical functions. They also supports mixing-in SQL queries, and converting query results to and from DataFrames. Because SparkR uses the Spark’s parallel engine underneath, operations take advantage of multiple cores or multiple machines, and can scale to data sizes much larger than standalone R programs.\n<pre>people <- read.df(sqlContext, \"./examples/src/main/resources/people.json\", \"json\")\nhead(people)\n##  age    name\n##1  NA Michael\n##2  30    Andy\n##3  19  Justin\n# SparkR automatically infers the schema from the JSON file\nprintSchema(people)\n# root\n#  |-- age: integer (nullable = true)\n#  |-- name: string (nullable = true)</pre>\n<h2>Window functions and other DataFrame improvements</h2>\nThis release adds window functions to Spark SQL and in Spark’s DataFrame library. Window functions are popular for data analysts and allow users to compute statistics over window ranges.\n<pre>val w = Window.partitionBy(\"name\").orderBy(\"id\")\ndf.select(\n  sum(\"price\").over(w.rangeBetween(Long.MinValue, 2)),\n  avg(\"price\").over(w.rowsBetween(0, 4))\n)</pre>\nIn addition, we have also implemented many new features for DataFrames, including enriched support for <a href=\"http://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html\">statistics and mathematical functions</a> (random data generation, descriptive statistics and correlations, and contingency tables), as well as functionalities for working with missing data.\n\nTo make Dataframe operations execute quickly, this release also ships the initial pieces of <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>, a broad performance initiative which will be a central theme in Spark's upcoming 1.5 release. Spark 1.4 adds improvements to serializer memory use and options to enable fast binary aggregations.\n<h2>ML pipelines graduates from alpha</h2>\nSpark introduced <a href=\"http://www.databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">a machine learning (ML) pipelines API</a> in Spark 1.2. Pipelines enable production ML workloads that include many steps, such as data pre-processing, feature extraction and transformation, model fitting, and validation stages. Pipelines have added many components in the 1.3 and 1.4 releases, and in Spark 1.4, they officially graduates from an alpha component meaning API’s will be stable going forward. As part of graduation this release brings the Python API into parity with the Java and Scala interfaces. Pipelines also add a variety of new feature transformers such as <code>RegexTokenizer</code>, <code>OneHotEncoder</code>, and <code>VectorAssembler</code>, and new algorithms like linear models with elastic-net and tree models are now available within the pipeline API.\n<h2>Visualization and monitoring across the stack</h2>\nProduction Spark programs can be complex, with long workflows comprised of many different stages. Spark 1.4 adds visual debugging and monitoring utilities to understand the runtime behavior of Spark applications. An application timeline viewer profiles the completion of stages and tasks inside a running program. Spark 1.4 also exposes a visual representation of the underlying computation graph (or “DAG”) that is tied directly to metrics of physical execution. Spark streaming adds visual monitoring over data streams, to continuously track the latency and throughput. Finally, Spark SQL's JDBC server adds its own monitoring UI to list and track the progress of user-submitted queries.\n\n<img class=\" wp-image-4161 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/DAG-visualization-930x1024.png\" alt=\"DAG visualization\" width=\"600\" height=\"661\" />\n\nThis post only scratches the surface of all the new features in Spark 1.4. Stay tuned to the Databricks blog, where we’ll be writing posts about each of the major features in this release.\n\nTo download Spark 1.4, head on over to the <a href=\"http://spark.apache.org/downloads.html\">Apache Spark download</a> page. For a list of major patches in this release, visit the <a href=\"http://spark.apache.org/releases/spark-release-1-4-0.html\">release notes</a>.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Partners)</td><td>List(2015-06-15, 2015-06-15, UTC)</td><td>At today’s <a href=\"https://spark-summit.org/2015/\">Spark Summit</a>, Databricks and IBM announced a joint effort to contribute key machine learning capabilities to the Apache Spark Project.  Over the course of the next few months, Databricks and IBM will collaborate to expand Spark’s machine learning capabilities. The companies plan to introduce new domain specific algorithms to the Spark ecosystem and add new machine learning primitives in the Apache Spark Project. IBM and Databricks will also collaborate to integrate IBM’s SystemML – a robust machine-learning engine for large-scale data, with the Spark platform.\n\n“The size and scale of companies that are partnering with Databricks to support the Spark movement is both inspiring and validating,” said Ion Stoica, CEO at Databricks. “We are looking forward to IBM becoming a key member of the Spark community, as seen by their investment in a Spark Technology Center in San Francisco. This collaboration will help Spark continue to gain mainstream adoption and deliver next-generation big data analytics and applications.”\n\n<hr />\n\nPress release: <a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1200794\" target=\"_blank\">Databricks and IBM Collaborate to Advance Machine Learning to the Apache Spark Project</a>\n\n<hr />\n\n&nbsp;\n<h3>Learn More</h3>\nFor Spark enthusiasts abroad, the first <a href=\"http://spark-summit.org/eu-2015\">Spark Summit Europe</a> will be in Amsterdam from October 27th to 29th. <a href=\"http://spark-summit.org/eu-2015/cfp\">Submit a presentation</a> by June 23 and <a href=\"https://www.prevalentdesignevents.com/sparksummit2015/europe/registration.aspx?source=SummitBlog615\">register now</a> to get a discount.\n\nTo keep up with Spark and Databricks news, don’t forget to sign up for <a href=\"https://databricks.com/resources/newsletters\">our monthly newsletter</a>.\n\n&nbsp;\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Product)</td><td>List(2015-06-15, 2015-06-15, UTC)</td><td>We are excited to announce today, at <a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Spark Summit 2015</a>, the general availability of the Databricks – a hosted data platform from the team that created Apache Spark. With Databricks, you can effortlessly launch Spark clusters, explore data interactively, run production jobs, and connect third-party applications. We believe Databricks is the easiest way to use big data.\n\n<hr />\n\n<a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1201002\" target=\"_blank\">Press Release</a> | <a href=\"https://databricks.com/registration\" target=\"_blank\">Sign up for a 14-day free trial</a>\n\n<hr />\n\nOur vision at Databricks is to <b>make big data simple</b> and enable <b>every</b> organization to turn its data into value. We first unveiled Databricks at Spark Summit 2014, and launched it in limited availability in November. The excitement for the platform has been fantastic, with thousands of people requesting access and tremendous feedback from users. We’re now delighted to take the next step towards this vision by making Databricks available to everyone.\n\nWe want to enable that instant productivity for all of your data problems.\n\n[embed]https://vimeo.com/130273206[/embed]\n\n&nbsp;\n<h3>Why Databricks?</h3>\nAs many data scientists and engineers can attest, the majority of their time is spent not on the data analysis itself but on the supporting infrastructure. Key issues include deploying software, keeping production jobs up, and connecting disparate tools to process and visualize data. Equally problematic is the need for data engineers to re-implement the models developed by data scientists for production. With Databricks, data scientists and engineers can eliminate these issues and just spend their time focusing on their data.\n\n&nbsp;\n<h3>Instant Spark Clusters</h3>\nInstead of taking weeks to months to provision hardware, instantly launch and manage optimized Spark clusters on Amazon EC2. You can scale from a few nodes to hundreds, all with just a few clicks. You can also use Amazon spot instances to save on costs.\n\n<a href=\"https://databricks.com/wp-content/uploads/2014/10/cluster-screen.png\"><img class=\"alignnone wp-image-4016\" src=\"https://databricks.com/wp-content/uploads/2014/10/cluster-screen-1024x362.png\" alt=\"cluster-screen\" width=\"600\" height=\"212\" /></a>\n\n&nbsp;\n<h3>Interactively Explore and Visualize Your Data</h3>\nDatabricks includes <i>notebooks</i>, an interactive and collaborative multi-user environment for exploration and visualization. You can combine text, code execution, visualization, and advanced analytics such as machine learning (MLlib) and graphs (GraphX) – all within the same notebook. You can write notebooks in SQL, Python, Scala, Java, and R. This way Databricks allows you to be instantly productive in your language of choice.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/06/mobile-devices-by-geo.png\"><img class=\"alignnone wp-image-4290\" src=\"https://databricks.com/wp-content/uploads/2015/06/mobile-devices-by-geo-1024x862.png\" alt=\"mobile devices by geo\" width=\"850\" height=\"715\" /></a>\n\n&nbsp;\n<h3>Easily Deploy Production Pipelines</h3>\nDatabricks has a powerful Jobs feature for taking applications from prototype to production. With Jobs, you can run both notebooks and standalone Spark or Spark Streaming programs. Jobs include a flexible scheduler, email alerts, automatic retries, run history, and cluster reuse. Furthermore, Databricks Jobs are the first Software as a Service platform to support Spark Streaming, making it easy to deploy scalable, fault-tolerant streaming applications. Databricks Jobs are simply the easiest way to run Spark applications.\n\n<a href=\"https://databricks.com/wp-content/uploads/2014/11/databricks_jobs_screenshot.png\"><img class=\"alignnone wp-image-4025\" src=\"https://databricks.com/wp-content/uploads/2014/11/databricks_jobs_screenshot-1024x323.png\" alt=\"databricks_jobs_screenshot\" width=\"748\" height=\"236\" /></a>\n\n&nbsp;\n<h3>New Features for GA</h3>\nWith the launch of General Availability today, we’re also releasing three new features that users have been requesting. These are immediately available in Databricks deployments:\n<ul>\n\t<li><b>Spark 1.4 support:  </b>Choose <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">Apache Spark 1.4</a> when provisioning a Databricks cluster.</li>\n\t<li><b>Spark Streaming in notebooks:</b> Experiment with Spark Streaming interactively in notebooks, or deploy it in production jobs.</li>\n\t<li><b>Improved commenting:</b> Comment on individual text selections within a notebook and respond to comments via the new sidebar.</li>\n</ul>\n<img class=\"aligncenter wp-image-4268\" src=\"https://databricks.com/wp-content/uploads/2015/06/new-cluster-version-300x273.png\" alt=\"new-cluster-version\" width=\"200\" height=\"182\" />\n\n&nbsp;\n<h3>What’s Coming Next?</h3>\nIn addition to general availability, at Spark Summit we have also announced several major new features that we are rolling out over the next few months. Expect to see these in your Databricks deployments soon:\n<ul>\n\t<li><b>R-language notebooks:</b> Analyze data using R and <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">SparkR</a>, including all of R’s standard visualization and statistics packages.</li>\n\t<li><b>Access control and private notebooks: </b>Manage permissions to view and execute code at an individual level.</li>\n\t<li><b>Version control and GitHub: </b>Track changes to source code in Databricks, and store notebooks in GitHub to work with them from outside the platform.</li>\n</ul>\n&nbsp;\n<h3>How to Get Started</h3>\nDatabricks runs in your own Amazon Web Services account or Virtual Private Cloud. To try it out, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign up for a 14-day free trial today</a>.\n\nFor more information on Databricks, check out:\n<ul>\n\t<li><a href=\"http://dbricks.co/intro-to-db\" target=\"_blank\">Product demo video</a></li>\n\t<li><a href=\"https://databricks.com/resources/videos\" target=\"_blank\">Customer videos</a></li>\n\t<li><a href=\"http://dbricks.co/prod-brochure\" target=\"_blank\">Product brochure</a></li>\n\t<li><a href=\"http://dbricks.co/feature-primer\" target=\"_blank\">Feature primer</a></li>\n\t<li><a href=\"http://dbricks.co/prod-datasheet\" target=\"_blank\">Data sheet</a></li>\n\t<li><a href=\"https://databricks.com/resources/briefs\" target=\"_blank\">Spark and Databricks primers</a></li>\n\t<li><a href=\"http://go.databricks.com/ovum_otr_report_download\" target=\"_blank\">Ovum On the Radar: Databricks</a></li>\n\t<li><a href=\"http://venturebeat.com/2015/06/15/more-data-more-complexity-making-big-data-simple\" target=\"_blank\">Venturebeat article: More data, more complexity? Making big data simple</a></li>\n</ul></td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-06-16, 2015-06-16, UTC)</td><td>This is a guest post from our friends at DataStax.\n\n<hr />\n\nApache Cassandra™ is a fully distributed, highly scalable database that allows users to create online applications that are always-on and can process large amounts of data.  Apache Spark™ is a processing engine that enables applications in Hadoop clusters to run up to 100X faster in memory, and even 10X faster when running on disk. So it was just a matter of time until the two technologies found each other to deliver ridiculously fast analytics on real-time, operational data stored in a high-performance transactional database.\n\nThis blog post will go into details on the inner workings of Spark and how you can shape your application to take advantage of interactions between Spark and Cassandra, and answers to of the most commonly asked questions on the Cassandra + Spark topic.\n<h2>Spark Architecture Basics</h2>\nSpark is centered around 4 processes, we can view them on a running system by using the <code>jps</code> command.\n<pre>9:57:59 /~ jps # Java PS, lists all running java processes\n15687 DseSparkMaster # Spark Master (May be incorporated in DseDaemon)\n22232 DseSparkWorker # Spark Worker\n22652 CoarseGrainedExecutorBackend # Spark Executor\n22653 Jps\n22415 SparkSubmit # Spark Driver (Your Application)</pre>\n\n<img class=\"aligncenter wp-image-4293\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.24.49-PM-1024x619.png\" alt=\"Screen Shot 2015-06-15 at 7.24.49 PM\" width=\"500\" height=\"302\" />\n<h3>Spark Master JVM</h3>\nAnalogous to Hadoop Job Tracker, doesn’t need a lot of RAM since all it does is distribute work to the cluster.\n<h3>Spark Worker JVM</h3>\nAnalogous to Hadoop Task Tracker, also doesn’t need a lot of RAM since its main responsibility is starting up executor processes.\n\nIn DSE/StandAlone mode, a Worker will only start a single executor JVM per application, but this does not mean that you will use only 1 core on the machine. The single executor JVM will use up to the “max cores” set as available on the worker (spark.cores.max). Having more than 1 executor JVM is only possible with multiple workers (a DSE 4.7 feature)\n<h3>Spark Executor JVM</h3>\nThe most important part of Spark performance. Basically this is going to be a set of processes that do nothing but process RDD tasks.\n<h4>CPU Requirements</h4>\nEach core that is allocated to this executor will be able to act on one task at a time. That means a cluster with 20 cores defined by Spark will be able to act on 20 tasks concurrently. If the RDD being processed has less than 20 partitions, then the cluster will not be fully utilized. There will be more details on how tasks/partitions are generated below, but in general the number of tasks should be greater than the number of cores.\n\nYou can set more cores available on a worker than there are physical cores. This can have benefits for I/O bound tasks, but it is most likely a bad idea to oversubscribe CPUs on a node also running Cassandra. If oversubscribed, the system will be relying on the OS to decide when Cassandra gets to respond to requests or send out heartbeats. There are some interesting ideas to mitigate this using cgroups to limit cluster resources but I don’t know enough about these strategies to recommend them.\n<h4>RAM Requirements</h4>\nNow let’s imagine within this cluster we have 4 physical nodes with 5 cores on each. This means that every machine will have an executor JVM (most likely named<i>CoarseGrainedExecutorBackend</i>.) This JVM’s heap will be shared between the executors and will have all the same caveats that any other JVM-based application has. A large heap will cause longer garbage collections and extremely large heaps are untenable. The size restriction is of less importance in batch applications like Spark since a 1-second stop the world GC doesn’t mean too much in a 30 minute task. Databricks has a recommended size of 55 GB in the heap. When setting your executor JVM size remember that you are taking away memory from C* and the Operating System. Be sure to leave enough for C* to run and for page cache to help out with C* operations.\n<h5><b>RDD Storage Fraction</b></h5>\nThe largest portion is the cache which will be used for keeping RDD partitions in memory. Since actually retrieving data from Cassandra is most likely going to be a bottleneck for most users since most applications will pull a sizable amount of data from C* and then work on it in memory. The default is 60% of heap and is set with (<i>spark.storage.memoryFraction</i>). Feel free to adjust this but keep in mind the other two portions of the heap. <i>Note: as per the Spark documentation, this fraction should be roughly the same as the old generation size in your JVM</i>.\n<h5><b>Application Code and Shuffle Storage</b></h5>\nSpark shuffles are organized and performed through a shuffle management service which uses the space in the shuffle.storage portion of the executor to actually move around data (and sort it) before writing it to files and shipping it across the network. Any operations that require a full <i>sort</i>, <i>groupBy</i>, or <i>join</i> will trigger a full shuffle so be careful when reducing this setting (<i>spark.shuffle.memoryFraction</i>) from the default of 0.2. The remaining portion of the heap is for your application code and can be scaled depending on what code and jars are required for your application.\n\nThere must be at least the requested amount of RAM available on each worker to create an executor for the Spark Application. In heterogeneous clusters, this means the executor memory can be no larger than the smallest worker if you wish tasks to run on all the machines.\n<h5><b>Networking</b></h5>\nAmong these components the following connections will be established:\n<ul>\n \t<li>Driver &lt;-&gt; Master</li>\n \t<li>Master &lt;-&gt; Worker</li>\n \t<li>Driver &lt;-&gt; Executor</li>\n</ul>\n<img class=\" wp-image-4294 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.24.49-PM1-1024x619.png\" alt=\"Screen Shot 2015-06-15 at 7.24.49 PM\" width=\"500\" height=\"302\" />\n\nOne of the key things to troubleshoot here is the connection between the Driver and the Executors. Usually there is an issue with this communication if jobs start but then terminate prematurely with “unexpected exception in ….” stemming from timed out futures. Most network connectivity issues are because that last link (between the driver and executor) is not successfully being established. Remember that this means that not only must the driver be able to communicate with the executor, but the executor must be able to connect with the driver. If you are having difficulty make sure that the Spark config option <i>spark.driver.host</i> (set in <i>spark-defaults.conf</i>) matches a reachable IP address on the machine running the driver application. In some situations we have found that using an IP address works when having a resolvable domain name does not.\n<h2>The Anatomy of an RDD</h2>\nThis is where we get real deep real fast. Let’s start about talking about what an RDD is at its very base.\n\nAn RDD has several main components:\n<h3>A Dependency Graph</h3>\nThis details what RDDs must be computed before the current RDD can be successfully executed. This can be empty for an RDD coming from nowhere (like <i>sc.cassandraTable</i>) or a long chain of operations and dependencies (like with <i>rdd.map.filter.shuffle.join.map</i>).\n\nThe graph for any RDD can be viewed with <i>toDebugString</i>\n<pre>scala&gt; println(sc.cassandraTable(\"test\",\"tab\").toDebugString)\n(1) CassandraRDD[3] at RDD at CassandraRDD.scala:48\nscala&gt; println(sc.parallelize(1 to 10).map(_*2).map(_*2).map(_*2).toDebugString\n(6) MappedRDD[7] at map at :60\n| MappedRDD[6] at map at :60\n| MappedRDD[5] at map at :60\n| ParallelCollectionRDD[4] at parallelize at :60</pre>\n<h3>Partitions</h3>\nA description of how the RDD is partitioned and associated metadata describing the properties of each partition. Each partition should be thought of as a discrete chunk of the data represented by the RDD.\n<h3>Compute Method</h3>\nA compute method takes a piece of partition metadata (and the task context) and does something to that partition returning an iterator. This is the lazy method which will be executed when an action is called on the RDD.\n\nFor example, in the CassandraRDD this method reads metadata for each partition to get Cassandra token ranges and returns an iterator that yields C* data from that range. The Map RDD on the other hand uses the partition to retrieve an iterator from the previous RDD and then applies the given function to that iterator. (For more information see the video <a href=\"https://academy.datastax.com/demos/how-spark-cassandra-connector-reads-data\">How the Cassandra Connector Reads Data</a>.)\n<h3>Preferred Location Method</h3>\nA method which describes the preferred location where a particular partition should be computed. This location is defined by the RDD but most RDD types delegate this to the previous RDD in the chain. In all cases, this will be ignored if the partition has been check-pointed since the computed partition already exists. In CassandraRDD this method uses information from the custom partition class to see which node actually contains the ranges specified in the partition. Note that this is a “preferred” not “guaranteed” location. Whether or not a partition will be streamed to another node or computed locally is dependent on the <i>spark.locality.wait</i> parameters. This parameter can be set to 0 to force all partitions to only be computed on local nodes.\n\nWhen an action is performed on an RDD the dependency tree is analyzed and separated into independent subtrees. Each independent subtree becomes a stage. All of the stages are processed until results can be provided to the user.\n<h2>Keeping the Dependency Graph Narrow</h2>\nMany things you do in Spark will only require one partition from the previous RDD (for example: <i>map</i>, <i>flatMap</i>, <i>keyBy</i>). Since computing a new partition in an RDD generated from one of these transforms only requires a single previous partition we can build them quickly and in place. These are the most efficient and reliable operations you can do in Spark. Since each new partition relies only on a single past partition they can be retried independently and should require no network operations.\n\n&nbsp;\n\n<img class=\" wp-image-4295 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.31.24-PM-1024x820.png\" alt=\"Screen Shot 2015-06-15 at 7.31.24 PM\" width=\"500\" height=\"401\" />\n\nOn the other hand some transforms require data being moved about the cluster because they require knowledge of all of the previous RDD’s partitions to work. Transforms such as shuffles, groupBy, join, and sort all require a shuffle under the hood and thus are dependent on all of the previous RDD’s partitions to do their work. You should attempt to keep these transformations to a minimum and push them as far down in your graph as possible (after any filtering you are doing.) It’s also a great idea to cache the result of these expensive actions so that any further references to it will not require a recompute of the entire dependency tree.\n\n<img class=\" wp-image-4296 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.32.17-PM-1024x788.png\" alt=\"Screen Shot 2015-06-15 at 7.32.17 PM\" width=\"500\" height=\"385\" />\n\n<b>Where Operations should be in your Chain of RDD Operations</b>\n<table class=\"table\">\n<tbody>\n<tr>\n<td>Placement</td>\n<td>&lt;- Earliest</td>\n<td></td>\n<td></td>\n<td></td>\n<td>Latest -&gt;</td>\n</tr>\n<tr>\n<td>Type of Operation</td>\n<td>Cassandra RDD Specific</td>\n<td>Filters on the\nSpark Side</td>\n<td>Independent\nTransforms</td>\n<td>Per Partition Combinable\nTransforms</td>\n<td>Full Shuffle\nOperations</td>\n</tr>\n<tr>\n<td>Examples</td>\n<td>where\nselect</td>\n<td>filter\nsample</td>\n<td>map\nmapByPartition\nkeyBy</td>\n<td>reduceByKey\naggregateByKey</td>\n<td>groupByKey\njoin\nsort\nshuffle</td>\n</tr>\n</tbody>\n</table>\n<i>**Note if two RDDs share the same partitioner some of these operations become much cheaper. For example a join between RDDs with the same partitioner is essentially a 1 to 1 partition transform. **</i>\n\nThere are several ways to keep your graph narrow and minimize shuffles:\n<h3>Never let Spark Sort</h3>\nYou almost never *actually* want to sort all your data because every sort has a shuffle and shuffles are not your friend. (What are you going to do with 10 PB of sorted records anyway?) Every shuffle basically erases any data locality you once had as data will be randomly moved throughout your Spark cluster. If the actual goal is a TopX or BottomX consider the <i>top</i> and <i>takeOrdered</i> operations. These operations keep local TopX records on each partition so they don’t require a full shuffle to get the results and locality is preserved. If you need the data to be sorted within subgroup check out the Let Cassandra Sort section below.\n\nOne exception to this rule is when writing; there may be occasions when having the data sorted by C* partition key may will allow the Spark Cassandra Connector to write faster. This hasn’t been benchmarked yet and there is bound to be a balance between the time it takes to sort and the amount of data to be written.\n<h3>Do joins, groupBys, etc. on filtered data sets</h3>\nThe spark dependency graph doesn’t know the internal schema of your data (unless you are using SparkSQL) so that means that spark can’t optimize the execution path. This means the onus is on the developer to ensure that when you eventually do a <i>join</i> or <i>groupBy</i> you use the smallest dataset possible.\n\nThis means you want to ensure you never do this:\n<pre>rdd1.join(rdd2).filter(rdd1value = 30)\n</pre>\nWhen you could do this:\n<pre>rdd1.filter(rdd1value = 30).join(rdd2)\n</pre>\nOr even better if you are able to push the filter down to Cassandra:\n<pre>rdd1.where(rdd1value = 30).join(rdd2)</pre>\n<h3>Cache AFTER Hard Work</h3>\nAny time you have actually done a task which takes a long time (like a shuffle or reading data from C*), that might be a good time to cache your RDD. This tells Spark to make sure that the result of the compute from this RDD is stored. You can specify the storage_level with several parameters; see <a href=\"http://spark.apache.org/docs/1.2.0/programming-guide.html\">Spark Programming Guide</a> under RDD Persistence. A cached partition will not have to be recomputed if used multiple times.\n<pre>val hardWork = sc.cassandraTable(BigTable).map(expensiveFunction()).groupBy(someVal)\nhardWork.cache\nhardWork.operationOne().saveToCassandra\nhardWork.operationTwo().saveToCassandra\nhardWork.operationThree().saveToCassandra\n</pre>\n<h3>Never Collect and then Parallelize: Keep Data On The Cluster</h3>\nA huge anti-pattern is to <i>collect</i> an RDD, do some work on the driver, then <i>parallelize</i> it back to the cluster. Regardless of which language you are using with Spark, there is no excuse for ever doing work on the driver that could be done on the cluster instead. In practical terms, this means keeping your data as RDDs for the complete duration of the operation. The reason that this is so important is twofold; First, every time you perform a collect you have to serialize the contents of the RDD to the driver application (which may be a small JVM or running on small machine). Second, the client driver isn’t taking advantage of the cluster resources so you are almost guaranteed that driver code will be less performant than similar distributed code.\n\nExample:\n\nNever\n<pre>val array = sc.cassandraTable().filter().collect\nval newArray = someFunction(array)\nval rdd = sc.parallelize(newArray)\n</pre>\nInstead:\n<pre>val array =3D sc.cassandraTable()\n.filter()\n.mapPartitions( someFunction(_))\n.collect()\n</pre>\n<h2>Take Advantage of Cassandra</h2>\nThe fusion of Spark and Cassandra is more than just availability and durability. Cassandra is a tried a true OLTP solution and we can leverage its advantages within Spark as well!\n<h3>Let Cassandra Sort</h3>\nMost of the time if you want your records sorted by some field within a grouping there is no need to have Spark do this work for you. For example, consider a situation where you have incoming scores streaming in for a game which need to be ordered per user. Instead of sorting the data in Spark you can have Cassandra sort the data as it writes it into Cassandra Partitions. This will remove the need for a Spark-side shuffle and it will be quickly retrievable.\n<h3>Use Cassandra Specific RDD Functions</h3>\nRemember how I said to be careful with <i>groupBy</i> or <i>Join</i>? Well there are some specific <i>groupBy</i>s and <i>joins</i> which are actually very performant and you can (and should) use them as often as you like. These special operations are those which are only acting on data that resides within the same Cassandra partition. Cassandra collocates all data that resides within a Cassandra partition so we won’t need to shuffle and can take advantage of data locality.\n\nSince the raw Spark methods (<i>groupBy</i>, <i>Join</i>) will be shuffling your data, the connector provides new methods which do not require a repartitioning and when doing very specific operations. These methods are <i>spanBy</i> and <i>spanByKey</i>. With these methods you can quickly group up logical rows that share a common primary key without the pain that comes with a shuffle. Once you have grouped your partitions you can perform intra-partition operations on the newly made collections without fear of shuffles.\n\n**Note this functionality currently only works if the span is defined in the clustering key in C*. This means a table with a Primary Key (id, time, type) can be spanned by (id), (id,time), (id,time,type) but not (type) or (time,type). **\n<h3>Push Down Column Selection</h3>\nOne key way to save on network and serialization costs is to use the <i>select</i> method of the CassandraRDD. This method will push down column selections to Cassandra so the data retrieved and stored in spark is only what you actually want to act on.\n\nFor example, instead of:\n<pre>sc.cassandraTable(\"keyspace\",\"table\").map(row =&gt; (row.getInt(\"element1\"),row.getInt(\"element2\")))\n</pre>\nUse:\n<pre>sc.cassandraTable(\"keyspace\",\"table\").select(SomeColumns(\"element1\",\"element2\")\n</pre>\n<h3><b>Push Down Where Clauses</b></h3>\nThe Spark Cassandra Connector lets you push down where clauses to the Cassandra database. This will end up letting you do filtering on clustering columns and utilize secondary indexes. Some users may notice that this ends up putting “ALLOW FILTERING” on the underlying Cassandra queries. These same users will most likely be quick to point out that “ALLOW FILTERING” is a known Cassandra smell. This is correct, and you should normally never be using ALLOW FILTERING in a standard Cassandra OLTP application but here we are doing some quite different. In Spark, we won’t be executing these queries very often (hopefully just at RDD generation) and we are already going to hit every node in the cluster from the get go. The major ill effects of secondary indexes revolve around their need to hit all nodes in the cluster and since we are going to be doing this anyway, we can only improve our performance by taking advantage of pushing down this index if it exists. This is especially true as the ratio of “Data That You Want” / “Data In Your Table” shrinks.\n<h3><b>RDD/Cassandra Inner joins</b></h3>\nWhen you are already aware of the keys that you want to retrieve from a Cassandra table you can avoid doing a full table scan by using the inner join functionality in the Connector. This functionality is accessed by calling <i>joinWithCassandraTable(keyspace,table)</i> on any RDD writable to Cassandra. This method is most useful when you have a large subset of data from your Cassandra Table which can be specified with partition keys. The additional function <i>repartitionByCassandraReplica</i>(keyspace,table) can be used in cases when the RDD is not already partitioned in a way which is data local with Cassandra. This places all of the requests which will access the same Cassandra node in the same Spark partition.\n\nWorst: Filter on the Spark Side\n<pre>sc.cassandraTable().filter(partitionkey in keys)\n</pre>\nBad: Filter on the C* Side in a Single Operation\n<pre>sc.cassandraTable().where(keys in veryLargeListOfKeys)\n</pre>\nBest: Filter on the C* side in a distributed and concurrent fashion\n<pre>sc.parallelize(keys).joinWithCassandraTable()</pre>\n<h3>Spark Cassandra Connector Metrics</h3>\nThe Spark Cassandra Connector now includes metrics on the throughput to and from Cassandra. These metrics are on by default but can be disabled. They are integrated into the Spark UI so you can now see exactly how many bytes are being serialized to and from Cassandra. To view these metrics go to the stage detail section of the Spark UI and you will see the Input and Output columns populated with the number of bytes read from and written to C*. (Note: Due to the way metrics are implemented in Spark the input and output will be shown as “Hadoop”.)\n\n<img class=\"aligncenter wp-image-4297\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-15-at-7.39.44-PM-1024x145.png\" alt=\"Screen Shot 2015-06-15 at 7.39.44 PM\" width=\"800\" height=\"113\" />\n<h2>DataStax at Spark Summit 2015</h2>\nTo learn more about DataStax and our support for Spark, visit our booth at Spark Summit 2015! You can also let us know about what more features you would like in the Spark Cassandra Connector at <a href=\"https://github.com/datastax/spark-cassandra-connector\">github.com/datastax/spark-cassandra-connector</a>.\n\nAlso, Russ Spitzer, Software Engineer at DataStax, will be sharing his strategies on how to optimize Cassandra and Spark to perform lightning fast analytics on the world’s most scalable OLTP database.  This session will be on Monday, June 15th from 4:00 - 4:15pm in Room 1.\n\nHere’s the abstract for more details: <a href=\"https://spark-summit.org/2015/events/cassandra-and-spark-optimizing-for-data-locality/\">https://spark-summit.org/2015/events/cassandra-and-spark-optimizing-for-data-locality/</a></td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-06-17, 2015-06-17, UTC)</td><td>This is a guest post from our friends at MapR.\n\n<hr />\n\n&nbsp;\n<p class=\"Normal1\">This blog summarizes my conversations over the last few months with users who have deployed Apache Spark in production on the MapR Distribution including Hadoop. My key observations overall are that Spark is indeed making inroads into our user community, which is leveraging not just the rapid application development and performance capabilities of Spark, but also the power of a complete Spark stack that the MapR platform uniquely supports.</p>\n\n<h3>Why Spark?</h3>\nWe asked our users what they learned after deploying Spark, and here is what they had to share:\n<ol>\n \t<li>Traditional MapReduce is definitely hard to code and maintain. Users want to build a number of applications as quickly as possible, and Spark now allows them to cut down on the development and maintenance time. This trend is in line with a survey that we conducted recently, and found that <a href=\"http://www.google.com/url?q=http%3A%2F%2Fwww.techvalidate.com%2Ftvid%2F432-D51-48F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNGrCAuPcK-xEKecBqNjIwABMX95fg\">18% of MapR customers have deployed over 50 use cases on a single cluster</a>. Users mentioned that platform capabilities such as multi-tenancy, high availability and data protection are even more critical when deploying so many applications, so rapidly.</li>\n \t<li>Although Scala provides good advantages for Spark app development, there are enough developers out there who are using Java APIs to build Spark applications. Java 8, with support for Lambda expressions, is expected to make their life considerably easier. Python APIs are mostly being used by a smaller subset of users—the data scientist community— mainly for initial data modeling purposes.</li>\n</ol>\n&nbsp;\n<h3>Use Cases Overview</h3>\nThere are many different use cases that have been deployed combining Spark with MapR. Here are a few:\n<ol>\n \t<li><u>Faster batch applications</u>: Spark in-memory speeds are a definite plus point, especially for customer-facing applications. Many users have figured out that if their datasets can easily fit into memory based on the number of nodes they have, and if latency matters for that particular use case, then they need to quickly move towards converting those apps to Spark to gain performance advantages. A leading sales performance management company has done exactly this for their production application, originally written using traditional MapReduce.</li>\n \t<li><u>ETL data pipelines</u>: Given the full Spark stack support on MapR, a number of users are merging complex ETL pipelines into simpler programs that include feeding MLLib/Spark Streaming output to Spark SQL and GraphX applications. <a href=\"http://www.datanami.com/2015/01/19/creating-flexible-big-data-solutions-drug-discovery/\">Novartis does this</a> for drug discovery, using Spark for graph manipulations at scale.Several large financial services customers of MapR are doing ETL on streaming data from web clickstream and loading into transactional applications for call center applications so that customer service reps have all the latest information about what customers have been researching online.</li>\n \t<li><u>OLAP Cubes</u>: An emerging Spark use case across our customer base is one of an OLAP cube, where the end user can slice and dice an OLAP cube based on preconfigured datasets and filters. Predefined data loaded within a Spark context can be altered in real time by end users via predefined filters that kick off on-the-fly aggregations and simple linear regressions in the background. This solution is being used to deploy customer-facing services for real-time multidimensional OLAP analysis. As an example, <a href=\"https://www.mapr.com/customers/quantium\">Quantium</a>, one of the largest analytics services provider in Australia, has implemented this solution for its end users.</li>\n \t<li><u>Operational Analytics</u>: Yet another use case is real-time dashboarding and alerting systems based on streaming data, time-series data or operational data such as web clickstreams where a NoSQL store such as MapR-DB is being deployed as a durable, high-throughput persistence layer. A large retail analytics firm, a prominent financial services firm as well as a Fortune 100 healthcare company are implementing such solutions in production.</li>\n</ol>\n&nbsp;\n<h3>Platform Capabilities Still Matter</h3>\nIt may not come as a surprise, but the same enterprise-grade features that MapR customers have traditionally enjoyed continue to be applicable for Spark apps on Hadoop. NFS ingestion, high availability, a great option for an in-Hadoop NoSQL database, disaster recovery, and cross-datacenter replication still continue to matter and complete the story for production deployments.\n\n&nbsp;\n<h3>Want to Learn More?</h3>\nRead customer case studies for <a href=\"https://www.mapr.com/products/apache-spark\">Spark on Hadoop</a>.\n\nReview the <a href=\"https://www.mapr.com/apache-spark-dzone-ref-card\">Essential Apache Spark Cheat Sheet</a>.\n\nIf you are new to big data, check out our Spark-based <a href=\"https://www.mapr.com/solutions/big-data-and-hadoop-quick-start-solutions\">Quick Start Solutions </a>for Hadoop.\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Company Blog, Events)</td><td>List(2015-06-19, 2015-06-19, UTC)</td><td><a href=\"https://databricks.com/wp-content/uploads/2015/06/image1.jpg\"><img class=\"alignnone size-full wp-image-4345\" src=\"https://databricks.com/wp-content/uploads/2015/06/image1.jpg\" alt=\"image1\" width=\"640\" height=\"480\" /></a>\n\nUPDATE: Slides and videos from the Summit are now available! <a href=\"https://spark-summit.org/2015/\" target=\"_blank\">Check them out now!</a>\n\n<hr />\n\nWe are delighted about the success of  <a href=\"https://spark-summit.org/2015/\">Spark Summit 2015</a> in San Francisco on June 15th and 16th, with three different sold-out <a href=\"https://spark-summit.org/2015/training/\">Spark Training</a> sessions on June 17th.   This is the largest Spark Summit to date with more than 2000 attendees!   Databricks is proud to make all talk videos, slides, training talk videos, and training materials available online for free as a service to the Apache Spark community. Slides will be available on the <a href=\"https://spark-summit.org/2015/schedule/\">Spark Summit 2015 agenda page</a> and videos will be published there too as soon as we finish editing them.\n<h3>Key Announcements</h3>\nMatei Zaharia, the creator of Spark, and Patrick Wendell - both co-founders of Databricks - opened the summit with a talk about the <a href=\"https://spark-summit.org/2015/events/keynote-1/\">Spark Community Update</a>. In it they described how Apache Spark continues to grow quickly, with new features including data frames, R support, and machine learning pipelines added in the past few releases.\n\nIn the next keynote, Ion Stoica, CEO of Databricks, and Ali Ghodsi, VP Engineering and Product Management of Databricks, talked about <a href=\"https://spark-summit.org/2015/events/keynote-2/\">Powering Data Science with Spark</a>.  In it, they talked about how Databricks makes big data simple by enabling data professionals to easily solve their data challenges and by leveraging the power of Spark.   In it, they announced that <a href=\"https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html\">Databricks is generally available</a>!\n\nIn a great partnership milestone, Databricks and IBM had also <a href=\"http://www2.marketwire.com/mw/release_html_b1?release_id=1200794\">announced a joint effort</a> to contribute key machine learning capabilities to the <a href=\"https://spark.apache.org/\">Apache Spark</a> Project.  Over the course of the next few months, Databricks and IBM will collaborate to expand Spark’s machine learning capabilities.\n<h3>Keynotes</h3>\nWith Spark Summit being a community event focused on data science and data engineering at scale, some of our keynote highlights included:\n<ul>\n \t<li><a href=\"https://spark-summit.org/2015/events/tim-oreilly/\">Software Above the Level of a Single Device: The Implications</a> – Tim O'Reilly (O'Reilly Media)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/spark-at-nasa-jpl/\">Spark at NASA/JPL</a> – Chris Mattmann (NASA)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-4/\">Perspectives on Big Data &amp; Analytics</a> - Doug Wolfe (Central Intelligence Agency)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-5/\">Fireside chat</a> with Ben Horowitz (Andreessen Horowitz)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-8/\">Field Notes from Expeditions in the Cloud</a> – Matt Wood (Amazon Web Services)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/keynote-10/\">How Spark Fits into Baidu's Scale</a> – James Peng (Baidu)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/a-tale-of-a-data-driven-culture/\">A Tale of a Data-Driven Culture</a> – Gloria Lau (Timeful/Google)</li>\n</ul>\n<h3>Community Talks</h3>\nWith more than 260 submissions, this year’s Spark Summit had one of the most amazing <a href=\"https://spark-summit.org/2015/schedule/\">schedules</a> to date, with some session highlights including:\n<ul>\n \t<li><a href=\"https://spark-summit.org/2015/events/appraiser-how-airbnb-generates-complex-models-in-spark-for-demand-prediction/\">Appraiser : How Airbnb Generates Complex Models in Spark for Demand Prediction</a> – Hector Yee (Airbnb)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/\">Spark and Spark Streaming at Netflix</a> – Kedar Sadekar (Netflix), Monal Daxini (Netflix)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/hybrid-community-detection-for-web-scale-e-commerce-using-spark-streaming-and-graphx/\">Dynamic Community Detection for Large-scale e-Commerce data with Spark Streaming and GraphX</a> – Ming Huang (Taobao Inc, Alibaba Group)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/lessons-learned-with-spark-at-the-us-patent-trademark-office/\">Lessons Learned with Spark at the US Patent &amp; Trademark Office</a> – Christopher Bradford (OpenSource Connections)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/Solving-low-latency-query-over-big-data-with-Spark-SQL/\">Solving Low Latency Query Over Big Data with Spark SQL</a> – Julien Pierre (Microsoft)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/\">SparkR: The Past, the Present and the Future</a> – Shivaram Venkataraman (UC Berkeley AMPLAB), Rui Sun (Intel Asia Pacific R&amp;D)</li>\n \t<li><a href=\"https://spark-summit.org/2015/events/use-of-spark-mllib-for-predicting-the-offlining-of-digital-media/\">Use of Spark MLlib for Predicting the Offlining of Digital Media</a> – Christopher Burdorf (NBC Universal)</li>\n</ul>\n<h3>Training</h3>\nWe had <a href=\"https://spark-summit.org/2015/training/\">Spark Training</a> the day following Spark Summit, we trained over 500 students to use Spark in three parallel classes.\n<ul>\n \t<li><a href=\"https://spark-summit.org/2015/training/#intro\">Intro to Apache Spark</a></li>\n \t<li><a href=\"https://spark-summit.org/2015/training/#datasci\">Advanced: Data Science with Apache Spark</a></li>\n \t<li><a href=\"https://spark-summit.org/2015/training/#devops\">Advanced: Devops with Apache Spark</a></li>\n</ul>\nLearn more about Spark training classes run by Databricks on the <a href=\"https://databricks.com/services/spark-training\">training portion</a> of our website.\n<h3>Learn More</h3>\nFor Spark enthusiasts abroad, the first <a href=\"http://spark-summit.org/eu-2015\">Spark Summit Europe</a> will be in Amsterdam from October 27th to 29th. <a href=\"http://spark-summit.org/eu-2015/cfp\">Submit a presentation</a> by June 23 and <a href=\"https://www.prevalentdesignevents.com/sparksummit2015/europe/registration.aspx?source=SummitBlog615\">register now</a> to get a discount.\n\nTo keep up with Spark and Databricks news, don’t forget to sign up for <a href=\"https://databricks.com/resources/newsletters\">our monthly newsletter</a>.\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-06-22, 2015-06-22, UTC)</td><td><p style=\"text-align: center;\"><i>The greatest value of a picture is when it forces us to notice what we never expected to see.\n</i><i>- John Tukey</i></p>\nIn the past, the Apache Spark UI has been instrumental in helping users debug their applications. <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\" target=\"_blank\">In the latest Spark 1.4 release</a>, we are happy to announce that the data visualization wave has found its way to the Spark UI. The new visualization additions in this release includes three main components:\n<ul>\n \t<li>Timeline view of Spark events</li>\n \t<li>Execution DAG</li>\n \t<li>Visualization of Spark Streaming statistics</li>\n</ul>\nThis blog post will be the first in a two-part series. This post will cover the first two components and save the last for a future post in the upcoming week.\n<h2>Timeline View of Spark Events</h2>\nSpark events have been part of the user-facing API since early versions of Spark. In the latest release, the Spark UI displays these events in a timeline such that the relative ordering and interleaving of the events are evident at a glance.\n\nThe timeline view is available on three levels: <i>across all jobs</i>, <i>within one job</i>, and <i>within one stage</i>. On the landing page, the timeline displays all Spark events in an application across all jobs. Consider the following example:\n\n<img class=\" wp-image-4358 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.55.07-PM-1024x481.png\" alt=\"Screen Shot 2015-06-19 at 1.55.07 PM\" width=\"600\" height=\"282\" />\n\nThe sequence of events here is fairly straightforward. Shortly after all executors have registered, the application runs 4 jobs in parallel, one of which failed while the rest succeeded. Then, when all jobs have finished and the application exits, the executors are removed with it. Now let’s click into one of the jobs.\n\n<img class=\"aligncenter wp-image-4360\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.56.30-PM-1024x426.png\" alt=\"Screen Shot 2015-06-19 at 1.56.30 PM\" width=\"599\" height=\"249\" />\n\nThis job runs word count on 3 files and joins the results at the end. From the timeline, it’s clear that the the 3 word count stages run in parallel as they do not depend on each other. However, the join at the end does depend on the results from the first 3 stages, and so the corresponding stage (the collect at the end) does not begin until all preceding stages have finished. Let’s look further inside one of the stages.\n\n<img class=\"aligncenter wp-image-4362\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.57.36-PM-1024x823.png\" alt=\"Screen Shot 2015-06-19 at 1.57.36 PM\" width=\"600\" height=\"483\" />\n\nThis stage has 20 partitions (not all are shown) spread out across 4 machines. Each bar represents a single task within the stage. From this timeline view, we can gather several insights about this stage. First, the partitions are fairly well distributed across the machines. Second, a majority of the task execution time comprises of raw computation rather than network or I/O overheads, which is not surprising because we are shuffling very little data. Third, the level of parallelism can be increased if we allocate the executors more cores; currently it appears that each executor can execute no more than two tasks at once.\n\nI would like to take the opportunity to showcase another feature in Spark using this timeline: <i>dynamic allocation</i>. This feature allows Spark to scale the number of executors dynamically based on the workload such that cluster resources are shared more efficiently. Let’s see it in action through a timeline.\n\n<img class=\"aligncenter wp-image-4365\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-1.59.30-PM-1024x424.png\" alt=\"Screen Shot 2015-06-19 at 1.59.30 PM\" width=\"600\" height=\"248\" />\n\nThe first thing to note is that the application acquires executors over the course of a job rather than reserving them in advance. Then, shortly after the first job finishes, the set of executors used for the job becomes idle and is returned to the cluster. This allows other applications running in the same cluster to use our resources in the meantime, thereby increasing cluster utilization. Only when a new job comes in does our Spark application acquire a fresh set of executors to run it.\n\nThe ability to view Spark events in a timeline is useful for identifying the bottlenecks in an application. The next step in debugging the application is to map a particular task or stage to the Spark operation that gave rise to it.\n<h2>Execution DAG</h2>\nThe second visualization addition to the latest Spark release displays the execution DAG for each job. In Spark, a job is associated with a chain of RDD dependencies organized in a direct acyclic graph (DAG) that looks like the following:\n\n<img class=\"aligncenter wp-image-4366\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.00.59-PM.png\" alt=\"Screen Shot 2015-06-19 at 2.00.59 PM\" width=\"300\" height=\"395\" />\n\nThis job performs a simple word count. First, it performs a <i>textFile </i>operation to read an input file in HDFS, then a <i>flatMap</i> operation to split each line into words, then a <i>map</i> operation to form (word, 1) pairs, then finally a <i>reduceByKey</i> operation to sum the counts for each word.\n\nThe blue shaded boxes in the visualization refer to the Spark operation that the user calls in his / her code. The dots in these boxes represent RDDs created in the corresponding operations. The operations themselves are grouped by the stage they are run in.\n\nThere are a few observations that can be garnered from this visualization. First, it reveals the Spark optimization of pipelining operations that are not separated by shuffles. In particular, after reading from an input partition from HDFS, each executor directly applies the subsequent <i>flatMap </i>and <i>map</i> functions to the partition in the same task, obviating the need to trigger another stage.\n\nSecond, one of the RDDs is cached in the first stage (denoted by the green highlight). Since the enclosing operation involves reading from HDFS, caching this RDD means future computations on this RDD can access at least a subset of the original file from memory instead of from HDFS.\n\nThe value of the DAG visualization is most pronounced in complex jobs. As an example, the Alternating Least Squares (ALS) implementation in MLlib computes an approximate product of two factor matrices iteratively. This involves a series of <i>map</i>, <i>join</i>, <i>groupByKey</i> operations under the hood.\n\n<img class=\"aligncenter wp-image-4368\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.02.25-PM-1024x727.png\" alt=\"Screen Shot 2015-06-19 at 2.02.25 PM\" width=\"600\" height=\"426\" />\n\nIt is worth noting that, in ALS, caching at the correct places is critical to the performance because the algorithm reuses previously computed results extensively in each iteration. With the DAG visualization, users and developers alike can now pinpoint whether certain RDDs are cached correctly at a glance and, if not, understand quickly why an implementation is slow.\n\nAs with the timeline view, the DAG visualization allows the user to click into a stage and expand on details within the stage. The following depicts the DAG visualization for a single stage in ALS.\n\n<img class=\" wp-image-4369 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.03.21-PM-553x1024.png\" alt=\"Screen Shot 2015-06-19 at 2.03.21 PM\" width=\"300\" height=\"555\" />\n\nIn the stage view, the details of all RDDs belonging to this stage are expanded automatically. The user can now find information about specific RDDs quickly without having to resort to guess and check by hovering over individual dots on the job page.\n\nLastly, I would like to highlight a preliminary integration between the DAG visualization and Spark SQL. Since Spark SQL users are more familiar with higher level physical operators than with low level Spark primitives, the former should be displayed instead. The result is something that resembles a SQL query plan mapped onto the underlying execution DAG.\n\n<img class=\" wp-image-4370 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.04.05-PM-1024x879.png\" alt=\"Screen Shot 2015-06-19 at 2.04.05 PM\" width=\"600\" height=\"515\" />\n\nIntegration with Spark Streaming is also implemented in Spark 1.4 but will be showcased in a separate post.\n\nIn the near future, the Spark UI will be even more aware of the semantics of higher level libraries to provide more relevant details. Spark SQL will be given its own tab analogous to the existing Spark Streaming one. Within Spark Core, additional information such as number of partitions, call site, and cached percentages will be displayed on the DAG when the user hovers over an RDD.\n<h2>Summary</h2>\nThe latest Spark 1.4.0 release introduces several major visualization additions to the Spark UI. This effort stems from the project’s recognition that presenting details about an application in an intuitive manner is just as important as exposing the information in the first place. Future releases will continue the trend of making the Spark UI more accessible to users of both Spark Core and the higher level libraries built on top of it.\n\nStay tuned for the second half of this two-part series about UI improvements in Spark Streaming!\n<h2>Acknowledgment</h2>\nThe features showcased in this post are the fruits of labor of several contributors in the Spark community. In particular, <i>@sarutak</i> of <i>NTT Data</i> is the main author of the timeline view feature.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog)</td><td>List(2015-06-29, 2015-06-29, UTC)</td><td>We have been working in collaboration with professors at UC Berkeley and UCLA to produce two freely available Massive Open Online Courses (MOOCs). The first MOOC was released earlier this month and has been a tremendous success, with over 60K students enrolled and a large number of active students.  We are excited to announce that the second MOOC is launching today!\n\nThis new course, called <em>CS190.1x: Scalable Machine Learning</em>, introduces the underlying statistical and algorithmic principles required to develop scalable machine learning pipelines, and provides hands-on experience using Apache Spark.  This course is freely available on the edX MOOC platform, and edX Verified Certificates are also available for a fee.  Moreover, this course was developed in conjunction with the first Databricks-sponsored MOOC, called <em>CS100.1x: Introduction to Big Data with Apache Spark</em>. Students who complete both courses will receive a BerkeleyX Big Data <a href=\"https://www.edx.org/xseries\" target=\"_blank\">XSeries</a> Certificate.\n\nStudents have shown an overwhelming interest in these courses, as exhibited by the following statistics (as of 6/26/15):\n<ul>\n \t<li>100K+ enrolled students</li>\n \t<li>4K+ Verified Certificate enrollments</li>\n \t<li>24.2% active students in CS100.1x</li>\n \t<li>11.3% completion rate for CS100.1x</li>\n</ul>\nIt is our mission to enable data scientists and engineers around the world to leverage the power of big data by making it simple, and an important part of this mission is to educate the next generation.\n\nYou can still sign up for both courses today:\n<ol>\n \t<li><a href=\"https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\" target=\"_blank\">Introduction to Big Data with Apache Spark</a></li>\n \t<li><a href=\"https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\" target=\"_blank\">Scalable Machine Learning</a></li>\n</ol></td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Customers)</td><td>List(2015-07-02, 2015-07-02, UTC)</td><td>To learn more about how Databricks helped MyFitnessPal with analytics, check out an <a href=\"http://blogs.wsj.com/cio/2015/06/03/spark-a-tool-at-big-datas-cutting-edge-helps-under-armour-perform-faster-analytics/\" target=\"_blank\">earlier article in Wall Street Journal</a> (log-in required) or <a href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_MyFitnessPal.pdf\" target=\"_blank\">download the case study</a>.\n\n<hr />\n\nWe are excited to announce that MyFitnessPal (An Under Armour company) uses Databricks to build the production pipeline for its new “Verified Foods” feature, gaining many performance and productivity benefits in the process.\n\nMyFitnessPal aims to build the largest health and fitness community online, by helping people to achieve healthier lifestyles through better diet and more exercise. Health-conscious people can use the MyFitnessPal website or the smartphone app to track their diet and exercise patterns and use the information to reach their fitness goals. MyFitnessPal wanted to further streamline the diet tracking functionality by offering a feature called “Verified Foods”, where one can get accurate and up-to-date nutritional information of food items by simply typing the name of the food in the MyFitnessPal application.\n\nTo deliver the functionality of “Verified Foods”, MyFitnessPal needed to create an accurate food database with a set of sophisticated algorithms. Prior attempts to implement these algorithms without Databricks proved to be not scalable, nor fast enough: They took weeks to run due to the enormous volume of data and their extreme complexity.\n\nMyFitnessPal chose Databricks to implement these algorithms in a production pipeline based on Apache Spark because Databricks delivers the speed and flexibility of Apache Spark in a simple-to-use, zero management platform. Because of the high reliability and fast performance of the data pipeline powered by Databricks, the “Verified Foods” database now includes a comprehensive list of items with readily available and highly accurate nutritional information.\n\nIn addition to powering the “Verified Foods” feature, Databricks also delivered a number of key benefits to the Data Engineering &amp; Science team at MyFitnessPal:\n<ul>\n \t<li>10X speed improvement, reducing the algorithm run time from weeks to mere hours.</li>\n \t<li>Dramatically higher team productivity as measured by the number of projects completed in the past quarter.</li>\n \t<li>Improved team efficiency due to the availability of mature libraries in Spark, and the ability to easily share and re-use code in the Databricks platform.</li>\n</ul>\nDownload the <a href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_MyFitnessPal.pdf\" target=\"_blank\">case study</a> to learn more about Databricks.\n\n<a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">Sign up for a 14-day free trial to try Databricks today.</a></td></tr><tr><td>null</td><td>List(Apache Spark, Ecosystem, Engineering Blog)</td><td>List(2015-07-02, 2015-07-02, UTC)</td><td>This is a guest blog from our friend Vincenzo Selvaggio who contributed this feature. He is a Senior Java Technical Architect and Project Manager, focusing on delivering advanced business process solutions for investment banks.\n\n<hr />\n\nThe recently released Apache Spark 1.4 introduces PMML support to MLlib for linear models and k-means clustering. This achievement is the result of active discussions from the community on JIRA (<a href=\"https://issues.apache.org/jira/browse/SPARK-1406\">https://issues.apache.org/jira/browse/SPARK-1406</a>) and GitHub (<a href=\"https://github.com/apache/spark/pull/3062\">https://github.com/apache/spark/pull/3062</a>) and embraces interoperability between Apache Spark and other platforms when it comes to predictive analytics.\n<h2>What is PMML?</h2>\nPredictive Model Markup Language (PMML) is the leading data mining standard developed by The Data Mining Group (DMG), an independent consortium, and it has been adopted by major  vendors and organizations (<a href=\"http://www.dmg.org/products.html\">http://www.dmg.org/products.html</a>). PMML uses XML to represent data mining models. A PMML document is an XML document with the following components:\n<ul>\n\t<li>a <b>Header</b> giving general information such as a description of the model and the application used to generate it</li>\n\t<li>a <b>DataDictionary</b> containing the definition of fields used by the model</li>\n\t<li>a <b>Model</b> defining the structure and the parameters of the data mining model</li>\n</ul>\n<h2>Why use PMML?</h2>\nPMML allows users to build a model in one system, export it and deploy it in a different environment for prediction. In other words, it enables different platforms to speak the same language, removing the need for custom storage formats.\n\n[caption id=\"attachment_4426\" align=\"aligncenter\" width=\"600\"]<img class=\"wp-image-4426\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-02-at-9.53.31-AM-1024x229.png\" alt=\"PMML\" width=\"600\" height=\"134\" /> Image courtesy of Villu Ruusmann[/caption]\n\nWhat's more, adopting a standard encourages best practices (established ways of structuring models) and transparency (PMML documents are fully intelligible and not black boxes).\n<h2>Why is Spark supporting PMML?</h2>\nBuilding a model (producer) and scoring it (consumer) are two tasks very much decoupled as they require different systems and supporting infrastructure.\n\nModel building is a complex task, it is performed on a large amount of historical data and requires a fast and scalable engine to produce correct results: this is where Apache Spark's MLlib shines.\n\nModel scoring is performed by operational applications tuned for high throughput and detached from the analytical platform. Exporting MLlib's models in PMML enables sharing models between Spark and operational apps and is key for the success of predictive analytics.\n<h2>A Code Example</h2>\nIn Spark, Exporting a data mining model to PMML is as simple as calling <code>model.toPMML</code>. Here a complete example, in Scala, of building a KMeansModel and exporting it to a local file:\n\n[scala]\nimport org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.linalg.Vectors\n\n// Load and parse the data\nval data = sc.textFile(&quot;/path/to/file&quot;)\n  .map(s =&gt; Vectors.dense(s.split(',').map(_.toDouble)))\n\n// Cluster the data into three classes using KMeans\nval numIterations = 20\nval numClusters = 3\nval kmeansModel = KMeans.train(data, numClusters, numIterations)\n\n// Export clustering model to PMML\nkmeansModel.toPMML(&quot;/path/to/kmeans.xml&quot;)\n[/scala]\n\nThe PMML document generated is in this file: <code><a href=\"https://databricks.com/wp-content/uploads/2015/07/kmeans.pmml_.txt\" target=\"_blank\">kmeans.pmml</a></code>\n\nFor more examples of models exported and how those may be scored separately from Spark using the JPMML library, see\n\n<a href=\"http://spark-packages.org/package/selvinsource/spark-pmml-exporter-validator\">http://spark-packages.org/package/selvinsource/spark-pmml-exporter-validator</a>.\n<h2>Summary</h2>\nWith Apache Spark 1.4 PMML model export has been introduced, making MLlib interoperable with PMML compliant systems. You can find the supported models and how to export those to PMML from the official documentation page:\n\n<a href=\"http://spark.apache.org/docs/latest/mllib-pmml-model-export.html\">http://spark.apache.org/docs/latest/mllib-pmml-model-export.html</a>. We want to thank everyone who helped review and QA the implementation.\n\nThere is still work to do for MLlib’s PMML support, for example, supporting PMML export for more models and add Python API. For more details, please visit <a href=\"https://issues.apache.org/jira/browse/SPARK-8545\">https://issues.apache.org/jira/browse/SPARK-8545</a>.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2015-07-08, 2015-07-08, UTC)</td><td>Earlier, we presented <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">new visualizations</a> introduced in Apache Spark 1.4.0 to understand the behavior of Spark applications. Continuing the theme, this blog highlights new visualizations introduced specifically for understanding Spark Streaming applications. We have updated the Streaming tab of the Spark UI to show the following:\n<ul>\n\t<li>Timelines and statistics of events rates, scheduling delays and processing times of past batches.</li>\n\t<li>Details of all the Spark jobs in each batch.</li>\n</ul>\nAdditionally, the <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">execution DAG visualization</a> is augmented with the streaming information for understanding the job execution in the context of the streaming operations.\n\nLet’s take a look at these in more detail with an end-to-end example of analyzing a streaming application.\n<h2>Timelines and Histograms for Processing Trends</h2>\nWhen debugging Spark Streaming applications, users are often interested in the rate at which data is being received and the processing time of each batch. The new UI in the streaming tab makes it easy to see the current metrics as well as the trends over that past 1000 batches. While running a streaming application, you will see something like <i>figure 1</i> below if you visit the streaming tab in the Spark UI (Red letters such as [A] are our annotations, not part of the UI):\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image1.png\"><img class=\"aligncenter wp-image-4459\" src=\"https://databricks.com/wp-content/uploads/2015/07/image1-1024x822.png\" alt=\"Streaming UI figure 1\" width=\"600\" height=\"482\" /></a>Figure 1: Streaming tab in the Spark UI</p>\nThe first line (marked as <b>[A]</b>) shows the current status of the streaming application - in this example, the application has been running for almost 40 minutes at a 1-second batch interval. Below that, the timeline of <b>Input Rate</b> (marked as <b>[B]</b>) shows that the streaming app has been receiving data at a rate of about 49 events/second across all its sources. In this example, the timeline shows a slight dip in the average rate in the middle (marked as<b> [C]</b>), from which the application recovered towards the end of the timeline. If you want get more details, you can click the dropdown beside <b>Input Rate</b> (near<b> [B]</b>) to show timelines organized by each source, as shown in <i>figure 2 </i>below.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image2.png\"><img class=\"aligncenter wp-image-4461\" src=\"https://databricks.com/wp-content/uploads/2015/07/image2.png\" alt=\"Spark streaming UI figure 2\" width=\"600\" height=\"404\" /></a>Figure 2</p>\n<em>Figure 2</em> shows that the app had two sources (<b><i>SocketReceiver-0</i></b> and <b><i>SocketReceiver-1),</i></b> one of which caused the overall receive rate to dip because it had stopped receiving data for a short duration.\n\nFurther down in the page (marked as <b><i>[D]</i></b> in<i> figure 1</i>), the timeline for <b><i>Processing Time </i></b>shows that these batches have been processed within 20 ms on average. Having a shorter processing time comparing to the batch interval (1s in this example) means that the <b><i>Scheduling Delay</i></b> (defined as the time a batch waits for previous batches to complete, and marked as <b><i>[E] </i></b>in <i>figure 1</i>) is mostly zero because the batches are processed as fast as they are created. This scheduling delay is the key indicator of whether your streaming application is stable or not, and this UI makes it easy to monitor it.\n<h2>Batch Details</h2>\nReferring to <i>figure 1</i> once again, you may be curious regarding why some batches towards the right took longer to complete (note<b> [F]</b> in <i>figure 1</i>). You can easily analyze this through the UI. First of all, you can click on the points in the timeline graph that have higher batch processing times. This will take you to the list of completed batches further down in the page.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image3.png\"><img class=\"aligncenter wp-image-4464\" src=\"https://databricks.com/wp-content/uploads/2015/07/image3.png\" alt=\"image3\" width=\"600\" height=\"195\" /></a>Figure 3</p>\nIt will show all primary details of the individual batch (highlighted in green in <em>figure 3</em> above). As you can see, this batch has longer processing time than other batches. The next obvious question is what Spark jobs caused the longer processing time of this batch. You can investigate this by clicking on the batch time (the blue links in the first column), which will take you to the detailed information of the corresponding batch to show you the output operations and their Spark jobs (<i>Figure 4</i>).\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image4.png\"><img class=\"aligncenter wp-image-4465\" src=\"https://databricks.com/wp-content/uploads/2015/07/image4.png\" alt=\"image4\" width=\"600\" height=\"174\" /></a>Figure 4</p>\n<i>Figure 4</i> above shows that there was one output operation that generated 3 Spark jobs. You can click on the job IDs to continue digging into the stages and tasks for further analysis.\n<h2>Execution DAGs of Streaming RDDs</h2>\nOnce you have started analyzing the tasks and stages generated by the batch jobs, it is useful to get a deeper understanding of the execution graph. As shown in the <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\">previous blog post</a>, Spark 1.4.0 has added visualizations of the execution DAG (that is, directed acyclic graph) that shows the chain of RDD dependencies and how the RDDs are processed with a chain of dependent stages. If these RDDs are generated by DStreams in a streaming application, then the visualization shows additional streaming semantics. Let’s start with a simple streaming word count program in which we count the words received in each batch. See the example <a href=\"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\">NetworkWordCount</a>. It uses DStream operations <b><i>flatMap, map</i></b> and <b><i>reduceByKey</i></b> compute the word count. The execution DAG of a Spark job in any batch will look like <i>figure 5</i> below.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image5.png\"><img class=\"aligncenter wp-image-4466\" src=\"https://databricks.com/wp-content/uploads/2015/07/image5.png\" alt=\"image5\" width=\"400\" height=\"436\" /></a>Figure 5</p>\nThe black dots in the visualization represents the RDDs generated by DStream at batch time 16:06:50. The blue shaded boxes refer to the DStream operations that were used to transform the RDDs, and the pink boxes refer to the stages in which these transformations were executed. Overall this shows the following:\n<ul>\n\t<li>The data was received from a single<b><i> socket text stream</i></b> at batch time 16:06:50</li>\n\t<li>The job used two stages to compute word counts from the data using the transformations <b><i>flatMap</i></b>, <b><i>map</i></b>, and <b><i>reduceByKey</i></b>.</li>\n</ul>\n<p style=\"text-align: left;\">While this was a simple graph, it can get more complex with more input streams and advanced DStream transformations like <b><i>window</i></b> operations and <b><i>updateStateByKey</i></b> operation. For example, if we compute counts over a moving window of 3 batches (that is, using <b><i>reduceByKeyAndWindow</i></b>) using data from two socket text streams, the execution DAG of one of the batch jobs would look like <i>figure 6</i> below:</p>\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image6.png\"><img class=\"aligncenter wp-image-4467\" src=\"https://databricks.com/wp-content/uploads/2015/07/image6.png\" alt=\"image6\" width=\"600\" height=\"148\" /></a>Figure 6</p>\n<i>Figure 6 </i>shows a lot of information about a Spark job that counts words across data from 3 batches:\n<ul>\n\t<li>The first three stages essentially count the words within each of the 3 batches in the window. These are roughly similar to the first stage in the simple <em><strong>NetworkWordCount</strong></em> above, with <em><strong>map</strong></em> and <em><strong>flatMap</strong></em> operations. However note the following differences:\n<ol>\n\t<li>There were two input RDDs, one from each of the two <b><i>socket text streams.</i></b> These two RDDs were <strong><i>union</i>ed</strong> together into a single RDD and then further transformed to generate the per-batch intermediate counts.</li>\n\t<li>Two of these stages are grayed out because the intermediate counts of the older two batches are already cached in memory and hence do not require recomputation. Only the latest batch needs to be computed from scratch.</li>\n</ol>\n</li>\n\t<li>The last stage on the right uses <b><i>reduceByKeyAndWindow</i></b> to combine per-batch word counts into the “windowed” word counts.</li>\n</ul>\n<p style=\"text-align: left;\">These visualizations enable developers to monitor the status and trends of streaming applications as well as understand their relations with the underlying Spark jobs and execution plans.</p>\n\n<h2>Future Directions</h2>\n<p style=\"text-align: left;\">One significant improvement expected in Spark 1.5.0 is more information about input data in every batch (<a href=\"https://issues.apache.org/jira/browse/SPARK-8701\">JIRA</a>, <a href=\"https://github.com/apache/spark/pull/7081\">PR</a>). For example, if you were using Kafka, the batch details page will show the topics, partitions and offsets processed in that batch. Here is a preview:</p>\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/07/image7.png\"><img class=\"aligncenter wp-image-4468\" src=\"https://databricks.com/wp-content/uploads/2015/07/image7.png\" alt=\"image7\" width=\"600\" height=\"210\" /></a>Figure 7</p></td></tr><tr><td>null</td><td>List(Announcements, Company Blog)</td><td>List(2015-07-10, 2015-07-10, UTC)</td><td>Today, we are happy to announce <em>SparkHub</em> (<a href=\"http://sparkhub.databricks.com\">http://sparkhub.databricks.com</a>), a service for the Apache Spark community to easily find the most relevant Spark resources on the web.\n\n<a href=\"http://sparkhub.databricks.com\"><img class=\"alignnone wp-image-4517\" src=\"https://www.databricks.com/wp-content/uploads/2015/07/sparkhub-frontpage-1024x554.png\" alt=\"sparkhub-frontpage\" width=\"750\" height=\"406\" /></a>\n\n<em>SparkHub</em> contains the <a href=\"http://sparkhub.databricks.com/news/\">latest news</a> about Spark, <a href=\"http://sparkhub.databricks.com/videos/\">newest videos</a> of Spark talks, most <a href=\"http://sparkhub.databricks.com/resources/\">recent Spark packages</a>, and upcoming <a href=\"http://sparkhub.databricks.com/events/\">Spark events</a> around the world.  Want to find the next Spark Meetup close to you? <em>SparkHub</em> also has a <a href=\"http://sparkhub.databricks.com/meetups/\">directory</a> to help you to do so easily.\n\nWe will continue to expand the site in the coming months as we add even more content. We hope <em>SparkHub</em> will help you find Spark related information faster than ever. Everything is sourced from the Spark community, and we welcome input from you as well.\n\nPlease check out <em>SparkHub</em> now! If you have content suggestions, questions, or comments, we would like to <a href=\"mailto:sparkhub@databricks.com?Subject=Spark%20Summit%20Content%20Suggestion\">hear from you!</a>\n\n&nbsp;</td></tr><tr><td>null</td><td>List(Company Blog, Partners)</td><td>List(2015-08-03, 2015-08-03, UTC)</td><td>This is a guest blog from Tao Wang at <a href=\"http://www.sequoiadb.com/\">SequoiaDB</a>. He is the co-founder and CTO of SequoiaDB, leading its long-term technology vision, and is responsible for the leadership of advanced technology incubations. SequoiaDB is a JSON document-oriented transactional database.\n\n<hr />\n\n<h2>Why We Chose Apache Spark</h2>\nSequoiaDB is a NoSQL database that has the capability to replicate data on different physical nodes and allows users to specify which “copy of data” that the application should access. It is capable of running analytical and operational workloads simultaneously on the same cluster with minimal I/O or CPU contention.\n\nThe joint solution of Apache Spark and SequoiaDB allows users to build a single platform such that a wide variety of workloads (e.g., interactive SQL and streaming) can run together on the same physical cluster.\n<h2>Making SequoiaDB Work with Spark: The Spark-SequoiaDB Connector</h2>\n<a href=\"http://spark-packages.org/package/SequoiaDB/spark-sequoiadb\">Spark-SequoiaDB Connector</a> is a Spark data source that allows users to read and write data against SequoiaDB collections with Spark SQL. It is used to integrate SequoiaDB and Spark, combining the advantages of a schema-less storage model with dynamic indexing and the power of Spark clusters.\n\n<img class=\" wp-image-4497 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-08-at-11.28.23-AM-1024x443.png\" alt=\"Screen Shot 2015-07-08 at 11.28.23 AM\" width=\"600\" height=\"260\" />\n\nSpark and SequoiaDB can be installed in the same physical environment or in different clusters. The Spark-SequoiaDB Connector pushes down the search conditions to SequoiaDB, and only retrieves the records that match the query predicates.  This optimization enables analytics against operational data sources without the need to perform ETL between SequoiaDB and Spark.\n\nHere is a code example of how to use the Spark-SequoiaDB Connector in SparkSQL:\n\n<pre>sqlContext.sql(\"CREATE temporary table org_department ( deptno string, deptname string, mgrno string, admrdept string, location string ) using com.sequoiadb.spark OPTIONS ( host 'host-60-0-16-2:50000', collectionspace 'org', collection 'department', username 'sdbreader', password 'sdb_reader_pwd')\")\nres2: org.apache.spark.sql.DataFrame = []\nsqlContext.sql(\"CREATE temporary table org_employee ( empno int, firstnme string, midinit string, lastname string, workdept string, phoneno string, hiredate date, job string, edlevel int, sex string, birthdate date, salary int, bonus int, comm int ) using com.sequoiadb.spark OPTIONS ( host 'host-60-0-16-2:50000', collectionspace 'org', collection 'employee', username 'sdb_reader', password 'sdb_reader_pwd')\")\nres3: org.apache.spark.sql.DataFrame = []\nsqlContext.sql(\"select * from org_department a, org_employee b where a.deptno='D11'\").collect().take(3).foreach(println)\n[D11,MANUFACTURING SYSTEMS,000060,D01,null,10,CHRISTINE,I,HAAS,A00,3978,null,PRES,18,F,null,152750,1000,4220]\n[D11,MANUFACTURING SYSTEMS,000060,D01,null,20,MICHAEL,L,THOMPSON,B01,3476,null,MANAGER,18,M,null,94250,800,3300]\n[D11,MANUFACTURING SYSTEMS,000060,D01,null,30,SALLY,A,KWAN,C01,4738,null,MANAGER,20,F,null,98250,800,3060]</pre>\n\n<h2>Financial Services Industry Use Case: Improved Transaction History Archiving System</h2>\nThe joint solution of Spark and SequoiaDB can help organizations to retain more and get more value out of their data. Here we will showcase a financial services industry example, where a bank implemented an improved transaction history archiving system with Spark and SequoiaDB.\n\nFor the past few decades, most banks run their core banking systems on mainframes. The technical limitations of their mainframe systems meant that transaction history older than one year had to be removed from the mainframes and archived on tapes.\n\nHowever, banking customers today have much higher expectations of customer service than the past, driven by the broad adoption of online and mobile banking. To compete for customers more effectively, one of our customers - who is a large bank - wanted to improve its offering by allowing their customers to search for transactions older than one year.\n\n<img class=\"aligncenter wp-image-4720\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-29-at-11.52.34-AM-1024x960.png\" alt=\"Screen Shot 2015-07-29 at 11.52.34 AM\" width=\"400\" height=\"375\" />\n\nUsing SequoiaDB, this bank can save 15 years of all customer transaction data in 50 physical nodes (occupying more than 1PB disk space). This new system allows customers to access their full transaction history easily on the mobile device as well as the website.\n\n<img class=\"aligncenter wp-image-4721\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-29-at-11.53.23-AM-1024x784.png\" alt=\"Screen Shot 2015-07-29 at 11.53.23 AM\" width=\"400\" height=\"306\" />\n<h3><b>Financial Services Industry Use Case: Product Recommendation with Spark and SequoiaDB Integration</b></h3>\nOnce the full transaction history of all customers is readily available, the bank built a customer profiling system based on the transaction data to find the appropriate investment products for each customer.\n\nOnce the customer profiling system calculates product recommendations when processing all of the transaction data and logs, these properties are written back to a collection with a tag array for each customer.\n\nThese properties are used by the front desk staff and the recommendation engine to identify the potential interests of each customer. After deploying this system, the success rate of financial derivatives recommendation increased by more than ten times.\n\n<img class=\"aligncenter wp-image-4504\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-08-at-11.38.57-AM-1024x481.png\" alt=\"Screen Shot 2015-07-08 at 11.38.57 AM\" width=\"500\" height=\"235\" />\n<h3><b>What next with Spark at SequoiaDB</b></h3>\nMost of our financial customers are interested in streaming (for anti-money laundering and high-frequency trading use cases) or interactive SQL processing (for government supervision). We intend to put more efforts to improve the features and stability of these Apache Spark components, such as helping SparkSQL to support standard SQL2003.\n\nFor more information about Spark-SequoiaDB Connector please visit:\n\n<a href=\"https://github.com/SequoiaDB/spark-sequoiadb\">https://github.com/SequoiaDB/spark-sequoiadb</a></td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-07-13, 2015-07-13, UTC)</td><td>Apache Spark 1.4 was <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">released</a> on June 11 and one of the exciting new features was <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">SparkR</a>. I am happy to announce that we now support R notebooks and SparkR in Databricks, our <a href=\"https://databricks.com/blog/2015/06/15/databricks-is-now-generally-available.html\">hosted Spark service</a>. Databricks lets you easily use SparkR in an interactive notebook environment or standalone jobs.\n\nR and Spark nicely complement each other for several important use cases in statistics and data science. Databricks R Notebooks include the SparkR package by default so that data scientists can effortlessly benefit from the power of Apache Spark in their R analyses. In addition to SparkR, any R package can be easily installed into the notebook. In this blog post, I will highlight a few of the features in our R Notebooks.\n<h2>Getting Started with SparkR</h2>\n<img class=\" wp-image-4542 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-10-at-1.16.56-PM.png\" alt=\"Screen Shot 2015-07-10 at 1.16.56 PM\" width=\"500\" height=\"222\" />\n\nTo get started with R in Databricks, simply choose R as the language when creating a notebook.  Since SparkR is a recent addition to Spark, remember to attach the R notebook to any cluster running Spark version 1.4 or later. The SparkR package is imported and configured by default. You can run Spark queries in R:\n\nUsing SparkR you can access and manipulate very large data sets (e.g., terabytes of data) from distributed storage (e.g., Amazon S3) or data warehouses (e.g., Hive).\n\n<pre>airlinesDF <- read.df(sqlContext, path=\"dbfs:/databricks-datasets/airlines\", \n   source=\"com.databricks.spark.csv\", header=\"true\")\nregisterTempTable(airlinesDF, \"airlines\")</pre>\n\nSparkR offers distributed DataFrames that are syntax compatible with R data frames. You can also collect a SparkR DataFrame to local data frames.\n\n<pre>delays <- collect(sql(sqlContext, \"select avg(Distance) as distance, \n  avg(ArrDelay) as arrivalDelay, \n  avg(DepDelay) as departureDelay, \n  Origin, \n  Dest, \n  UniqueCarrier as carrier from airlines group by Origin, Dest, UniqueCarrier\"))</pre>\n\nFor an overview of SparkR features see our recent <a href=\"https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html\">blog post</a>. Additional details on SparkR API can be found on the <a href=\"http://spark.apache.org/docs/latest/api/R/index.html\">Spark website.</a>\n<h2>Autocomplete and Libraries</h2>\n<img class=\" wp-image-4526 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-09-at-1.40.09-PM.png\" alt=\"Screen Shot 2015-07-09 at 1.40.09 PM\" width=\"500\" height=\"247\" />Databricks R notebooks offer autocomplete similar to the R shell. Pressing TAB will complete the code or present available options if multiple exist.\n\nYou can install any R library in your notebooks using <code>install.packages()</code>. Once you import the new library, autocomplete will also apply to the newly introduced methods and objects.\n<h2>Interactive Visualization</h2>\nAt Databricks we believe visualization is a critical part of data analysis. As a result we embraced R’s powerful visualization and complemented it with many additional visualization features.\n<h3>Inline plots</h3>\nIn R Notebooks you can use any R visualization library, including base plotting, <a href=\"http://ggplot2.org/\">ggplot</a>, Lattice, or any other plotting library. Plots are displayed inline in the notebook and can be conveniently resized with the mouse.\n\n<pre>library(ggplot2)\np <- ggplot(delays, aes(departureDelay, arrivalDelay)) +  \n  geom_point(alpha = 0.2) + facet_wrap(~carrier)\np</pre>\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/ggplot1.png\"><img class=\"aligncenter wp-image-4534\" src=\"https://databricks.com/wp-content/uploads/2015/07/ggplot1.png\" alt=\"ggplot1\" width=\"700\" height=\"700\" /></a>\n\nYou can set options to change aspect ratio and resolution of inline plots.\n\n<pre>options(repr.plot.height = 500, repr.plot.res = 120)\np + geom_point(aes(color = Dest)) + geom_smooth() + \n  scale_x_log10() + scale_y_log10() + theme_bw()</pre>\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/ggplot2.png\"><img class=\"aligncenter wp-image-4535\" src=\"https://databricks.com/wp-content/uploads/2015/07/ggplot2.png\" alt=\"ggplot2\" width=\"700\" height=\"365\" /></a>\n<h3>One-click visualizations</h3>\nYou can use Databricks’s built-in<code> display()</code> function on any R or SparkR DataFrame. The result will be rendered as a table in the notebook, which you can then plot with one click without writing any custom code.\n\n<img class=\"aligncenter wp-image-4536 size-full\" src=\"https://databricks.com/wp-content/uploads/2015/07/display-animation.gif\" alt=\"display animation\" width=\"700\" height=\"260\" />\n<h3>Advanced interactive visualizations</h3>\nSimilar to other Databricks notebooks, you can use <code>displayHTML()</code> function in R notebooks to render any HTML and Javascript visualization.\n<h1>Running Production Jobs</h1>\nDatabricks is an end-to-end solution to make building a data pipeline easier - from ingest to production. The same concept applies to R Notebooks as well: You can schedule your R notebooks to run as jobs on existing or new Spark clusters. The results of each job run, including visualizations, are available to browse, making it much simpler and faster to turn the work of data scientists into production.\n\n<img class=\" wp-image-4537 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/Screen-Shot-2015-07-10-at-1.09.54-PM-1024x488.png\" alt=\"Screen Shot 2015-07-10 at 1.09.54 PM\" width=\"700\" height=\"334\" />\n<h2>Summary</h2>\nR Notebooks in Databricks let anyone familiar with R take advantage of the power of Spark through simple Spark cluster management, rich one-click visualizations, and instant deployment to production jobs. We believe SparkR and R Notebooks will bring even more people to the rapidly growing Spark community.\n\nTo try out the powerful R Notebooks for yourself, <a href=\"https://accounts.cloud.databricks.com/registration.html#signup\">sign-up for a 14-day free trial of Databricks today</a>!</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-07-15, 2015-07-15, UTC)</td><td>In this blog post, we introduce the new window function feature that was added in <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\">Apache Spark 1.4</a>. Window functions allow users of Spark SQL to calculate results such as the rank of a given row or a moving average over a range of input rows. They significantly improve the expressiveness of Spark’s SQL and DataFrame APIs. This blog will first introduce the concept of window functions and then discuss how to use them with Spark SQL and Spark’s DataFrame API.\n<h2>What are Window Functions?</h2>\nBefore 1.4, there were two kinds of functions supported by Spark SQL that could be used to calculate a single return value. <i>Built-in functions</i> or <i>UDFs</i>, such as <code>substr</code> or <code>round</code>, take values from a single row as input, and they generate a single return value for every input row. <i>Aggregate functions, </i>such as <code>SUM</code> or <code>MAX</code><i>,</i> operate on a group of rows and calculate a single return value for every group.\n\nWhile these are both very useful in practice, there is still a wide range of operations that cannot be expressed using these types of functions alone. Specifically, there was no way to both operate on a group of rows while still returning a single value for every input row. This limitation makes it hard to conduct various data processing tasks like calculating a moving average, calculating a cumulative sum, or accessing the values of a row appearing before the current row. Fortunately for users of Spark SQL, window functions fill this gap.\n\nAt its core, a window function calculates a return value for every input row of a table based on a group of rows, called the <i>Frame</i>. Every input row can have a unique frame associated with it. This characteristic of window functions makes them more powerful than other functions and allows users to express various data processing tasks that are hard (if not impossible) to be expressed without window functions in a concise way. Now, let’s take a look at two examples.\n\nSuppose that we have a <i>productRevenue</i> table as shown below.\n\n<img class=\"aligncenter wp-image-4572\" src=\"https://databricks.com/wp-content/uploads/2015/07/1-1.png\" alt=\"1-1\" width=\"300\" height=\"353\" />\n\nWe want to answer two questions:\n<ol>\n \t<li>What are the best-selling and the second best-selling products in every category?</li>\n \t<li>What is the difference between the revenue of each product and the revenue of the best-selling product in the same category of that product?</li>\n</ol>\nTo answer the first question “<i>What are the best-selling and the second best-selling products in every category?</i>”, we need to rank products in a category based on their revenue, and to pick the best selling and the second best-selling products based the ranking. Below is the SQL query used to answer this question by using window function <code>dense_rank</code> (we will explain the syntax of using window functions in next section).\n<pre>SELECT\n  product,\n  category,\n  revenue\nFROM (\n  SELECT\n    product,\n    category,\n    revenue,\n    dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) as rank\n  FROM productRevenue) tmp\nWHERE\n  rank &lt;= 2\n</pre>\nThe result of this query is shown below. Without using window functions, it is very hard to express the query in SQL, and even if a SQL query can be expressed, it is hard for the underlying engine to efficiently evaluate the query.\n\n<img class=\" wp-image-4574 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/1-2.png\" alt=\"1-2\" width=\"300\" height=\"184\" />\n\nFor the second question “<i>What is the difference between the revenue of each product and the revenue of the best selling product in the same category as that product?</i>”, to calculate the revenue difference for a product, we need to find the highest revenue value from products in the same category for each product. Below is a Python DataFrame program used to answer this question.\n<pre>import sys\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as func\nwindowSpec = \\\n  Window \n    .partitionBy(df['category']) \\\n    .orderBy(df['revenue'].desc()) \\\n    .rangeBetween(-sys.maxsize, sys.maxsize)\ndataFrame = sqlContext.table(\"productRevenue\")\nrevenue_difference = \\\n  (func.max(dataFrame['revenue']).over(windowSpec) - dataFrame['revenue'])\ndataFrame.select(\n  dataFrame['product'],\n  dataFrame['category'],\n  dataFrame['revenue'],\n  revenue_difference.alias(\"revenue_difference\"))\n</pre>\nThe result of this program is shown below. Without using window functions, users have to find all highest revenue values of all categories and then join this derived data set with the original <em>productRevenue</em> table to calculate the revenue differences.\n\n<img class=\"aligncenter wp-image-4578\" src=\"https://databricks.com/wp-content/uploads/2015/07/1-3.png\" alt=\"1-3\" width=\"400\" height=\"287\" />\n<h2>Using Window Functions</h2>\nSpark SQL supports three kinds of window functions: ranking functions, analytic functions, and aggregate functions. The available ranking functions and analytic functions are summarized in the table below. For aggregate functions, users can use any existing aggregate function as a window function.\n<table class=\"table\">\n<tbody>\n<tr>\n<td></td>\n<td><strong>SQL</strong></td>\n<td><strong>DataFrame API</strong></td>\n</tr>\n<tr>\n<td rowspan=\"5\"><strong>Ranking functions</strong></td>\n<td>rank</td>\n<td>rank</td>\n</tr>\n<tr>\n<td>dense_rank</td>\n<td>denseRank</td>\n</tr>\n<tr>\n<td>percent_rank</td>\n<td>percentRank</td>\n</tr>\n<tr>\n<td>ntile</td>\n<td>ntile</td>\n</tr>\n<tr>\n<td>row_number</td>\n<td>rowNumber</td>\n</tr>\n<tr>\n<td rowspan=\"5\"><strong>Analytic functions</strong></td>\n<td>cume_dist</td>\n<td>cumeDist</td>\n</tr>\n<tr>\n<td>first_value</td>\n<td>firstValue</td>\n</tr>\n<tr>\n<td>last_value</td>\n<td>lastValue</td>\n</tr>\n<tr>\n<td>lag</td>\n<td>lag</td>\n</tr>\n<tr>\n<td>lead</td>\n<td>lead</td>\n</tr>\n</tbody>\n</table>\nTo use window functions, users need to mark that a function is used as a window function by either\n<ul>\n \t<li>Adding an <i>OVER</i> clause after a supported function in SQL, e.g. <code>avg(revenue) OVER (...)</code>; or</li>\n \t<li>Calling the <i>over</i> method on a supported function in the DataFrame API, e.g. <code>rank().over(...)</code><i>.</i></li>\n</ul>\nOnce a function is marked as a window function, the next key step is to define the <i>Window Specification </i>associated with this function. A window specification defines which rows are included in the frame associated with a given input row. A window specification includes three parts:\n<ol>\n \t<li>Partitioning Specification: controls which rows will be in the same partition with the given row. Also, the user might want to make sure all rows having the same value for  the category column are collected to the same machine before ordering and calculating the frame.  If no partitioning specification is given, then all data must be collected to a single machine.</li>\n \t<li>Ordering Specification: controls the way that rows in a partition are ordered, determining the position of the given row in its partition.</li>\n \t<li>Frame Specification: states which rows will be included in the frame for the current input row, based on their relative position to the current row.  For example, \"the three rows preceding the current row to the current row\" describes a frame including the current input row and three rows appearing before the current row.</li>\n</ol>\nIn SQL, the <code>PARTITION BY</code> and <code>ORDER BY</code> keywords are used to specify partitioning expressions for the partitioning specification, and ordering expressions for the ordering specification, respectively. The SQL syntax is shown below.\n\n<code>OVER (PARTITION BY ... ORDER BY ...)</code>\n\nIn the DataFrame API, we provide utility functions to define a window specification. Taking Python as an example, users can specify partitioning expressions and ordering expressions as follows.\n<pre>from pyspark.sql.window import Window\nwindowSpec = \\\n  Window \\\n    .partitionBy(...) \\\n    .orderBy(...)\n</pre>\nIn addition to the ordering and partitioning, users need to define the start boundary of the frame, the end boundary of the frame, and the type of the frame, which are three components of a frame specification.\n\nThere are five types of boundaries, which are<code> UNBOUNDED PRECEDING</code>, <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING</code>. <code>UNBOUNDED PRECEDING</code> and <code>UNBOUNDED FOLLOWING</code> represent the first row of the partition and the last row of the partition, respectively. For the other three types of boundaries, they specify the offset from the position of the current input row and their specific meanings are defined based on the type of the frame. There are two types of frames, <i>ROW</i> frame and <i>RANGE</i> frame.\n\n<b>ROW frame</b>\n\nROW frames are based on physical offsets from the position of the current input row, which means that <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, or <code>&lt;value&gt; FOLLOWING</code> specifies a physical offset. If <code>CURRENT ROW</code> is used as a boundary, it represents the current input row. <code>&lt;value&gt; PRECEDING</code> and <code>&lt;value&gt; FOLLOWING</code> describes the number of rows appear before and after the current input row, respectively. The following figure illustrates a ROW frame with a<code> 1 PRECEDING</code> as the start boundary and <code>1 FOLLOWING</code> as the end boundary (<code>ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING</code> in the SQL syntax).\n\n<img class=\"aligncenter wp-image-4581\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-1-1024x338.png\" alt=\"2-1\" width=\"700\" height=\"231\" />\n\n<b>RANGE frame</b>\n\nRANGE frames are based on logical offsets from the position of the current input row, and have similar syntax to the ROW frame. A logical offset is the difference between the value of the ordering expression of the current input row and the value of that same expression of the boundary row of the frame. Because of this definition, when a RANGE frame is used, only a single ordering expression is allowed. Also, for a RANGE frame, all rows having the same value of the ordering expression with the current input row are considered as same row as far as the boundary calculation is concerned.\n\nNow, let’s take a look at an example. In this example, the ordering expressions is <code>revenue</code>; the start boundary is <code>2000 PRECEDING</code>; and the end boundary is <code>1000 FOLLOWING</code> (this frame is defined as <code>RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING</code> in the SQL syntax). The following five figures illustrate how the frame is updated with the update of the current input row. Basically, for every current input row, based on the value of revenue, we calculate the revenue range <code>[current revenue value - 2000, current revenue value + 1000]</code>. All rows whose revenue values fall in this range are in the frame of the current input row.\n\n<img class=\"aligncenter wp-image-4582\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-2-1024x369.png\" alt=\"2-2\" width=\"700\" height=\"252\" />\n\n<img class=\" wp-image-4583 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-3-1024x263.png\" alt=\"2-3\" width=\"700\" height=\"180\" />\n\n<img class=\" wp-image-4584 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-4-1024x263.png\" alt=\"2-4\" width=\"700\" height=\"180\" />\n\n<img class=\"aligncenter wp-image-4585\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-5-1024x263.png\" alt=\"2-5\" width=\"700\" height=\"180\" />\n\n<img class=\"aligncenter wp-image-4586\" src=\"https://databricks.com/wp-content/uploads/2015/07/2-6-1024x263.png\" alt=\"2-6\" width=\"700\" height=\"180\" />\n\nIn summary, to define a window specification, users can use the following syntax in SQL.\n\n<code>OVER (PARTITION BY ... ORDER BY ... frame_type BETWEEN start AND end)</code>\n\nHere, <code>frame_type</code> can be either ROWS (for ROW frame) or RANGE (for RANGE frame); <code>start</code> can be any of <code>UNBOUNDED PRECEDING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING</code>; and <code>end</code> can be any of <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING.</code>\n\nIn the Python DataFrame API, users can define a window specification as follows.\n<pre>from pyspark.sql.window import Window\n# Defines partitioning specification and ordering specification.\nwindowSpec = \\\n  Window \\\n    .partitionBy(...) \\\n    .orderBy(...)\n# Defines a Window Specification with a ROW frame.\nwindowSpec.rowsBetween(start, end)\n# Defines a Window Specification with a RANGE frame.\nwindowSpec.rangeBetween(start, end)\n</pre>\n<h2>What’s next?</h2>\nSince the release of Spark 1.4, we have been actively working with community members on optimizations that improve the performance and reduce the memory consumption of the operator evaluating window functions. Some of these will be added in Spark 1.5, and others will be added in our future releases. Besides performance improvement work, there are two features that we will add in the near future to make window function support in Spark SQL even more powerful. First, we have been working on adding Interval data type support for Date and Timestamp data types (<a href=\"https://issues.apache.org/jira/browse/SPARK-8943\">SPARK-8943</a>). With the Interval data type, users can use intervals as values specified in <code>&lt;value&gt; PRECEDING</code> and <code>&lt;value&gt; FOLLOWING</code> for RANGE frame, which makes it much easier to do various time series analysis with window functions. Second, we have been working on adding the support for user-defined aggregate functions in Spark SQL (<a href=\"https://issues.apache.org/jira/browse/SPARK-3947\">SPARK-3947</a>). With our window function support, users can immediately use their user-defined aggregate functions as window functions to conduct various advanced data analysis tasks.\n\n<em>To try out these Spark features, <a href=\"https://databricks.com/try-databricks\">get a free trial of Databricks or use the Community Edition</a>.</em>\n<h2>Acknowledgements</h2>\nThe development of the window function support in Spark 1.4 is is a joint work by many members of the Spark community. In particular, we would like to thank Wei Guo for contributing the initial patch.</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog)</td><td>List(2015-07-16, 2015-07-16, UTC)</td><td>This is a joint blog post with our partner Hortonworks. Zhan Zhang is a member of technical staff at Hortonworks, where he collaborated with the Databricks team on this new feature.\n\n<hr />\n\nIn version 1.2.0, Apache Spark introduced a <a href=\"https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">Data Source API</a> (<a href=\"https://issues.apache.org/jira/browse/SPARK-3247\">SPARK-3247</a>) to enable deep platform integration with a larger number of data sources and sinks. We are proud to announce that support for the Apache <a href=\"https://orc.apache.org/\">Optimized Row Columnar</a> (ORC) file format is included in Spark 1.4 as a new data source. This support was added through a collaboration between Hortonworks and Databricks, tracked by <a href=\"https://issues.apache.org/jira/browse/SPARK-2883\">SPARK-2883</a>.\n\nThe Apache ORC file format and associated libraries recently became <a href=\"https://orc.apache.org/\">a top level project</a> at the Apache Software Foundation. ORC is a self-describing type-aware columnar file format designed for Hadoop ecosystem workloads. The columnar format lets the reader read, decompress, and process only the columns that are required for the current query. In addition, it has support for ACID transactions and snapshot isolation, build-in indexes and complex types. Many large Hadoop deployments rely on ORC, including those at Yahoo! and Facebook.\n\nSpark’s ORC support leverages recent improvements to the data source API included in Spark 1.4 (<a href=\"https://issues.apache.org/jira/browse/SPARK-5180\">SPARK-5180</a>). This API makes it easier to bring more data to Spark by simply providing new data source implementations. The API includes support for optimizations such as data partitioning and filter push-down. Since these concepts are now first class in the data source API, new data source implementations only need to focus on the data format specific logic in the physical plan execution without worrying about higher layer query plan optimization.\n\nAs ORC is one of the primary file formats supported in Apache Hive, users of Spark’s SQL and DataFrame APIs will now have fast access to ORC data contained in Hive tables.\n<h2>Accessing ORC in Spark</h2>\nSpark’s ORC data source supports complex data types (i.e., array, map, and struct), and provides read and write access to ORC files. It leverages Spark SQL's Catalyst engine to do common optimizations, such as column pruning, predicate push-down, and partition pruning, etc.\n\nWe’ll now give several examples of Spark’s ORC integration and show how such optimizations are applied to user programs. To get started, Spark’s ORC support requires only a HiveContext instance:\n\n<pre>import org.apache.spark.sql._\nval sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)</pre>\n\nOur examples will use a few data structures to demonstrate working with complex types. The <b>Person</b> struct has name, age, and a sequence of <b>Contact</b>’s, which are themselves defined by names and phone numbers.\n\n<pre>case class Contact(name: String, phone: String)\ncase class Person(name: String, age: Int, contacts: Seq[Contact])</pre>\n\nWe create 100 records as below to be used in our example. In the physical file, they will be saved in the columnar format, but users still see rows when accessing ORC files via DataFrame API. Each row represents one Person record.\n\n<pre>val records = (1 to 100).map { i =>\n    Person(s\"name_$i\", i, (0 to 1).map { m => Contact(s\"contact_$m\", s\"phone_$m\") })\n}</pre>\n\n<h2>Reading and Writing with ORC</h2>\nSpark’s <b>DataFrameReader</b> and <b>DataFrameWriter</b> are used to access ORC files, in a similar manner to other data sources.\n\nWe can write People objects as ORC files to directory “people” using\n\n<pre>sc.parallelize(records).toDF().write.format(\"orc\").save(\"people\")</pre>\n\nFurthermore, we can read it back by\n\n<pre>val people = sqlContext.read.format(\"orc\").load(\"people\")</pre>\n\nFor reuse in future operations, we register it as a temporary table “people” as below:\n\n<pre>people.registerTempTable(\"people\")</pre>\n\n<h2>Column Pruning</h2>\nNow the table is registered as a temporary table named “people”. The following SQL query references two columns from the underlying table. At runtime, the physical table scan will only load columns <b>name</b> and <b>age</b>, without reading the <b>contacts</b> column from the file system, and thus speeds up read performance:\n\n<pre>sql(\"SELECT name FROM people WHERE age < 15\").count()</pre>\n\nORC saves IO bandwidth by only touching required columns, and requires significantly fewer seek operations because all columns within a single stripe are stored together on disk.\n<h2>Predicate Push-down</h2>\nThe columnar nature of the ORC format helps to avoid reading unnecessary columns. However, we are still reading unnecessary rows even if the query has <i>WHERE</i> clause filter. In our example, we have to read all rows with age between 0 and 100, although only the rows with age less than 15 are required and all others will be discarded.  Such full table scanning is an expensive operation.\n\nORC is able to avoid this type of overhead by performing predicate push-down with its build-in indexes.  ORC provides three level of indexes within each file, file level, stripe level, and row level. The file and stripe level statistics are in the file footer so that they are easy to access to determine if the rest of the file needs to be read at all. Row level indexes include both column statistics for each row group and position for seeking to the start of the row group. ORC utilizes these indexes to moves the filter operation to the data loading phase by only reading the data that potentially includes required rows..\n\nThe combination of indexed data and columnar storage reduces disk IO significantly, especially for larger datasets where IO bandwidth becomes the main bottleneck for performance.\n\nBy default, ORC predicate push-down is disabled in the Spark SQL and need to be explicitly enabled:\n\n<pre>sqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\")</pre>\n\n<h2>Partition Pruning</h2>\nWhen predicate pushdown is not applicable, for example if all stripes containing records matching the predicate condition, a query with <i>WHERE</i> clause filter may need to read the entire data set, which becomes a bottleneck over a large table. Partition pruning is another optimization method that can avoid reading large amounts of data by exploiting query semantics.\n\nPartition pruning is possible when data within a table is split across multiple logical partitions. Each partition corresponds to a particular value(s) of partition column(s) and is stored as a sub-directory within the table’s root directory on HDFS. When the table is queried, where applicable, only the required partitions (subdirectories) of the table are queried, thereby avoiding unnecessary IO.\n\nSpark supports saving data out in a partitioned layout seamlessly, through the <b>partitionBy </b>method available during data source writes. In this example we partition the people table by the “age” column:\n\n<pre>person.write.format(\"orc\").partitionBy(\"age\").save(\"peoplePartitioned\")</pre>\n\nRecords will be automatically partitioned by the age field and saved into different directories, for example, peoplePartitioned/age=1/, peoplePartitioned/age=2/, etc.\n\nAfter partitioning the data, future queries which access the data will be able to skip large amounts of IO when the partition column is referenced in predicates. For example, following query will automatically locate and load the file under peoplePartitioned/age=20/ only, and skip all others.\n\n<pre>val peoplePartitioned = sqlContext.read.format(\"orc\").load(\"peoplePartitioned\")\npeoplePartitioned.registerTempTable(\"peoplePartitioned\")\nsql(\"SELECT * FROM peoplePartitioned WHERE age = 20\")</pre>\n\n<h2>DataFrame Support</h2>\nSpark 1.3 added a new DataFrame API. DataFrames look similar to Spark’s RDDs, but have higher level semantics built into their operators, allowing optimization to be pushed down to the underlying query engine. ORC data can be conveniently loaded into DataFrames.\n\nHere's the Scala API translation of the SELECT query above using the DataFrame API\n\n<pre>val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\nsqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\") \nval people = sqlContext.read.format(\"orc\").load(\"peoplePartitioned\")\npeople.filter(people(\"age\")<15).select(\"name\").show()</pre>\n\nAnd of course, DataFrames aren’t limited to Scala, there is a Java API, and, for data scientists, a Python API binding:\n\n<pre>sqlContext = HiveContext(sc)\nsqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\") \npeople = sqlContext.read.format(\"orc\").load(\"peoplePartitioned\")\npeople.filter(people.age < 15).select(\"name\").show()</pre>\n\nThat's it! Simply save your data in ORC, adopt the DataFrame API for working with datasets, and you can significantly speedup queries over large datasets. And we haven't even looked at the compression and run-length encoding features yet—both of which can reduce the IO bandwidth even further.\n<h2>Putting It All Together</h2>\nWe've just given a quick overview of how Spark 1.4 supports ORC files. The combination of the ORC storage format, optimized for query performance, and the DataFrame API means that Spark applications can work with data stored in ORC files as easily as any other data source, yet gain significant performance advantages compared to unoptimized storage formats. And because it can also be used by other tools and applications in the Hadoop stack, ORC-formatted data generated by other parts of a large system, can be easily consumed by Spark applications and other interactive tools.\n<h2>What’s Next?</h2>\nCurrently, the code for Spark SQL ORC support is under package org.apache.spark.sql.hive and must be used together with Spark SQL's HiveContext. This is because ORC is still tightly coupled with Hive for now. However, it doesn't require existing Hive installation to access ORC files.\n\nNow that ORC has already become an independent Apache top level project. After decoupling ORC from Hive, Hive dependencies will not be necessary to access ORC files.\n\nWe look forward to helping producing a future version of Apache Spark which makes ORC even easier to work with.\n<h2>Further Information</h2>\nIf you want to know more about Spark's ORC Support, <a href=\"http://spark.apache.org/\">download Apache Spark</a> 1.4.0 or later versions, and explore the new features through the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">DataFrame API</a>.\n<h2>References</h2>\n<ul>\n\t<li>Apache ORC website: <a href=\"https://orc.apache.org/\">https://orc.apache.org/</a></li>\n\t<li>ORC performance: <a href=\"http://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/\">http://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/</a></li>\n</ul></td></tr><tr><td>null</td><td>List(Announcements, Company Blog)</td><td>List(2015-07-21, 2015-07-21, UTC)</td><td><p dir=\"ltr\"><strong>Please note that this survey is now closed.</strong></p>\n<p dir=\"ltr\">At Databricks, we are constantly working to improve Apache Spark. To help us and the Spark community, we would love to hear from you to help set Spark’s future direction. A recent example of the community helping to direct Spark would be SparkR. As noted in the Datanami article <a href=\"http://www.datanami.com/2015/07/13/python-versus-r-in-apache-spark/\" target=\"_blank\">Python Versus R in Apache Spark</a>, we were bombarded with requests over the past year to add support for R in Apache Spark.  Because of the overwhelming response, together with the Spark community we released SparkR as part of last month's <a href=\"https://databricks.com/blog/2015/06/11/announcing-apache-spark-1-4.html\" target=\"_blank\">Spark 1.4 release</a>.</p>\n<p dir=\"ltr\">To give you a voice in future releases of Apache Spark, we have designed a short 15-min survey.  In it we would like to better understand what technology integrations you are building,  your primary scenarios for using Spark, and the type of environments you are working in.  We hope you will take some time to fill it out to help us better understand your needs and to improve Spark.</p>\n<p dir=\"ltr\">In appreciation, you will be entered to win one of <strong>THREE</strong> prizes once you complete the survey: $200 Visa card, iPad Mini, or Beats headphones.</p>\nThank you and we hope to hear from you!\n\nTake the <a href=\"https://docs.google.com/forms/d/1gczrU_-oz_GQyeTfDKzc-KkNO3iTi0-mCp-fSRh3Y6c/viewform\" target=\"_blank\">Spark Survey</a></td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-07-23, 2015-07-23, UTC)</td><td>We are happy to announce that Yesware chose Databricks to build its production data pipeline, completing the project in record time -- in just under three weeks.\n\nPress release: <a href=\"http://www.marketwired.com/press-release/yesware-deploys-production-data-pipeline-in-record-time-with-databricks-2041188.htm\" target=\"_blank\">http://www.marketwired.com/press-release/yesware-deploys-production-data-pipeline-in-record-time-with-databricks-2041188.htm</a>\n\n<a href=\"http://www.yesware.com/\" target=\"_blank\">Yesware</a>, the leading sales acceleration software for sales teams at major enterprise companies such as eBay, New Relic, and IBM, enables sales professionals to have highly effective and successful engagements by providing analytics on their daily interactions with potential customers using billions of data points, including open and reply rate of emails, effectiveness of email templates, engagement rates of e-mails, CTA click-through’s, and more.\n\nYesware encountered many difficulties when it first attempted to build and operate a production data pipeline. It needed to ingest a large volume of data (hundreds of GB per day, and growing rapidly), build highly customizable reports, and must do all of the above with minimum latency. The initial solution was too slow, difficult to maintain, and just not scalable enough. Yesware decided it was time they sourced a better solution.\n\nApache Spark became the big data technology of choice because of its flexible and easy-to-learn API, fast performance, and native support for crucial capabilities such as SQL and machine learning algorithms. Yesware chose Databricks to implement and operationalize a Spark data pipeline. Why? Because the Databricks platform makes building and running production Spark applications much faster and easier with its cluster manager, interactive workspace, and <a href=\"https://databricks.com/blog/2015/03/18/databricks-launches-jobs-feature-for-production-workloads.html\" target=\"_blank\">job scheduler</a>. Databricks also easily integrated with Yesware's existing infrastructure in Amazon Web Services (AWS), further accelerating their adoption and on-ramp processes.\n\nThe benefits Yesware gained with Databricks included:\n<ul>\n\t<li>Deploying a production data pipeline in just under three weeks, compared to over six months with the prior solution.</li>\n\t<li>Processing over 180 days of historical data in two hours. This was a drastic speedup compared to their prior solution that took 12 hours to process 90 days of data.</li>\n\t<li>Improving the efficiency of their infrastructure by reducing the cost to just 10% of the prior solution.</li>\n\t<li>Accelerating the development of new features, enabling their developers and data scientists to collaborate seamlessly, reducing time to prototype new algorithm to hours instead of days.</li>\n</ul>\nDownload this <a title=\"Customer Case Studies\" href=\"https://databricks.com/wp-content/uploads/2015/07/Databricks_Case_Study_Yesware.pdf\" target=\"_blank\">case study</a> to learn more about how Yesware is using Databricks.\n\nTo try out Databricks for yourself, <a href=\"http://dbricks.co/1g8cHFA\" target=\"_blank\">sign-up for a 14-day free trial of Databricks today</a>!</td></tr><tr><td>null</td><td>List(Company Blog, Product)</td><td>List(2015-07-28, 2015-07-28, UTC)</td><td>In an earlier post, we described how you can <a href=\"https://databricks.com/blog/2015/06/05/making-databricks-cloud-better-for-developers-ide-integration.html\">easily integrate your favorite IDE with Databricks</a> to speed up your application development. In this post, we will show you how to import 3rd party libraries, specifically Apache Spark packages, into Databricks by providing Maven coordinates.\n<h2>Background on Spark Packages</h2>\n<img class=\" wp-image-4690 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/spark-packages-1024x584.png\" alt=\"spark-packages\" width=\"599\" height=\"342\" />\n\nSpark Packages (<a href=\"http://spark-packages.org\">http://spark-packages.org</a>) is a community package index for libraries built on top of Apache Spark. The purpose of Spark Packages is to bridge the gap between Spark developers and users. Without Spark Packages, you need to to go multiple repositories, such as GitHub, PyPl, and Maven Central, to find the libraries you want. This makes the search for a package that fits your needs a pain - the goal of Spark Packages is to simplify this process for you by becoming the one-stop-shop for your search.\n\nAt the time of this writing, there are 95 packages on Spark Packages, with a number of new packages appearing daily. These packages range from pluggable data sources and data formats for DataFrames (such as spark-csv, spark-avro, spark-redshift, spark-cassandra-connector, hbase) to machine learning algorithms, to deployment scripts that enable Spark deployment in cloud environments.\n<h2>Support for Spark Packages and Maven libraries in Databricks</h2>\nDid you know that you could download libraries from any public Maven repository, including all its dependencies, with a few clicks to Databricks? Databricks provides you with a browser that allows you to search both Spark Packages and Maven Central. Here’s how it all works:\n\nSelect where you would like to create the library in the <strong>Workspace</strong>, and open the <strong>Create Library</strong> dialog:\n\n<img class=\" wp-image-4691 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/create-lib.png\" alt=\"create-lib\" width=\"300\" height=\"168\" />\n\nFrom the <strong>Source</strong> drop-down menu, select <strong>Maven Coordinate</strong>:\n\n<img class=\" wp-image-4692 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/select-maven-1024x711.png\" alt=\"select-maven\" width=\"400\" height=\"278\" />\n\nIf you already know the Maven coordinate, you can enter it directly to create the library in Databricks instantly. Alternatively, you can also browse Maven libraries and Spark Packages to look through your options by clicking the <strong>Search Spark Packages and Maven Central</strong> button without entering a coordinate.\n\n<img class=\" wp-image-4693 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/maven-page-1024x571.png\" alt=\"maven-page\" width=\"400\" height=\"223\" />\n\nNow, all available Spark Packages are at your fingertips! You can sort packages by name, organization, and rating. You can also filter the results by writing a query in the search bar. The results will automatically refresh.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/browser.png\"><img class=\"aligncenter wp-image-4694\" src=\"https://databricks.com/wp-content/uploads/2015/07/browser-1024x548.png\" alt=\"browser\" width=\"800\" height=\"428\" /></a>\n\n&nbsp;\n\nIf you want to find out more details about a package, simply click on its name in the browser.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/sp-detail.png\"><img class=\" wp-image-4695 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/sp-detail-1024x561.png\" alt=\"sp-detail\" width=\"800\" height=\"438\" /></a>\n\n&nbsp;\n\nTo browse <strong>Maven Central</strong>, by select the Maven Central option from the drop-down menu on the top right.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/maven-central.png\"><img class=\"aligncenter wp-image-4696\" src=\"https://databricks.com/wp-content/uploads/2015/07/maven-central-1024x561.png\" alt=\"maven-central\" width=\"800\" height=\"438\" /></a>\n\n&nbsp;\n\nAfter you identify the package you are interested in, you can choose the release version from the drop-down menu. Once you click the <strong>Select</strong> button, the <strong>Maven Coordinate</strong> field from the previous menu will be automatically filled for you.\n\nYou can also provide more advanced options, such as the URL for a repository, and any dependencies that you would like to exclude (in case of dependency conflicts).\n\n<img class=\"aligncenter wp-image-4698\" src=\"https://databricks.com/wp-content/uploads/2015/07/resolving-1024x757.png\" alt=\"resolving\" width=\"400\" height=\"296\" />\n\n&nbsp;\n\nOnce you click the <strong>Create Library</strong> button, and the library and all its dependencies will be fetched for you automatically!\n\n<img class=\"aligncenter wp-image-4699\" src=\"https://databricks.com/wp-content/uploads/2015/07/resolved-1024x311.png\" alt=\"resolved\" width=\"600\" height=\"182\" />\n\n&nbsp;\n\nNow you can attach it to any cluster that you wish, and start using the library immediately!\n<h2>Summary</h2>\nIn this post, we showed how simple it is to integrate Spark Packages to your applications in Databricks using Maven coordinate support. Spark Packages help you to find the code you need to get the job done, and its tight integration with Databricks makes reusing existing code to speed up your Spark application development even simpler.\n\nTo try out Spark Packages in Databricks for yourself, <a href=\"http://dbricks.co/1g8cHFA\" target=\"_blank\">sign-up for a 14-day free trial today</a>!</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Machine Learning)</td><td>List(2015-07-29, 2015-07-29, UTC)</td><td>Apache Spark 1.2 introduced Machine Learning (ML) Pipelines to facilitate the creation, tuning, and inspection of practical ML workflows. Spark’s latest release, Spark 1.4, significantly extends the ML library.  In this post, we highlight  several new features in the ML Pipelines API, including:\n<ul>\n\t<li>A stable API — <i>Pipelines have graduated from Alpha!</i></li>\n\t<li>New feature transformers</li>\n\t<li>Additional ML algorithms</li>\n\t<li>A more complete Python API</li>\n\t<li>A pluggable API for customized, third-party Pipeline components</li>\n</ul>\nIf you’re new to using ML Pipelines, you can get familiar with the key concepts like Transformers and Estimators by reading our previous <a href=\"https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html\">blog post</a>.\n<h2>New Features in Spark 1.4</h2>\nWith significant contributions from the Spark community, ML Pipelines are much more featureful in the 1.4 release.  The API includes many common feature transformers and more algorithms.\n<h2>New Feature Transformers</h2>\nA big part of any ML workflow is massaging the data into the right features for use in downstream processing.  To simply feature extraction, Spark provides many feature transformers out-of-the-box.  The table below outlines most of the feature transformers available in Spark 1.4 along with descriptions of each one. Much of the API is inspired by scikit-learn; for reference, we provide names of similar scikit-learn transformers where available.\n<table class=\"table\">\n<tbody>\n<tr>\n<td><b>Transformer</b></td>\n<td><b>Description</b></td>\n<td><b>scikit-learn</b></td>\n</tr>\n<tr>\n<td>Binarizer</td>\n<td>Threshold numerical feature to binary</td>\n<td>Binarizer</td>\n</tr>\n<tr>\n<td>Bucketizer</td>\n<td>Bucket numerical features into ranges</td>\n<td></td>\n</tr>\n<tr>\n<td>ElementwiseProduct</td>\n<td>Scale each feature/column separately</td>\n<td></td>\n</tr>\n<tr>\n<td>HashingTF</td>\n<td>Hash text/data to vector. Scale by term frequency</td>\n<td>FeatureHasher</td>\n</tr>\n<tr>\n<td>IDF</td>\n<td>Scale features by inverse document frequency</td>\n<td>TfidfTransformer</td>\n</tr>\n<tr>\n<td>Normalizer</td>\n<td>Scale each row to unit norm</td>\n<td>Normalizer</td>\n</tr>\n<tr>\n<td>OneHotEncoder</td>\n<td>Encode k-category feature as binary features</td>\n<td>OneHotEncoder</td>\n</tr>\n<tr>\n<td>PolynomialExpansion</td>\n<td>Create higher-order features</td>\n<td>PolynomialFeatures</td>\n</tr>\n<tr>\n<td>RegexTokenizer</td>\n<td>Tokenize text using regular expressions</td>\n<td>(part of <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\">text methods</a>)</td>\n</tr>\n<tr>\n<td>StandardScaler</td>\n<td>Scale features to 0 mean and/or unit variance</td>\n<td>StandardScaler</td>\n</tr>\n<tr>\n<td>StringIndexer</td>\n<td>Convert String feature to 0-based indices</td>\n<td>LabelEncoder</td>\n</tr>\n<tr>\n<td>Tokenizer</td>\n<td>Tokenize text on whitespace</td>\n<td>(part of <a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\">text methods</a>)</td>\n</tr>\n<tr>\n<td>VectorAssembler</td>\n<td>Concatenate feature vectors</td>\n<td>FeatureUnion</td>\n</tr>\n<tr>\n<td>VectorIndexer</td>\n<td>Identify categorical features, and index</td>\n<td></td>\n</tr>\n<tr>\n<td>Word2Vec</td>\n<td>Learn vector representation of words</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n* Only 3 of the above transformers were available in Spark 1.3 (HashingTF, StandardScaler, and Tokenizer).\n\nThe following code snippet demonstrates how multiple feature encoders can be strung together into a complex workflow. This example begins with two types of features: <em>text</em> (String) and <em>userGroup</em> (categorical).  For example:\n\n<img class=\"aligncenter wp-image-4712\" src=\"https://databricks.com/wp-content/uploads/2015/07/table-data.png\" alt=\"table data\" width=\"400\" height=\"196\" />\n\nWe generate text features using both hashing and the Word2Vec algorithm, and then apply a one-hot encoding to <em>userGroup</em>.  Finally, we combine all features into a single feature vector which can be used by ML algorithms such as Logistic Regression.\n\n<pre>from pyspark.ml.feature import *\nfrom pyspark.ml import Pipeline\ntok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhtf = HashingTF(inputCol=\"words\", outputCol=\"tf\", numFeatures=200)\nw2v = Word2Vec(inputCol=\"text\", outputCol=\"w2v\")\nohe = OneHotEncoder(inputCol=\"userGroup\", outputCol=\"ug\")\nva = VectorAssembler(inputCols=[\"tf\", \"w2v\", \"ug\"], outputCol=\"features\")\npipeline = Pipeline(stages=[tok,htf,w2v,ohe,va])</pre>\n\nThe following diagram shows the full pipeline. Pipeline stages are shown as blue boxes, and DataFrame columns are shown as bubbles.\n\n<a href=\"https://databricks.com/wp-content/uploads/2015/07/simple-pipeline.png\"><img class=\" size-full wp-image-4713 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/simple-pipeline.png\" alt=\"simple pipeline\" width=\"436\" height=\"648\" /></a>\n<h2>Better Algorithm Coverage</h2>\nIn Spark 1.4, the Pipelines API now includes trees and ensembles: <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">Decision Trees</a>, <a href=\"https://en.wikipedia.org/wiki/Random_forest\">Random Forests</a>, and <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting\">Gradient-Boosted Trees</a>.  These are some of the most important algorithms in machine learning.  They can be used for both regression and classification, are flexible enough to handle many types of applications, and can use both continuous and categorical features.\n\nThe Pipelines API also includes Logistic Regression and Linear Regression using <a href=\"https://en.wikipedia.org/wiki/Elastic_net_regularization\">Elastic Net regularization</a>, an important statistical tool mixing L1 and L2 regularization.\n\nSpark 1.4 also introduces OneVsRest (a.k.a. One-Vs-All), which converts any binary classification \"base\" algorithm into a multiclass algorithm.  This flexibility to use any base algorithm in OneVsRest highlights the versatility of the Pipelines API.  By using DataFrames, which support varied data types, OneVsRest can remain oblivious to the specifics of the base algorithm.\n<h2>More Complete Python API</h2>\nML Pipelines have a near-complete Python API in Spark 1.4.  Python APIs have become much simpler to implement after significant improvements to internal Python APIs, plus the unified DataFrame API.  See the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html\">Python API docs for ML Pipelines</a> for a full feature list.\n<h2>Customizing Pipelines</h2>\nWe have opened up APIs for users to write their own Pipeline stages.  If you need a custom feature transformer, ML algorithm, or evaluation metric in your workflow, you can write your own and plug it into ML Pipelines.  Stages communicate via DataFrames, which act as a simple, flexible API for passing data through a workflow.\n\nThe key abstractions are:\n<ul>\n\t<li><b>Transformer</b>: This includes feature transformers (e.g., OneHotEncoder) and trained ML models (e.g., LogisticRegressionModel).</li>\n\t<li><b>Estimator</b>: This includes ML algorithms for training models (e.g., LogisticRegression).</li>\n\t<li><b>Evaluator</b>: These evaluate predictions and compute metrics, useful for tuning algorithm parameters (e.g., BinaryClassificationEvaluator).</li>\n</ul>\nTo learn more, start with the overview of ML Pipelines in the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">ML Pipelines Programming Guide</a>.\n<h2>Looking Ahead</h2>\nThe roadmap for Spark 1.5 includes:\n<ul>\n\t<li><i>API</i>: More complete algorithmic coverage in Pipelines, and more featureful Python API.  There is also initial work towards an MLlib API in Spark R.</li>\n\t<li><i>Algorithms</i>: More feature transformers (such as CountVectorizer, DiscreteCosineTransform, MinMaxScaler, and NGram) and algorithms (such as KMeans clustering and Naive Bayes).</li>\n\t<li><i>Developers</i>: Improvements for developers, including to the feature attributes API and abstractions.</li>\n</ul>\nML Pipelines do not yet cover all algorithms in MLlib, but the two APIs can interoperate.  If your workflow requires components from both APIs, all you need to do is convert between RDDs and DataFrames.  For more information on conversions, see the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds\">DataFrame guide</a>.\n<h2>Acknowledgements</h2>\nThanks very much to the community contributors during this release!  You can find a complete list of JIRAs for ML Pipelines with contributors on the <a href=\"https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20in%20(1.4.0%2C%201.4.1)%20AND%20component%20%3D%20ML%20ORDER%20BY%20priority%20DESC\">Apache Spark JIRA</a>.\n<h2>Learning More</h2>\nTo get started, <a href=\"http://spark.apache.org/downloads.html\">download Spark 1.4</a> and check out the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">ML Pipelines User Guide</a>!  Also try out the <a href=\"https://github.com/apache/spark/tree/07f778978d80f0af57d3dafda4c566a813ad2d09/examples/src/main/scala/org/apache/spark/examples/ml\">ML package code examples</a>. Experts can get started writing their own Transformers and Estimators by looking at the <a href=\"https://github.com/apache/spark/blob/07f778978d80f0af57d3dafda4c566a813ad2d09/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\">DeveloperApiExample code snippet</a>.\n\nTo contribute, follow the <a href=\"https://issues.apache.org/jira/browse/SPARK-8445\">MLlib 1.5 Roadmap JIRA</a>.  Good luck!</td></tr><tr><td>null</td><td>List(Apache Spark, Engineering Blog, Streaming)</td><td>List(2015-07-30, 2015-07-30, UTC)</td><td>With so many distributed stream processing engines available, people often ask us about the unique benefits of Apache Spark Streaming. From early on, Apache Spark has provided an unified engine that natively supports both batch and streaming workloads. This is different from other systems that either have a processing engine designed only for streaming, or have similar batch and streaming APIs but compile internally to different engines. Spark’s single execution engine and unified programming model for batch and streaming lead to some unique benefits over other traditional streaming systems. In particular, four major aspects are:\n<ul>\n\t<li>Fast recovery from failures and stragglers</li>\n\t<li>Better load balancing and resource usage</li>\n\t<li>Combining of streaming data with static datasets and interactive queries</li>\n\t<li>Native integration with advanced processing libraries (SQL, machine learning, graph processing)</li>\n</ul>\nIn this post, we outline Spark Streaming’s architecture and explain how it provides the above benefits. We also discuss some of the interesting ongoing work in the project that leverages the execution model.\n<h2>Stream Processing Architectures - The Old and the New</h2>\nAt a high level, modern distributed stream processing pipelines execute as follows:\n<ol>\n\t<li><b>Receive </b>streaming data from data sources (e.g. live logs, system telemetry data, IoT device data, etc.) into some data ingestion system like Apache Kafka, Amazon Kinesis, etc.</li>\n\t<li><b>Process</b> the data in parallel on a cluster. This is what stream processing engines are designed to do, as we will discuss in detail next.</li>\n\t<li><b>Output</b> the results out to downstream systems like HBase, Cassandra, Kafka, etc.</li>\n</ol>\nTo process the data, most traditional stream processing systems are designed with a <i>continuous operator model</i>, which works as follows:\n<ul>\n\t<li>There is a set of worker nodes, each of which run one or more <b>continuous operators</b>.</li>\n\t<li>Each continuous operator processes the streaming data one record at a time and forwards the records to other operators in the pipeline.</li>\n\t<li>There are “source” operators for receiving data from ingestion systems, and “sink” operators that output to downstream systems.</li>\n</ul>\n<p style=\"text-align: center;\"><img class=\" wp-image-4727 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/07/image11-1024x655.png\" alt=\"image1\" width=\"600\" height=\"384\" /><em>Figure 1: Architecture of traditional stream processing systems</em></p>\nContinuous operators are a simple and natural model. However, with today’s trend towards larger scale and more complex real-time analytics, this traditional architecture has also met some challenges. We designed Spark Streaming to satisfy the following requirements:\n<ul>\n\t<li><em>Fast failure and straggler recovery</em> - With greater scale, there is a higher likelihood of a cluster node failing or unpredictably slowing down (i.e. stragglers). The system must be able to automatically recover from failures and stragglers to provide results in real time. Unfortunately, the static allocation of continuous operators to worker nodes makes it challenging for traditional systems to recover quickly from faults and stragglers.</li>\n</ul>\n<ul>\n\t<li><em>Load balancing</em> - Uneven allocation of the processing load between the workers can cause bottlenecks in a continuous operator system. This is more likely to occur in large clusters and dynamically varying workloads. The system needs to be able to dynamically adapt the resource allocation based on the workload.</li>\n</ul>\n<ul>\n\t<li><i>Unification of streaming, batch and interactive workloads</i> - In many use cases, it is also attractive to query the streaming data interactively (after all, the streaming system has it all in memory), or to combine it with static datasets (e.g. pre-computed models). This is hard in continuous operator systems as they are not designed to the dynamically introduce new operators for ad-hoc queries. This requires a single engine that can combine batch, streaming and interactive queries.</li>\n</ul>\n<ul>\n\t<li><i>Advanced analytics like machine learning and SQL</i> <i>queries</i> - More complex workloads require continuously learning and updating data models, or even querying the “latest” view of streaming data with SQL queries. Again, having a common abstraction across these analytic tasks makes the developer’s job much easier.</li>\n</ul>\nTo address these requirements, Spark Streaming uses a new architecture called <a href=\"http://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf\" target=\"_blank\">discretized streams</a> that directly leverages the rich libraries and fault tolerance of the Spark engine.\n<h2>Architecture of Spark Streaming: Discretized Streams</h2>\nInstead of processing the streaming data one record at a time, Spark Streaming discretizes the streaming data into tiny, sub-second micro-batches. In other words, Spark Streaming’s Receivers accept data in parallel and buffer it in the memory of Spark’s workers nodes. Then the latency-optimized Spark engine runs short tasks (tens of milliseconds) to process the batches and output the results to other systems. Note that unlike the traditional continuous operator model, where the computation is statically allocated to a node, Spark tasks are assigned dynamically to the workers based on the locality of the data and available resources. This enables both better load balancing and faster fault recovery, as we will illustrate next.\n\nIn addition, each batch of data is a Resilient Distributed Dataset (RDD), which is the basic abstraction of a fault-tolerant dataset in Spark. This allows the streaming data to be processed using any Spark code or library.\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4730\" src=\"https://databricks.com/wp-content/uploads/2015/07/image21-1024x734.png\" alt=\"image2\" width=\"600\" height=\"430\" /><em>Figure 2: Spark Streaming Architecture</em></p>\n&nbsp;\n<h2>Benefits of Discretized Stream Processing</h2>\n<p style=\"text-align: left;\">Let’s see how this architecture allows Spark Streaming to achieve the goals we set earlier.</p>\n\n<h3>Dynamic load balancing</h3>\nDividing the data into small micro-batches allows for fine-grained allocation of computations to resources. For example, consider a simple workload where the input data stream needs to partitioned by a key and processed. In the traditional record-at-a-time approach taken by most other systems, if one of the partitions is more computationally intensive than the others, the node statically assigned to process that partition will become a bottleneck and slow down the pipeline. In Spark Streaming, the job’s tasks will be naturally load balanced across the workers -- some workers will process a few longer tasks, others will process more of the shorter tasks.\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4732\" src=\"https://databricks.com/wp-content/uploads/2015/07/image31-1024x581.png\" alt=\"image3\" width=\"600\" height=\"340\" /><em>Figure 3: Dynamic load balancing</em></p>\n&nbsp;\n<h3>Fast failure and straggler recovery</h3>\n<p style=\"text-align: left;\">In case of node failures, traditional systems have to restart the failed continuous operator on another node and replay some part of the data stream to recompute the lost information. Note that only one node is handling the recomputation, and the pipeline cannot proceed until the new node has caught up after the replay. In Spark, the computation is already discretized into small, deterministic tasks that can run anywhere without affecting correctness. So failed tasks can be relaunched <i>in</i> <i>parallel</i> on all the other nodes in the cluster, thus evenly distributing all the recomputations across many nodes, and recovering from the failure faster than the traditional approach.</p>\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4733\" src=\"https://databricks.com/wp-content/uploads/2015/07/image41-1024x602.png\" alt=\"image4\" width=\"600\" height=\"353\" /><em>Figure 4: Faster failure recovery with redistribution of computation</em></p>\n&nbsp;\n<h3>Unification of batch, streaming and interactive analytics</h3>\n<p style=\"text-align: left;\">The key programming abstraction in Spark Streaming is a DStream, or distributed stream. Each batch of streaming data is represented by an RDD, which is Spark’s concept for a distributed dataset. Therefore a DStream is just a series of RDDs. This common representation allows batch and streaming workloads to interoperate seamlessly. Users can apply arbitrary Spark functions on each batch of streaming data: for example, it’s easy to join a DStream with a precomputed static dataset (as an RDD).</p>\n\n<pre>// Create data set from Hadoop file\nval dataset = sparkContext.hadoopFile(\"file\")\n// Join each batch in stream with the dataset\nkafkaDStream.transform { batchRDD =>\n  batchRDD.join(dataset).filter(...)\n}</pre>\n\nSince the batches of streaming data are stored in the Spark’s worker memory, it can be interactively queried on demand. For example, you can expose all the streaming state through the Spark SQL JDBC server, as we will show in the next section. This kind of unification of batch, streaming and interactive workloads is very simple in Spark, but hard to achieve in systems without a common abstraction for these workloads.\n<h3>Advanced analytics like machine learning and interactive SQL</h3>\nSpark interoperability extends to rich libraries like MLlib (machine learning), SQL, DataFrames, and GraphX. Let’s explore a few use cases:\n<h4>Streaming + SQL and DataFrames</h4>\nRDDs generated by DStreams can be converted to DataFrames (the programmatic interface to Spark SQL), and queried with SQL. For example, using Spark SQL’s <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server\">JDBC server</a>, you can expose the state of the stream to any external application that talks SQL.\n\n<pre>val hiveContext = new HiveContext(sparkContext)\n// ...\nwordCountsDStream.foreachRDD { rdd =>\n  // Convert RDD to DataFrame and register it as a SQL table\n  val wordCountsDataFrame = rdd.toDF(\"word\", \"count\") \n  wordCountsDataFrame.registerTempTable(\"word_counts\") \n}\n// ...\n// Start the JDBC server\nHiveThriftServer2.startWithContext(hiveContext)</pre>\n\nThen you can interactively query the continuously updated “word_counts” table through the JDBC server, using the <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server\">beeline</a> client that ships with Spark, or tools like Tableau.\n\n<pre>> show tables;\n+--------------+--------------+\n|  tableName   | isTemporary  |\n+--------------+--------------+\n| word_counts  | true         |\n+--------------+--------------+\n1 row selected (0.102 seconds)\n\n> select * from word_counts;\n+-----------+--------+\n|   word    | count  |\n+-----------+--------+\n| 2015      | 264    |\n| PDT       | 264    |\n| 21:45:41  | 27     |</pre>\n\n<h4>Streaming + MLlib</h4>\nMachine learning models generated offline with MLlib can applied on streaming data. For example, the following code trains a KMeans clustering model with some static data and then uses the model to classify events in a Kafka data stream.\n\n<pre>// Learn model offline\nval model = KMeans.train(dataset, ...)\n// Apply model online on stream\nval kafkaStream = KafkaUtils.createDStream(...)\nkafkaStream.map { event => model.predict(featurize(event)) }</pre>\n\nWe demonstrated this offline-learning-online-prediction at our Spark Summit 2014 Databricks demo. Since then, we have also added streaming machine learning algorithms in MLLib that can continuously train from a labelled data stream. Other Spark libraries can also easily be called from Spark Streaming.\n<h2>Performance</h2>\nGiven the unique design of Spark Streaming, how fast does it run? In practice, Spark Streaming's ability to batch data and leverage the Spark engine leads to comparable or <a href=\"https://spark-summit.org/2015/events/towards-benchmarking-modern-distributed-streaming-systems/\">higher throughput</a> to other streaming systems. In terms of latency, Spark Streaming can achieve latencies as low as a few hundred milliseconds. Developers sometimes ask whether the micro-batching inherently adds too much latency. In practice, batching latency is only a small component of end-to-end pipeline latency. For example, many applications compute results over a sliding window, and even in continuous operator systems, this window is only updated periodically (e.g. a 20 second window that slides every 2 seconds). Many pipelines collect records from multiple sources and wait for a short period to process delayed or out-of-order data. Finally, any automatic triggering algorithm tends to wait for some time period to fire a trigger. Therefore, compared to the end-to-end latency, batching rarely adds significant overheads. In fact, the throughput gains from DStreams often means that you need fewer machines to handle the same workload.\n<h2>Future Directions for Spark Streaming</h2>\nSpark Streaming is one of the most widely used components in Spark, and there is a lot more coming for streaming users down the road. Some of the highest priority items our team is working on are discussed below. You can expect these in the next few releases of Spark:\n<ul>\n\t<li><i>Backpressure</i> - Streaming workloads can often have bursts of data (e.g. sudden spike in tweets during the Oscars) and the processing system must be able to handle them gracefully. In the upcoming Spark 1.5 release (next month), Spark will be adding better backpressure mechanisms that allow Spark Streaming dynamically control the ingestion rate for such bursts. This feature represents joint work between us at Databricks and engineers at Typesafe.</li>\n</ul>\n<ul>\n\t<li><i>Dynamic scaling</i> - Controlling the ingestion rate may not be sufficient to handle longer terms variations in data rates (e.g. sustained higher tweet rate during the day than night). Such variations can be handled by dynamically scaling the cluster resource based on the processing demands. This is very easy to do within the Spark Streaming architecture -- since the computation is already divided into small tasks, they can be dynamically redistributed to a larger cluster if more nodes are acquired from the cluster manager (YARN, Mesos, Amazon EC2, etc). We plan to add support for automatic dynamic scaling.</li>\n</ul>\n<ul>\n\t<li><i>Event time and out-of-order data </i>- In practice, users sometimes have records that are delivered out of order, or with a timestamp that differs from the time of ingestion. Spark streaming will support “event time” by allowing user-defined time extraction function. This will include a slack duration for late or out-of-order data.</li>\n</ul>\n<ul>\n\t<li><i>UI enhancements</i> - Finally, we want to make it easy for developers to debug their streaming applications. For this purpose, in Spark 1.4, we added <a href=\"https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-spark-streaming-applications.html\">new visualizations to the streaming Spark UI</a> that let developers closely monitor the performance of their application. In Spark 1.5, we are further improving this by showing more <a href=\"https://issues.apache.org/jira/browse/SPARK-8701\">input information like Kafka offsets</a> processed in each batch.</li>\n</ul>\nTo learn more about Spark Streaming, read the <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html\">official programming guide</a>, or the <a href=\"http://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf\" target=\"_blank\">Spark Streaming research paper</a> that introduces its execution and fault tolerance model.</td></tr><tr><td>null</td><td>List(Announcements, Company Blog, Product)</td><td>List(2015-08-05, 2015-08-05, UTC)</td><td>When we started Databricks, we thought that extracting insights from big data was insanely difficult for no good reason. You almost needed an advanced degree to be able to get any meaningful work done. As a result, only a select few in each organization could ask questions from their big data, the people who set up the clusters and knew how to use advanced tools such as Hive and MapReduce. We therefore set out to build a software as a service product that would drastically simplify big data processing.\n\nSoon after launching we learned something interesting. In many organizations, a democratization effort was taking place with more and more people starting to use Databricks to ask questions from the data. They were no longer bottlenecked by the chosen few that knew how to talk to the data. However, as some organizations had over hundreds of users using Databricks, a new set of challenges had risen. First, users wanted to control access to their data. Second, they wanted version control and management of multiple Apache Spark versions. Third, they wanted R-support. These requirements were all interlinked. I'm proud to announce that after much hard work, we are now releasing Databricks with all these features. Below I explain the story of how each of these came about and the lessons that are behind these features.\n\nAs employees with different functions started asking questions from the data, it very soon became a hard requirement to be able to control who in their organization should see or modify their queries, which could contain very sensitive information or could not be shared due to security compliance reasons. This is natural in a large organization. In our case, this requirement became even more important because we had developed a new way in which hundreds of users could use separate notebooks on the same shared Spark cluster, enabling huge cost savings for their organizations. This was not possible before, as before this feature, each notebook and user would have to have a separate isolated cluster. By enabling such cluster sharing, it was even more important that your coworkers couldn't snoop on your most sensitive notebooks. Databricks now comes with access control features that let you control who can see, who can run and parameterize, and who can edit and manage your notebooks. We are the first vendor to offer this feature for Spark.\n<p style=\"text-align: center;\"><img class=\" wp-image-4759 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-04-at-9.59.01-PM1-1024x540.png\" alt=\"Screen Shot 2015-08-04 at 9.59.01 PM\" width=\"400\" height=\"211\" /><em>Setting permissions in Databricks</em></p>\nWe tried from day one to make it really easy to collaborate on live notebooks, with features such as real-time updates and commenting features. But as collaboration started to seriously happen, users wanted to have auditability. Who modified my notebooks and how can I go back to an earlier version? Furthermore, many users were already using external version control systems, such as GitHub. Finally, many users wanted to sometimes explore some new features of a Spark release on a small experimental cluster, but continue to use old Spark versions on production clusters. As they gained more experience with the new Spark version, they would like to reuse their old notebooks on the new Spark version. Hence, they wanted to manage multiple Spark versions and be able to easily go between these with their jobs and notebooks. Databricks current release now comes with these features for version control, GitHub integration, and management of multiple Spark versions.\n<p style=\"text-align: center;\"><a href=\"https://databricks.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-05-at-7.28.06-AM.png\"><img class=\" wp-image-4761 aligncenter\" src=\"https://databricks.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-05-at-7.28.06-AM-1024x513.png\" alt=\"Screen Shot 2015-08-05 at 7.28.06 AM\" width=\"800\" height=\"401\" /></a><em>Notebook version control</em></p>\nFinally, as more job functions started asking questions from the data, we heard that more and wanted to use R as their preferred language to talk to the data. SQL and Python were already supported for a while and these were wildly popular. But we did not have R support. This trend seemed to be very prominent as a lot of people without computing degrees were being trained in R at universities, in classes, and other settings. We therefore accelerated the incorporation of SparkR into Spark and also <a href=\"https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html\" target=\"_blank\">added R as a first class language as part of Databricks</a>, making us the first company to commercially support SparkR.\n<p style=\"text-align: center;\"><img class=\"aligncenter wp-image-4525\" src=\"https://databricks.com/wp-content/uploads/2015/07/RNotebooks.png\" alt=\"RNotebooks\" width=\"400\" height=\"183\" /><i>R Notebooks in Databricks</i></p>\nThis release of Databricks is dubbed \"version 2.0\" since it contains many all of the above features that enable the democratization effort inside many organizations. I used quotes around the version number, because as a SaaS product versions don't play the same role as for traditional software. We will continue to maintain a two week release cadence, each containing new exciting features that our users requested.\n\n<a href=\"http://dbricks.co/1IoKfGA\" target=\"_blank\">Try these features yourself</a> and please let us know what you think.\n\n&nbsp;\n\n&nbsp;</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Nested Data\n\nThink of nested data as columns within columns. \n\nFor instance, look at the `dates` column."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/kqmfblujy9?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/kqmfblujy9?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates FROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dates</th></tr></thead><tbody><tr><td>List(2014-04-10, 2014-04-10, UTC)</td></tr><tr><td>List(2014-04-10, 2014-04-10, UTC)</td></tr><tr><td>List(2014-04-01, 2014-04-01, UTC)</td></tr><tr><td>List(2014-03-27, 2014-03-27, UTC)</td></tr><tr><td>List(2014-02-04, 2014-02-04, UTC)</td></tr><tr><td>List(2014-01-02, 2014-01-02, UTC)</td></tr><tr><td>List(2014-03-26, 2014-03-26, UTC)</td></tr><tr><td>List(2014-03-21, 2014-03-21, UTC)</td></tr><tr><td>List(2014-03-19, 2014-03-19, UTC)</td></tr><tr><td>List(2014-03-03, 2014-03-03, UTC)</td></tr><tr><td>List(2014-02-13, 2014-02-13, UTC)</td></tr><tr><td>List(2014-02-11, 2014-02-11, UTC)</td></tr><tr><td>List(2014-01-22, 2014-01-22, UTC)</td></tr><tr><td>List(2013-12-20, 2013-12-20, UTC)</td></tr><tr><td>List(2013-12-19, 2013-12-19, UTC)</td></tr><tr><td>List(2013-11-22, 2013-11-22, UTC)</td></tr><tr><td>List(2013-10-29, 2013-10-29, UTC)</td></tr><tr><td>List(2013-10-28, 2013-10-28, UTC)</td></tr><tr><td>List(2013-10-27, 2013-10-27, UTC)</td></tr><tr><td>List(2014-04-11, 2014-04-11, UTC)</td></tr><tr><td>List(2014-04-15, 2014-04-15, UTC)</td></tr><tr><td>List(2014-06-02, 2014-06-02, UTC)</td></tr><tr><td>List(2014-05-23, 2014-05-23, UTC)</td></tr><tr><td>List(2014-05-23, 2014-05-23, UTC)</td></tr><tr><td>List(2014-05-30, 2014-05-30, UTC)</td></tr><tr><td>List(2014-06-02, 2014-06-02, UTC)</td></tr><tr><td>List(2014-06-04, 2014-06-04, UTC)</td></tr><tr><td>List(2014-06-11, 2014-06-11, UTC)</td></tr><tr><td>List(2014-06-12, 2014-06-12, UTC)</td></tr><tr><td>List(2014-06-13, 2014-06-13, UTC)</td></tr><tr><td>List(2014-06-23, 2014-06-23, UTC)</td></tr><tr><td>List(2014-06-24, 2014-06-24, UTC)</td></tr><tr><td>List(2014-06-26, 2014-06-26, UTC)</td></tr><tr><td>List(2014-06-28, 2014-06-28, UTC)</td></tr><tr><td>List(2014-06-30, 2014-06-30, UTC)</td></tr><tr><td>List(2014-06-30, 2014-06-30, UTC)</td></tr><tr><td>List(2014-06-30, 2014-06-30, UTC)</td></tr><tr><td>List(2014-04-29, 2014-04-29, UTC)</td></tr><tr><td>List(2014-05-08, 2014-05-08, UTC)</td></tr><tr><td>List(2014-04-30, 2014-04-30, UTC)</td></tr><tr><td>List(2014-07-01, 2014-07-01, UTC)</td></tr><tr><td>List(2014-07-01, 2014-07-01, UTC)</td></tr><tr><td>List(2014-07-02, 2014-07-02, UTC)</td></tr><tr><td>List(2014-07-14, 2014-07-14, UTC)</td></tr><tr><td>List(2014-07-16, 2014-07-16, UTC)</td></tr><tr><td>List(2014-07-19, 2014-07-19, UTC)</td></tr><tr><td>List(2014-07-23, 2014-07-23, UTC)</td></tr><tr><td>List(2014-07-22, 2014-07-22, UTC)</td></tr><tr><td>List(2014-07-23, 2014-07-23, UTC)</td></tr><tr><td>List(2014-08-08, 2014-08-08, UTC)</td></tr><tr><td>List(2014-08-15, 2014-08-15, UTC)</td></tr><tr><td>List(2014-08-27, 2014-08-27, UTC)</td></tr><tr><td>List(2014-09-12, 2014-09-12, UTC)</td></tr><tr><td>List(2014-09-16, 2014-09-16, UTC)</td></tr><tr><td>List(2014-09-22, 2014-09-22, UTC)</td></tr><tr><td>List(2014-09-15, 2014-09-15, UTC)</td></tr><tr><td>List(2014-09-18, 2014-09-18, UTC)</td></tr><tr><td>List(2014-09-24, 2014-09-24, UTC)</td></tr><tr><td>List(2014-09-19, 2014-09-19, UTC)</td></tr><tr><td>List(2014-09-24, 2014-09-24, UTC)</td></tr><tr><td>List(2014-09-30, 2014-09-30, UTC)</td></tr><tr><td>List(2014-09-25, 2014-09-25, UTC)</td></tr><tr><td>List(2014-10-01, 2014-10-01, UTC)</td></tr><tr><td>List(2014-10-07, 2014-10-07, UTC)</td></tr><tr><td>List(2014-10-09, 2014-10-09, UTC)</td></tr><tr><td>List(2014-10-10, 2014-10-10, UTC)</td></tr><tr><td>List(2014-10-20, 2014-10-20, UTC)</td></tr><tr><td>List(2014-10-15, 2014-10-15, UTC)</td></tr><tr><td>List(2014-10-23, 2014-10-23, UTC)</td></tr><tr><td>List(2014-10-27, 2014-10-27, UTC)</td></tr><tr><td>List(2014-10-31, 2014-10-31, UTC)</td></tr><tr><td>List(2014-11-25, 2014-11-25, UTC)</td></tr><tr><td>List(2014-12-02, 2014-12-02, UTC)</td></tr><tr><td>List(2014-12-09, 2014-12-09, UTC)</td></tr><tr><td>List(2014-11-05, 2014-11-05, UTC)</td></tr><tr><td>List(2014-11-14, 2014-11-14, UTC)</td></tr><tr><td>List(2014-11-15, 2014-11-15, UTC)</td></tr><tr><td>List(2014-11-22, 2014-11-22, UTC)</td></tr><tr><td>List(2014-12-02, 2014-12-02, UTC)</td></tr><tr><td>List(2014-12-04, 2014-12-04, UTC)</td></tr><tr><td>List(2015-01-13, 2015-01-13, UTC)</td></tr><tr><td>List(2015-01-16, 2015-01-16, UTC)</td></tr><tr><td>List(2015-01-20, 2015-01-20, UTC)</td></tr><tr><td>List(2015-02-02, 2015-02-02, UTC)</td></tr><tr><td>List(2015-01-28, 2015-01-28, UTC)</td></tr><tr><td>List(2015-02-05, 2015-02-05, UTC)</td></tr><tr><td>List(2014-12-19, 2014-12-19, UTC)</td></tr><tr><td>List(2014-12-22, 2014-12-22, UTC)</td></tr><tr><td>List(2015-01-07, 2015-01-07, UTC)</td></tr><tr><td>List(2015-01-09, 2015-01-09, UTC)</td></tr><tr><td>List(2015-01-21, 2015-01-21, UTC)</td></tr><tr><td>List(2015-01-15, 2015-01-15, UTC)</td></tr><tr><td>List(2015-01-27, 2015-01-27, UTC)</td></tr><tr><td>List(2015-02-09, 2015-02-09, UTC)</td></tr><tr><td>List(2015-02-13, 2015-02-13, UTC)</td></tr><tr><td>List(2015-02-14, 2015-02-14, UTC)</td></tr><tr><td>List(2015-02-19, 2015-02-19, UTC)</td></tr><tr><td>List(2015-02-17, 2015-02-17, UTC)</td></tr><tr><td>List(2015-02-24, 2015-02-24, UTC)</td></tr><tr><td>List(2015-03-04, 2015-03-04, UTC)</td></tr><tr><td>List(2015-03-05, 2015-03-05, UTC)</td></tr><tr><td>List(2015-03-10, 2015-03-10, UTC)</td></tr><tr><td>List(2015-03-13, 2015-03-13, UTC)</td></tr><tr><td>List(2015-03-19, 2015-03-19, UTC)</td></tr><tr><td>List(2015-03-17, 2015-03-17, UTC)</td></tr><tr><td>List(2015-03-18, 2015-03-18, UTC)</td></tr><tr><td>List(2015-03-20, 2015-03-20, UTC)</td></tr><tr><td>List(2015-03-24, 2015-03-24, UTC)</td></tr><tr><td>List(2015-03-25, 2015-03-25, UTC)</td></tr><tr><td>List(2015-03-30, 2015-03-30, UTC)</td></tr><tr><td>List(2015-04-02, 2015-04-02, UTC)</td></tr><tr><td>List(2015-03-31, 2015-03-31, UTC)</td></tr><tr><td>List(2015-04-01, 2015-04-01, UTC)</td></tr><tr><td>List(2015-04-03, 2015-04-03, UTC)</td></tr><tr><td>List(2015-04-08, 2015-04-08, UTC)</td></tr><tr><td>List(2015-04-13, 2015-04-13, UTC)</td></tr><tr><td>List(2015-04-14, 2015-04-14, UTC)</td></tr><tr><td>List(2015-04-15, 2015-04-15, UTC)</td></tr><tr><td>List(2015-04-16, 2015-04-16, UTC)</td></tr><tr><td>List(2015-04-17, 2015-04-17, UTC)</td></tr><tr><td>List(2015-04-21, 2015-04-21, UTC)</td></tr><tr><td>List(2015-04-23, 2015-04-23, UTC)</td></tr><tr><td>List(2015-04-24, 2015-04-24, UTC)</td></tr><tr><td>List(2015-04-28, 2015-04-28, UTC)</td></tr><tr><td>List(2015-05-11, 2015-05-11, UTC)</td></tr><tr><td>List(2015-05-14, 2015-05-14, UTC)</td></tr><tr><td>List(2015-05-28, 2015-05-28, UTC)</td></tr><tr><td>List(2015-06-01, 2015-06-01, UTC)</td></tr><tr><td>List(2015-06-02, 2015-06-02, UTC)</td></tr><tr><td>List(2015-06-04, 2015-06-04, UTC)</td></tr><tr><td>List(2015-06-03, 2015-06-03, UTC)</td></tr><tr><td>List(2015-06-05, 2015-06-05, UTC)</td></tr><tr><td>List(2015-06-09, 2015-06-09, UTC)</td></tr><tr><td>List(2015-06-10, 2015-06-10, UTC)</td></tr><tr><td>List(2015-06-11, 2015-06-11, UTC)</td></tr><tr><td>List(2015-06-15, 2015-06-15, UTC)</td></tr><tr><td>List(2015-06-15, 2015-06-15, UTC)</td></tr><tr><td>List(2015-06-16, 2015-06-16, UTC)</td></tr><tr><td>List(2015-06-17, 2015-06-17, UTC)</td></tr><tr><td>List(2015-06-19, 2015-06-19, UTC)</td></tr><tr><td>List(2015-06-22, 2015-06-22, UTC)</td></tr><tr><td>List(2015-06-29, 2015-06-29, UTC)</td></tr><tr><td>List(2015-07-02, 2015-07-02, UTC)</td></tr><tr><td>List(2015-07-02, 2015-07-02, UTC)</td></tr><tr><td>List(2015-07-08, 2015-07-08, UTC)</td></tr><tr><td>List(2015-07-10, 2015-07-10, UTC)</td></tr><tr><td>List(2015-08-03, 2015-08-03, UTC)</td></tr><tr><td>List(2015-07-13, 2015-07-13, UTC)</td></tr><tr><td>List(2015-07-15, 2015-07-15, UTC)</td></tr><tr><td>List(2015-07-16, 2015-07-16, UTC)</td></tr><tr><td>List(2015-07-21, 2015-07-21, UTC)</td></tr><tr><td>List(2015-07-23, 2015-07-23, UTC)</td></tr><tr><td>List(2015-07-28, 2015-07-28, UTC)</td></tr><tr><td>List(2015-07-29, 2015-07-29, UTC)</td></tr><tr><td>List(2015-07-30, 2015-07-30, UTC)</td></tr><tr><td>List(2015-08-05, 2015-08-05, UTC)</td></tr><tr><td>List(2015-08-12, 2015-08-12, UTC)</td></tr><tr><td>List(2015-08-18, 2015-08-18, UTC)</td></tr><tr><td>List(2015-08-31, 2015-08-31, UTC)</td></tr><tr><td>List(2015-09-09, 2015-09-09, UTC)</td></tr><tr><td>List(2015-09-16, 2015-09-16, UTC)</td></tr><tr><td>List(2015-09-22, 2015-09-22, UTC)</td></tr><tr><td>List(2015-09-23, 2015-09-23, UTC)</td></tr><tr><td>List(2015-09-24, 2015-09-24, UTC)</td></tr><tr><td>List(2015-09-28, 2015-09-28, UTC)</td></tr><tr><td>List(2015-10-13, 2015-10-13, UTC)</td></tr><tr><td>List(2015-10-20, 2015-10-20, UTC)</td></tr><tr><td>List(2015-10-05, 2015-10-05, UTC)</td></tr><tr><td>List(2015-10-02, 2015-10-02, UTC)</td></tr><tr><td>List(2015-10-14, 2015-10-14, UTC)</td></tr><tr><td>List(2015-10-21, 2015-10-21, UTC)</td></tr><tr><td>List(2015-10-19, 2015-10-19, UTC)</td></tr><tr><td>List(2015-10-27, 2015-10-27, UTC)</td></tr><tr><td>List(2015-10-26, 2015-10-26, UTC)</td></tr><tr><td>List(2015-11-06, 2015-11-06, UTC)</td></tr><tr><td>List(2015-11-02, 2015-11-02, UTC)</td></tr><tr><td>List(2015-11-05, 2015-11-05, UTC)</td></tr><tr><td>List(2015-11-10, 2015-11-10, UTC)</td></tr><tr><td>List(2015-11-11, 2015-11-11, UTC)</td></tr><tr><td>List(2015-11-12, 2015-11-12, UTC)</td></tr><tr><td>List(2015-11-19, 2015-11-19, UTC)</td></tr><tr><td>List(2015-11-30, 2015-11-30, UTC)</td></tr><tr><td>List(2015-11-20, 2015-11-20, UTC)</td></tr><tr><td>List(2015-11-24, 2015-11-24, UTC)</td></tr><tr><td>List(2015-12-02, 2015-12-02, UTC)</td></tr><tr><td>List(2015-12-18, 2015-12-18, UTC)</td></tr><tr><td>List(2015-12-22, 2015-12-22, UTC)</td></tr><tr><td>List(2015-12-28, 2015-12-28, UTC)</td></tr><tr><td>List(2016-01-04, 2016-01-04, UTC)</td></tr><tr><td>List(2016-01-04, 2016-01-04, UTC)</td></tr><tr><td>List(2016-01-05, 2016-01-05, UTC)</td></tr><tr><td>List(2016-01-05, 2016-01-05, UTC)</td></tr><tr><td>List(2016-01-12, 2016-01-12, UTC)</td></tr><tr><td>List(2016-01-13, 2016-01-13, UTC)</td></tr><tr><td>List(2016-01-21, 2016-01-21, UTC)</td></tr><tr><td>List(2016-01-25, 2016-01-25, UTC)</td></tr><tr><td>List(2016-02-01, 2016-02-01, UTC)</td></tr><tr><td>List(2016-02-03, 2016-02-03, UTC)</td></tr><tr><td>List(2016-02-03, 2016-02-03, UTC)</td></tr><tr><td>List(2016-02-04, 2016-02-04, UTC)</td></tr><tr><td>List(2016-02-08, 2016-02-08, UTC)</td></tr><tr><td>List(2016-02-09, 2016-02-09, UTC)</td></tr><tr><td>List(2016-02-10, 2016-02-10, UTC)</td></tr><tr><td>List(2016-02-12, 2016-02-12, UTC)</td></tr><tr><td>List(2016-02-17, 2016-02-17, UTC)</td></tr><tr><td>List(2016-02-17, 2016-02-17, UTC)</td></tr><tr><td>List(2016-02-18, 2016-02-18, UTC)</td></tr><tr><td>List(2016-03-03, 2016-03-03, UTC)</td></tr><tr><td>List(2016-03-09, 2016-03-09, UTC)</td></tr><tr><td>List(2016-03-10, 2016-03-10, UTC)</td></tr><tr><td>List(2016-03-16, 2016-03-16, UTC)</td></tr><tr><td>List(2016-03-15, 2016-03-15, UTC)</td></tr><tr><td>List(2016-03-16, 2016-03-16, UTC)</td></tr><tr><td>List(2016-03-28, 2016-03-28, UTC)</td></tr><tr><td>List(2016-03-22, 2016-03-22, UTC)</td></tr><tr><td>List(2016-03-31, 2016-03-31, UTC)</td></tr><tr><td>List(2016-03-30, 2016-03-30, UTC)</td></tr><tr><td>List(2016-03-31, 2016-03-31, UTC)</td></tr><tr><td>List(2016-04-01, 2016-04-01, UTC)</td></tr><tr><td>List(2016-04-04, 2016-04-04, UTC)</td></tr><tr><td>List(2016-04-06, 2016-04-06, UTC)</td></tr><tr><td>List(2016-04-07, 2016-04-07, UTC)</td></tr><tr><td>List(2016-04-12, 2016-04-12, UTC)</td></tr><tr><td>List(2016-04-21, 2016-04-21, UTC)</td></tr><tr><td>List(2016-04-27, 2016-04-27, UTC)</td></tr><tr><td>List(2016-05-04, 2016-05-04, UTC)</td></tr><tr><td>List(2016-05-03, 2016-05-03, UTC)</td></tr><tr><td>List(2016-05-09, 2016-05-09, UTC)</td></tr><tr><td>List(2016-05-11, 2016-05-11, UTC)</td></tr><tr><td>List(2016-05-11, 2016-05-11, UTC)</td></tr><tr><td>List(2016-05-18, 2016-05-18, UTC)</td></tr><tr><td>List(2016-05-26, 2016-05-26, UTC)</td></tr><tr><td>List(2016-05-18, 2016-05-18, UTC)</td></tr><tr><td>List(2016-05-19, 2016-05-19, UTC)</td></tr><tr><td>List(2016-05-20, 2016-05-20, UTC)</td></tr><tr><td>List(2016-05-23, 2016-05-23, UTC)</td></tr><tr><td>List(2016-05-24, 2016-05-24, UTC)</td></tr><tr><td>List(2016-05-24, 2016-05-24, UTC)</td></tr><tr><td>List(2016-05-24, 2016-05-24, UTC)</td></tr><tr><td>List(2016-05-27, 2016-05-27, UTC)</td></tr><tr><td>List(2016-06-02, 2016-06-02, UTC)</td></tr><tr><td>List(2016-05-31, 2016-05-31, UTC)</td></tr><tr><td>List(2016-06-08, 2016-06-08, UTC)</td></tr><tr><td>List(2016-06-01, 2016-06-01, UTC)</td></tr><tr><td>List(2016-06-01, 2016-06-01, UTC)</td></tr><tr><td>List(2016-06-07, 2016-06-07, UTC)</td></tr><tr><td>List(2016-06-09, 2016-06-09, UTC)</td></tr><tr><td>List(2016-06-15, 2016-06-15, UTC)</td></tr><tr><td>List(2016-06-17, 2016-06-17, UTC)</td></tr><tr><td>List(2016-06-22, 2016-06-22, UTC)</td></tr><tr><td>List(2016-06-23, 2016-06-23, UTC)</td></tr><tr><td>List(2016-06-28, 2016-06-28, UTC)</td></tr><tr><td>List(2016-06-30, 2016-06-30, UTC)</td></tr><tr><td>List(2016-07-06, 2016-07-06, UTC)</td></tr><tr><td>List(2016-07-05, 2016-07-05, UTC)</td></tr><tr><td>List(2016-07-07, 2016-07-07, UTC)</td></tr><tr><td>List(2016-07-12, 2016-07-12, UTC)</td></tr><tr><td>List(2016-07-14, 2016-07-14, UTC)</td></tr><tr><td>List(2016-07-18, 2016-07-18, UTC)</td></tr><tr><td>List(2016-07-20, 2016-07-20, UTC)</td></tr><tr><td>List(2016-07-21, 2016-07-21, UTC)</td></tr><tr><td>List(2016-07-26, 2016-07-26, UTC)</td></tr><tr><td>List(2016-07-28, 2016-07-28, UTC)</td></tr><tr><td>List(2016-07-28, 2016-07-28, UTC)</td></tr><tr><td>List(2016-08-03, 2016-08-03, UTC)</td></tr><tr><td>List(2016-08-04, 2016-08-04, UTC)</td></tr><tr><td>List(2016-08-08, 2016-08-08, UTC)</td></tr><tr><td>List(2016-08-11, 2016-08-11, UTC)</td></tr><tr><td>List(2016-08-16, 2016-08-16, UTC)</td></tr><tr><td>List(2016-08-15, 2016-08-15, UTC)</td></tr><tr><td>List(2016-08-30, 2016-08-30, UTC)</td></tr><tr><td>List(2016-08-31, 2016-08-31, UTC)</td></tr><tr><td>List(2016-08-31, 2016-08-31, UTC)</td></tr><tr><td>List(2016-09-06, 2016-09-06, UTC)</td></tr><tr><td>List(2016-09-19, 2016-09-19, UTC)</td></tr><tr><td>List(2016-09-28, 2016-09-28, UTC)</td></tr><tr><td>List(2016-09-27, 2016-09-27, UTC)</td></tr><tr><td>List(2016-10-03, 2016-10-03, UTC)</td></tr><tr><td>List(2016-10-04, 2016-10-04, UTC)</td></tr><tr><td>List(2016-10-04, 2016-10-04, UTC)</td></tr><tr><td>List(2016-10-05, 2016-10-05, UTC)</td></tr><tr><td>List(2016-10-12, 2016-10-12, UTC)</td></tr><tr><td>List(2016-10-11, 2016-10-11, UTC)</td></tr><tr><td>List(2016-10-18, 2016-10-18, UTC)</td></tr><tr><td>List(2016-10-27, 2016-10-27, UTC)</td></tr><tr><td>List(2016-10-25, 2016-10-25, UTC)</td></tr><tr><td>List(2016-10-26, 2016-10-26, UTC)</td></tr><tr><td>List(2016-10-27, 2016-10-27, UTC)</td></tr><tr><td>List(2016-11-01, 2016-11-01, UTC)</td></tr><tr><td>List(2016-11-03, 2016-11-03, UTC)</td></tr><tr><td>List(2016-11-10, 2016-11-10, UTC)</td></tr><tr><td>List(2016-11-10, 2016-11-10, UTC)</td></tr><tr><td>List(2016-11-15, 2016-11-15, UTC)</td></tr><tr><td>List(2016-11-16, 2016-11-16, UTC)</td></tr><tr><td>List(2016-11-16, 2016-11-16, UTC)</td></tr><tr><td>List(2016-11-23, 2016-11-23, UTC)</td></tr><tr><td>List(2016-12-08, 2016-12-08, UTC)</td></tr><tr><td>List(2016-12-12, 2016-12-12, UTC)</td></tr><tr><td>List(2016-12-14, 2016-12-14, UTC)</td></tr><tr><td>List(2016-12-15, 2016-12-15, UTC)</td></tr><tr><td>List(2016-12-28, 2016-12-28, UTC)</td></tr><tr><td>List(2016-12-21, 2016-12-21, UTC)</td></tr><tr><td>List(2016-12-30, 2016-12-30, UTC)</td></tr><tr><td>List(2016-12-29, 2016-12-29, UTC)</td></tr><tr><td>List(2017-01-04, 2017-01-04, UTC)</td></tr><tr><td>List(2017-01-03, 2017-01-03, UTC)</td></tr><tr><td>List(2017-01-09, 2017-01-09, UTC)</td></tr><tr><td>List(2017-01-10, 2017-01-10, UTC)</td></tr><tr><td>List(2017-01-18, 2017-01-18, UTC)</td></tr><tr><td>List(2017-01-19, 2017-01-19, UTC)</td></tr><tr><td>List(2017-01-23, 2017-01-23, UTC)</td></tr><tr><td>List(2017-01-25, 2017-01-25, UTC)</td></tr><tr><td>List(2017-01-30, 2017-01-30, UTC)</td></tr><tr><td>List(2017-01-31, 2017-01-31, UTC)</td></tr><tr><td>List(2017-02-09, 2017-02-09, UTC)</td></tr><tr><td>List(2017-02-10, 2017-02-10, UTC)</td></tr><tr><td>List(2017-02-13, 2017-02-13, UTC)</td></tr><tr><td>List(2017-02-16, 2017-02-16, UTC)</td></tr><tr><td>List(2017-02-23, 2017-02-23, UTC)</td></tr><tr><td>List(2017-02-27, 2017-02-27, UTC)</td></tr><tr><td>List(2017-02-28, 2017-02-28, UTC)</td></tr><tr><td>List(2017-03-31, 2017-03-31, UTC)</td></tr><tr><td>List(2017-03-27, 2017-03-27, UTC)</td></tr><tr><td>List(2017-03-28, 2017-03-28, UTC)</td></tr><tr><td>List(2017-03-30, 2017-03-30, UTC)</td></tr><tr><td>List(2017-04-01, 2017-04-01, UTC)</td></tr><tr><td>List(2017-04-03, 2017-04-03, UTC)</td></tr><tr><td>List(2017-04-04, 2017-04-04, UTC)</td></tr><tr><td>List(2017-04-17, 2017-04-17, UTC)</td></tr><tr><td>List(2017-04-26, 2017-04-26, UTC)</td></tr><tr><td>List(2017-05-08, 2017-05-08, UTC)</td></tr><tr><td>List(2017-05-09, 2017-05-09, UTC)</td></tr><tr><td>List(2017-05-19, 2017-05-19, UTC)</td></tr><tr><td>List(2017-05-18, 2017-05-18, UTC)</td></tr><tr><td>List(2017-05-23, 2017-05-23, UTC)</td></tr><tr><td>List(2017-05-22, 2017-05-22, UTC)</td></tr><tr><td>List(2017-05-24, 2017-05-24, UTC)</td></tr><tr><td>List(2017-05-24, 2017-05-24, UTC)</td></tr><tr><td>List(2017-05-25, 2017-05-25, UTC)</td></tr><tr><td>List(2017-05-26, 2017-05-26, UTC)</td></tr><tr><td>List(2017-05-30, 2017-05-30, UTC)</td></tr><tr><td>List(2017-05-31, 2017-05-31, UTC)</td></tr><tr><td>List(2017-05-31, 2017-05-31, UTC)</td></tr><tr><td>List(2017-06-01, 2017-06-01, UTC)</td></tr><tr><td>List(2017-06-01, 2017-06-01, UTC)</td></tr><tr><td>List(2017-06-02, 2017-06-02, UTC)</td></tr><tr><td>List(2017-06-06, 2017-06-06, UTC)</td></tr><tr><td>List(2017-06-06, 2017-06-06, UTC)</td></tr><tr><td>List(2017-06-05, 2017-06-05, UTC)</td></tr><tr><td>List(2017-06-07, 2017-06-07, UTC)</td></tr><tr><td>List(2017-06-09, 2017-06-09, UTC)</td></tr><tr><td>List(2017-06-14, 2017-06-14, UTC)</td></tr><tr><td>List(2017-06-13, 2017-06-13, UTC)</td></tr><tr><td>List(2017-06-20, 2017-06-20, UTC)</td></tr><tr><td>List(2017-06-23, 2017-06-23, UTC)</td></tr><tr><td>List(2017-06-26, 2017-06-26, UTC)</td></tr><tr><td>List(2017-06-27, 2017-06-27, UTC)</td></tr><tr><td>List(2017-07-13, 2017-07-13, UTC)</td></tr><tr><td>List(2017-07-12, 2017-07-12, UTC)</td></tr><tr><td>List(2017-07-11, 2017-07-11, UTC)</td></tr><tr><td>List(2017-07-19, 2017-07-19, UTC)</td></tr><tr><td>List(2017-07-26, 2017-07-26, UTC)</td></tr><tr><td>List(2017-07-31, 2017-07-31, UTC)</td></tr><tr><td>List(2017-08-08, 2017-08-08, UTC)</td></tr><tr><td>List(2017-08-09, 2017-08-09, UTC)</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Pull out a specific subfield with \"dot\" notation."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.createdOn, dates.publishedOn \nFROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>createdOn</th><th>publishedOn</th></tr></thead><tbody><tr><td>2014-04-10</td><td>2014-04-10</td></tr><tr><td>2014-04-10</td><td>2014-04-10</td></tr><tr><td>2014-04-01</td><td>2014-04-01</td></tr><tr><td>2014-03-27</td><td>2014-03-27</td></tr><tr><td>2014-02-04</td><td>2014-02-04</td></tr><tr><td>2014-01-02</td><td>2014-01-02</td></tr><tr><td>2014-03-26</td><td>2014-03-26</td></tr><tr><td>2014-03-21</td><td>2014-03-21</td></tr><tr><td>2014-03-19</td><td>2014-03-19</td></tr><tr><td>2014-03-03</td><td>2014-03-03</td></tr><tr><td>2014-02-13</td><td>2014-02-13</td></tr><tr><td>2014-02-11</td><td>2014-02-11</td></tr><tr><td>2014-01-22</td><td>2014-01-22</td></tr><tr><td>2013-12-20</td><td>2013-12-20</td></tr><tr><td>2013-12-19</td><td>2013-12-19</td></tr><tr><td>2013-11-22</td><td>2013-11-22</td></tr><tr><td>2013-10-29</td><td>2013-10-29</td></tr><tr><td>2013-10-28</td><td>2013-10-28</td></tr><tr><td>2013-10-27</td><td>2013-10-27</td></tr><tr><td>2014-04-11</td><td>2014-04-11</td></tr><tr><td>2014-04-15</td><td>2014-04-15</td></tr><tr><td>2014-06-02</td><td>2014-06-02</td></tr><tr><td>2014-05-23</td><td>2014-05-23</td></tr><tr><td>2014-05-23</td><td>2014-05-23</td></tr><tr><td>2014-05-30</td><td>2014-05-30</td></tr><tr><td>2014-06-02</td><td>2014-06-02</td></tr><tr><td>2014-06-04</td><td>2014-06-04</td></tr><tr><td>2014-06-11</td><td>2014-06-11</td></tr><tr><td>2014-06-12</td><td>2014-06-12</td></tr><tr><td>2014-06-13</td><td>2014-06-13</td></tr><tr><td>2014-06-23</td><td>2014-06-23</td></tr><tr><td>2014-06-24</td><td>2014-06-24</td></tr><tr><td>2014-06-26</td><td>2014-06-26</td></tr><tr><td>2014-06-28</td><td>2014-06-28</td></tr><tr><td>2014-06-30</td><td>2014-06-30</td></tr><tr><td>2014-06-30</td><td>2014-06-30</td></tr><tr><td>2014-06-30</td><td>2014-06-30</td></tr><tr><td>2014-04-29</td><td>2014-04-29</td></tr><tr><td>2014-05-08</td><td>2014-05-08</td></tr><tr><td>2014-04-30</td><td>2014-04-30</td></tr><tr><td>2014-07-01</td><td>2014-07-01</td></tr><tr><td>2014-07-01</td><td>2014-07-01</td></tr><tr><td>2014-07-02</td><td>2014-07-02</td></tr><tr><td>2014-07-14</td><td>2014-07-14</td></tr><tr><td>2014-07-16</td><td>2014-07-16</td></tr><tr><td>2014-07-19</td><td>2014-07-19</td></tr><tr><td>2014-07-23</td><td>2014-07-23</td></tr><tr><td>2014-07-22</td><td>2014-07-22</td></tr><tr><td>2014-07-23</td><td>2014-07-23</td></tr><tr><td>2014-08-08</td><td>2014-08-08</td></tr><tr><td>2014-08-15</td><td>2014-08-15</td></tr><tr><td>2014-08-27</td><td>2014-08-27</td></tr><tr><td>2014-09-12</td><td>2014-09-12</td></tr><tr><td>2014-09-16</td><td>2014-09-16</td></tr><tr><td>2014-09-22</td><td>2014-09-22</td></tr><tr><td>2014-09-15</td><td>2014-09-15</td></tr><tr><td>2014-09-18</td><td>2014-09-18</td></tr><tr><td>2014-09-24</td><td>2014-09-24</td></tr><tr><td>2014-09-19</td><td>2014-09-19</td></tr><tr><td>2014-09-24</td><td>2014-09-24</td></tr><tr><td>2014-09-30</td><td>2014-09-30</td></tr><tr><td>2014-09-25</td><td>2014-09-25</td></tr><tr><td>2014-10-01</td><td>2014-10-01</td></tr><tr><td>2014-10-07</td><td>2014-10-07</td></tr><tr><td>2014-10-09</td><td>2014-10-09</td></tr><tr><td>2014-10-10</td><td>2014-10-10</td></tr><tr><td>2014-10-20</td><td>2014-10-20</td></tr><tr><td>2014-10-15</td><td>2014-10-15</td></tr><tr><td>2014-10-23</td><td>2014-10-23</td></tr><tr><td>2014-10-27</td><td>2014-10-27</td></tr><tr><td>2014-10-31</td><td>2014-10-31</td></tr><tr><td>2014-11-25</td><td>2014-11-25</td></tr><tr><td>2014-12-02</td><td>2014-12-02</td></tr><tr><td>2014-12-09</td><td>2014-12-09</td></tr><tr><td>2014-11-05</td><td>2014-11-05</td></tr><tr><td>2014-11-14</td><td>2014-11-14</td></tr><tr><td>2014-11-15</td><td>2014-11-15</td></tr><tr><td>2014-11-22</td><td>2014-11-22</td></tr><tr><td>2014-12-02</td><td>2014-12-02</td></tr><tr><td>2014-12-04</td><td>2014-12-04</td></tr><tr><td>2015-01-13</td><td>2015-01-13</td></tr><tr><td>2015-01-16</td><td>2015-01-16</td></tr><tr><td>2015-01-20</td><td>2015-01-20</td></tr><tr><td>2015-02-02</td><td>2015-02-02</td></tr><tr><td>2015-01-28</td><td>2015-01-28</td></tr><tr><td>2015-02-05</td><td>2015-02-05</td></tr><tr><td>2014-12-19</td><td>2014-12-19</td></tr><tr><td>2014-12-22</td><td>2014-12-22</td></tr><tr><td>2015-01-07</td><td>2015-01-07</td></tr><tr><td>2015-01-09</td><td>2015-01-09</td></tr><tr><td>2015-01-21</td><td>2015-01-21</td></tr><tr><td>2015-01-15</td><td>2015-01-15</td></tr><tr><td>2015-01-27</td><td>2015-01-27</td></tr><tr><td>2015-02-09</td><td>2015-02-09</td></tr><tr><td>2015-02-13</td><td>2015-02-13</td></tr><tr><td>2015-02-14</td><td>2015-02-14</td></tr><tr><td>2015-02-19</td><td>2015-02-19</td></tr><tr><td>2015-02-17</td><td>2015-02-17</td></tr><tr><td>2015-02-24</td><td>2015-02-24</td></tr><tr><td>2015-03-04</td><td>2015-03-04</td></tr><tr><td>2015-03-05</td><td>2015-03-05</td></tr><tr><td>2015-03-10</td><td>2015-03-10</td></tr><tr><td>2015-03-13</td><td>2015-03-13</td></tr><tr><td>2015-03-19</td><td>2015-03-19</td></tr><tr><td>2015-03-17</td><td>2015-03-17</td></tr><tr><td>2015-03-18</td><td>2015-03-18</td></tr><tr><td>2015-03-20</td><td>2015-03-20</td></tr><tr><td>2015-03-24</td><td>2015-03-24</td></tr><tr><td>2015-03-25</td><td>2015-03-25</td></tr><tr><td>2015-03-30</td><td>2015-03-30</td></tr><tr><td>2015-04-02</td><td>2015-04-02</td></tr><tr><td>2015-03-31</td><td>2015-03-31</td></tr><tr><td>2015-04-01</td><td>2015-04-01</td></tr><tr><td>2015-04-03</td><td>2015-04-03</td></tr><tr><td>2015-04-08</td><td>2015-04-08</td></tr><tr><td>2015-04-13</td><td>2015-04-13</td></tr><tr><td>2015-04-14</td><td>2015-04-14</td></tr><tr><td>2015-04-15</td><td>2015-04-15</td></tr><tr><td>2015-04-16</td><td>2015-04-16</td></tr><tr><td>2015-04-17</td><td>2015-04-17</td></tr><tr><td>2015-04-21</td><td>2015-04-21</td></tr><tr><td>2015-04-23</td><td>2015-04-23</td></tr><tr><td>2015-04-24</td><td>2015-04-24</td></tr><tr><td>2015-04-28</td><td>2015-04-28</td></tr><tr><td>2015-05-11</td><td>2015-05-11</td></tr><tr><td>2015-05-14</td><td>2015-05-14</td></tr><tr><td>2015-05-28</td><td>2015-05-28</td></tr><tr><td>2015-06-01</td><td>2015-06-01</td></tr><tr><td>2015-06-02</td><td>2015-06-02</td></tr><tr><td>2015-06-04</td><td>2015-06-04</td></tr><tr><td>2015-06-03</td><td>2015-06-03</td></tr><tr><td>2015-06-05</td><td>2015-06-05</td></tr><tr><td>2015-06-09</td><td>2015-06-09</td></tr><tr><td>2015-06-10</td><td>2015-06-10</td></tr><tr><td>2015-06-11</td><td>2015-06-11</td></tr><tr><td>2015-06-15</td><td>2015-06-15</td></tr><tr><td>2015-06-15</td><td>2015-06-15</td></tr><tr><td>2015-06-16</td><td>2015-06-16</td></tr><tr><td>2015-06-17</td><td>2015-06-17</td></tr><tr><td>2015-06-19</td><td>2015-06-19</td></tr><tr><td>2015-06-22</td><td>2015-06-22</td></tr><tr><td>2015-06-29</td><td>2015-06-29</td></tr><tr><td>2015-07-02</td><td>2015-07-02</td></tr><tr><td>2015-07-02</td><td>2015-07-02</td></tr><tr><td>2015-07-08</td><td>2015-07-08</td></tr><tr><td>2015-07-10</td><td>2015-07-10</td></tr><tr><td>2015-08-03</td><td>2015-08-03</td></tr><tr><td>2015-07-13</td><td>2015-07-13</td></tr><tr><td>2015-07-15</td><td>2015-07-15</td></tr><tr><td>2015-07-16</td><td>2015-07-16</td></tr><tr><td>2015-07-21</td><td>2015-07-21</td></tr><tr><td>2015-07-23</td><td>2015-07-23</td></tr><tr><td>2015-07-28</td><td>2015-07-28</td></tr><tr><td>2015-07-29</td><td>2015-07-29</td></tr><tr><td>2015-07-30</td><td>2015-07-30</td></tr><tr><td>2015-08-05</td><td>2015-08-05</td></tr><tr><td>2015-08-12</td><td>2015-08-12</td></tr><tr><td>2015-08-18</td><td>2015-08-18</td></tr><tr><td>2015-08-31</td><td>2015-08-31</td></tr><tr><td>2015-09-09</td><td>2015-09-09</td></tr><tr><td>2015-09-16</td><td>2015-09-16</td></tr><tr><td>2015-09-22</td><td>2015-09-22</td></tr><tr><td>2015-09-23</td><td>2015-09-23</td></tr><tr><td>2015-09-24</td><td>2015-09-24</td></tr><tr><td>2015-09-28</td><td>2015-09-28</td></tr><tr><td>2015-10-13</td><td>2015-10-13</td></tr><tr><td>2015-10-20</td><td>2015-10-20</td></tr><tr><td>2015-10-05</td><td>2015-10-05</td></tr><tr><td>2015-10-02</td><td>2015-10-02</td></tr><tr><td>2015-10-14</td><td>2015-10-14</td></tr><tr><td>2015-10-21</td><td>2015-10-21</td></tr><tr><td>2015-10-19</td><td>2015-10-19</td></tr><tr><td>2015-10-27</td><td>2015-10-27</td></tr><tr><td>2015-10-26</td><td>2015-10-26</td></tr><tr><td>2015-11-06</td><td>2015-11-06</td></tr><tr><td>2015-11-02</td><td>2015-11-02</td></tr><tr><td>2015-11-05</td><td>2015-11-05</td></tr><tr><td>2015-11-10</td><td>2015-11-10</td></tr><tr><td>2015-11-11</td><td>2015-11-11</td></tr><tr><td>2015-11-12</td><td>2015-11-12</td></tr><tr><td>2015-11-19</td><td>2015-11-19</td></tr><tr><td>2015-11-30</td><td>2015-11-30</td></tr><tr><td>2015-11-20</td><td>2015-11-20</td></tr><tr><td>2015-11-24</td><td>2015-11-24</td></tr><tr><td>2015-12-02</td><td>2015-12-02</td></tr><tr><td>2015-12-18</td><td>2015-12-18</td></tr><tr><td>2015-12-22</td><td>2015-12-22</td></tr><tr><td>2015-12-28</td><td>2015-12-28</td></tr><tr><td>2016-01-04</td><td>2016-01-04</td></tr><tr><td>2016-01-04</td><td>2016-01-04</td></tr><tr><td>2016-01-05</td><td>2016-01-05</td></tr><tr><td>2016-01-05</td><td>2016-01-05</td></tr><tr><td>2016-01-12</td><td>2016-01-12</td></tr><tr><td>2016-01-13</td><td>2016-01-13</td></tr><tr><td>2016-01-21</td><td>2016-01-21</td></tr><tr><td>2016-01-25</td><td>2016-01-25</td></tr><tr><td>2016-02-01</td><td>2016-02-01</td></tr><tr><td>2016-02-03</td><td>2016-02-03</td></tr><tr><td>2016-02-03</td><td>2016-02-03</td></tr><tr><td>2016-02-04</td><td>2016-02-04</td></tr><tr><td>2016-02-08</td><td>2016-02-08</td></tr><tr><td>2016-02-09</td><td>2016-02-09</td></tr><tr><td>2016-02-10</td><td>2016-02-10</td></tr><tr><td>2016-02-12</td><td>2016-02-12</td></tr><tr><td>2016-02-17</td><td>2016-02-17</td></tr><tr><td>2016-02-17</td><td>2016-02-17</td></tr><tr><td>2016-02-18</td><td>2016-02-18</td></tr><tr><td>2016-03-03</td><td>2016-03-03</td></tr><tr><td>2016-03-09</td><td>2016-03-09</td></tr><tr><td>2016-03-10</td><td>2016-03-10</td></tr><tr><td>2016-03-16</td><td>2016-03-16</td></tr><tr><td>2016-03-15</td><td>2016-03-15</td></tr><tr><td>2016-03-16</td><td>2016-03-16</td></tr><tr><td>2016-03-28</td><td>2016-03-28</td></tr><tr><td>2016-03-22</td><td>2016-03-22</td></tr><tr><td>2016-03-31</td><td>2016-03-31</td></tr><tr><td>2016-03-30</td><td>2016-03-30</td></tr><tr><td>2016-03-31</td><td>2016-03-31</td></tr><tr><td>2016-04-01</td><td>2016-04-01</td></tr><tr><td>2016-04-04</td><td>2016-04-04</td></tr><tr><td>2016-04-06</td><td>2016-04-06</td></tr><tr><td>2016-04-07</td><td>2016-04-07</td></tr><tr><td>2016-04-12</td><td>2016-04-12</td></tr><tr><td>2016-04-21</td><td>2016-04-21</td></tr><tr><td>2016-04-27</td><td>2016-04-27</td></tr><tr><td>2016-05-04</td><td>2016-05-04</td></tr><tr><td>2016-05-03</td><td>2016-05-03</td></tr><tr><td>2016-05-09</td><td>2016-05-09</td></tr><tr><td>2016-05-11</td><td>2016-05-11</td></tr><tr><td>2016-05-11</td><td>2016-05-11</td></tr><tr><td>2016-05-18</td><td>2016-05-18</td></tr><tr><td>2016-05-26</td><td>2016-05-26</td></tr><tr><td>2016-05-18</td><td>2016-05-18</td></tr><tr><td>2016-05-19</td><td>2016-05-19</td></tr><tr><td>2016-05-20</td><td>2016-05-20</td></tr><tr><td>2016-05-23</td><td>2016-05-23</td></tr><tr><td>2016-05-24</td><td>2016-05-24</td></tr><tr><td>2016-05-24</td><td>2016-05-24</td></tr><tr><td>2016-05-24</td><td>2016-05-24</td></tr><tr><td>2016-05-27</td><td>2016-05-27</td></tr><tr><td>2016-06-02</td><td>2016-06-02</td></tr><tr><td>2016-05-31</td><td>2016-05-31</td></tr><tr><td>2016-06-08</td><td>2016-06-08</td></tr><tr><td>2016-06-01</td><td>2016-06-01</td></tr><tr><td>2016-06-01</td><td>2016-06-01</td></tr><tr><td>2016-06-07</td><td>2016-06-07</td></tr><tr><td>2016-06-09</td><td>2016-06-09</td></tr><tr><td>2016-06-15</td><td>2016-06-15</td></tr><tr><td>2016-06-17</td><td>2016-06-17</td></tr><tr><td>2016-06-22</td><td>2016-06-22</td></tr><tr><td>2016-06-23</td><td>2016-06-23</td></tr><tr><td>2016-06-28</td><td>2016-06-28</td></tr><tr><td>2016-06-30</td><td>2016-06-30</td></tr><tr><td>2016-07-06</td><td>2016-07-06</td></tr><tr><td>2016-07-05</td><td>2016-07-05</td></tr><tr><td>2016-07-07</td><td>2016-07-07</td></tr><tr><td>2016-07-12</td><td>2016-07-12</td></tr><tr><td>2016-07-14</td><td>2016-07-14</td></tr><tr><td>2016-07-18</td><td>2016-07-18</td></tr><tr><td>2016-07-20</td><td>2016-07-20</td></tr><tr><td>2016-07-21</td><td>2016-07-21</td></tr><tr><td>2016-07-26</td><td>2016-07-26</td></tr><tr><td>2016-07-28</td><td>2016-07-28</td></tr><tr><td>2016-07-28</td><td>2016-07-28</td></tr><tr><td>2016-08-03</td><td>2016-08-03</td></tr><tr><td>2016-08-04</td><td>2016-08-04</td></tr><tr><td>2016-08-08</td><td>2016-08-08</td></tr><tr><td>2016-08-11</td><td>2016-08-11</td></tr><tr><td>2016-08-16</td><td>2016-08-16</td></tr><tr><td>2016-08-15</td><td>2016-08-15</td></tr><tr><td>2016-08-30</td><td>2016-08-30</td></tr><tr><td>2016-08-31</td><td>2016-08-31</td></tr><tr><td>2016-08-31</td><td>2016-08-31</td></tr><tr><td>2016-09-06</td><td>2016-09-06</td></tr><tr><td>2016-09-19</td><td>2016-09-19</td></tr><tr><td>2016-09-28</td><td>2016-09-28</td></tr><tr><td>2016-09-27</td><td>2016-09-27</td></tr><tr><td>2016-10-03</td><td>2016-10-03</td></tr><tr><td>2016-10-04</td><td>2016-10-04</td></tr><tr><td>2016-10-04</td><td>2016-10-04</td></tr><tr><td>2016-10-05</td><td>2016-10-05</td></tr><tr><td>2016-10-12</td><td>2016-10-12</td></tr><tr><td>2016-10-11</td><td>2016-10-11</td></tr><tr><td>2016-10-18</td><td>2016-10-18</td></tr><tr><td>2016-10-27</td><td>2016-10-27</td></tr><tr><td>2016-10-25</td><td>2016-10-25</td></tr><tr><td>2016-10-26</td><td>2016-10-26</td></tr><tr><td>2016-10-27</td><td>2016-10-27</td></tr><tr><td>2016-11-01</td><td>2016-11-01</td></tr><tr><td>2016-11-03</td><td>2016-11-03</td></tr><tr><td>2016-11-10</td><td>2016-11-10</td></tr><tr><td>2016-11-10</td><td>2016-11-10</td></tr><tr><td>2016-11-15</td><td>2016-11-15</td></tr><tr><td>2016-11-16</td><td>2016-11-16</td></tr><tr><td>2016-11-16</td><td>2016-11-16</td></tr><tr><td>2016-11-23</td><td>2016-11-23</td></tr><tr><td>2016-12-08</td><td>2016-12-08</td></tr><tr><td>2016-12-12</td><td>2016-12-12</td></tr><tr><td>2016-12-14</td><td>2016-12-14</td></tr><tr><td>2016-12-15</td><td>2016-12-15</td></tr><tr><td>2016-12-28</td><td>2016-12-28</td></tr><tr><td>2016-12-21</td><td>2016-12-21</td></tr><tr><td>2016-12-30</td><td>2016-12-30</td></tr><tr><td>2016-12-29</td><td>2016-12-29</td></tr><tr><td>2017-01-04</td><td>2017-01-04</td></tr><tr><td>2017-01-03</td><td>2017-01-03</td></tr><tr><td>2017-01-09</td><td>2017-01-09</td></tr><tr><td>2017-01-10</td><td>2017-01-10</td></tr><tr><td>2017-01-18</td><td>2017-01-18</td></tr><tr><td>2017-01-19</td><td>2017-01-19</td></tr><tr><td>2017-01-23</td><td>2017-01-23</td></tr><tr><td>2017-01-25</td><td>2017-01-25</td></tr><tr><td>2017-01-30</td><td>2017-01-30</td></tr><tr><td>2017-01-31</td><td>2017-01-31</td></tr><tr><td>2017-02-09</td><td>2017-02-09</td></tr><tr><td>2017-02-10</td><td>2017-02-10</td></tr><tr><td>2017-02-13</td><td>2017-02-13</td></tr><tr><td>2017-02-16</td><td>2017-02-16</td></tr><tr><td>2017-02-23</td><td>2017-02-23</td></tr><tr><td>2017-02-27</td><td>2017-02-27</td></tr><tr><td>2017-02-28</td><td>2017-02-28</td></tr><tr><td>2017-03-31</td><td>2017-03-31</td></tr><tr><td>2017-03-27</td><td>2017-03-27</td></tr><tr><td>2017-03-28</td><td>2017-03-28</td></tr><tr><td>2017-03-30</td><td>2017-03-30</td></tr><tr><td>2017-04-01</td><td>2017-04-01</td></tr><tr><td>2017-04-03</td><td>2017-04-03</td></tr><tr><td>2017-04-04</td><td>2017-04-04</td></tr><tr><td>2017-04-17</td><td>2017-04-17</td></tr><tr><td>2017-04-26</td><td>2017-04-26</td></tr><tr><td>2017-05-08</td><td>2017-05-08</td></tr><tr><td>2017-05-09</td><td>2017-05-09</td></tr><tr><td>2017-05-19</td><td>2017-05-19</td></tr><tr><td>2017-05-18</td><td>2017-05-18</td></tr><tr><td>2017-05-23</td><td>2017-05-23</td></tr><tr><td>2017-05-22</td><td>2017-05-22</td></tr><tr><td>2017-05-24</td><td>2017-05-24</td></tr><tr><td>2017-05-24</td><td>2017-05-24</td></tr><tr><td>2017-05-25</td><td>2017-05-25</td></tr><tr><td>2017-05-26</td><td>2017-05-26</td></tr><tr><td>2017-05-30</td><td>2017-05-30</td></tr><tr><td>2017-05-31</td><td>2017-05-31</td></tr><tr><td>2017-05-31</td><td>2017-05-31</td></tr><tr><td>2017-06-01</td><td>2017-06-01</td></tr><tr><td>2017-06-01</td><td>2017-06-01</td></tr><tr><td>2017-06-02</td><td>2017-06-02</td></tr><tr><td>2017-06-06</td><td>2017-06-06</td></tr><tr><td>2017-06-06</td><td>2017-06-06</td></tr><tr><td>2017-06-05</td><td>2017-06-05</td></tr><tr><td>2017-06-07</td><td>2017-06-07</td></tr><tr><td>2017-06-09</td><td>2017-06-09</td></tr><tr><td>2017-06-14</td><td>2017-06-14</td></tr><tr><td>2017-06-13</td><td>2017-06-13</td></tr><tr><td>2017-06-20</td><td>2017-06-20</td></tr><tr><td>2017-06-23</td><td>2017-06-23</td></tr><tr><td>2017-06-26</td><td>2017-06-26</td></tr><tr><td>2017-06-27</td><td>2017-06-27</td></tr><tr><td>2017-07-13</td><td>2017-07-13</td></tr><tr><td>2017-07-12</td><td>2017-07-12</td></tr><tr><td>2017-07-11</td><td>2017-07-11</td></tr><tr><td>2017-07-19</td><td>2017-07-19</td></tr><tr><td>2017-07-26</td><td>2017-07-26</td></tr><tr><td>2017-07-31</td><td>2017-07-31</td></tr><tr><td>2017-08-08</td><td>2017-08-08</td></tr><tr><td>2017-08-09</td><td>2017-08-09</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Both `createdOn` and `publishedOn` are stored as strings.\n\nCast those values to SQL timestamps:\n\nIn this case, use a single `SELECT` statement to:\n0. Cast `dates.publishedOn` to a `timestamp` data type.\n0. \"Flatten\" the `dates.publishedOn` column to just `publishedOn`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       cast(dates.publishedOn AS timestamp) AS publishedOn \nFROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>publishedOn</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>2014-04-10T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>2014-04-10T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>2014-04-01T00:00:00.000+0000</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>2014-03-27T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>2014-02-04T00:00:00.000+0000</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>2014-01-02T00:00:00.000+0000</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>2014-03-26T00:00:00.000+0000</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>2014-03-21T00:00:00.000+0000</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>2014-03-19T00:00:00.000+0000</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>2014-03-03T00:00:00.000+0000</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>2014-02-13T00:00:00.000+0000</td></tr><tr><td>Databricks at the O'Reilly Strata Conference 2014</td><td>2014-02-11T00:00:00.000+0000</td></tr><tr><td>Apache Spark and Hadoop: Working Together</td><td>2014-01-22T00:00:00.000+0000</td></tr><tr><td>Apache Spark 0.8.1 Released</td><td>2013-12-20T00:00:00.000+0000</td></tr><tr><td>Highlights From Spark Summit 2013</td><td>2013-12-19T00:00:00.000+0000</td></tr><tr><td>Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications</td><td>2013-11-22T00:00:00.000+0000</td></tr><tr><td>Databricks and Cloudera Partner to Support Apache Spark</td><td>2013-10-29T00:00:00.000+0000</td></tr><tr><td>The Growing Apache Spark Community</td><td>2013-10-28T00:00:00.000+0000</td></tr><tr><td>Databricks and the Apache Spark Platform</td><td>2013-10-27T00:00:00.000+0000</td></tr><tr><td>Databricks and MapR</td><td>2014-04-11T00:00:00.000+0000</td></tr><tr><td>Making Apache Spark Easier to Use in Java with Java 8</td><td>2014-04-15T00:00:00.000+0000</td></tr><tr><td>Databricks Announces Apache Spark Training Workshops</td><td>2014-06-02T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Atigeo xPatterns</td><td>2014-05-23T00:00:00.000+0000</td></tr><tr><td>Pivotal Hadoop Integrates the Full Apache Spark Stack</td><td>2014-05-23T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.0</td><td>2014-05-30T00:00:00.000+0000</td></tr><tr><td>Exciting Performance Improvements on the Horizon for Spark SQL</td><td>2014-06-02T00:00:00.000+0000</td></tr><tr><td>MicroStrategy \"Certified on Apache Spark\"</td><td>2014-06-04T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Arimo</td><td>2014-06-11T00:00:00.000+0000</td></tr><tr><td>Spark Summit 2014 Brings Together Apache Spark Community</td><td>2014-06-12T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Lightbend</td><td>2014-06-13T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Apervi</td><td>2014-06-23T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Qlik</td><td>2014-06-24T00:00:00.000+0000</td></tr><tr><td>Databricks Launches \"Certified Apache Spark Distribution\" Program</td><td>2014-06-26T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Elasticsearch</td><td>2014-06-28T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Pentaho</td><td>2014-06-30T00:00:00.000+0000</td></tr><tr><td>Sparkling Water = H20 + Apache Spark</td><td>2014-06-30T00:00:00.000+0000</td></tr><tr><td>Databricks Unveils Apache Spark-Based Cloud Platform; Announces Series B Funding</td><td>2014-06-30T00:00:00.000+0000</td></tr><tr><td>Databricks Application Spotlight at Spark Summit 2014</td><td>2014-04-29T00:00:00.000+0000</td></tr><tr><td>Databricks and Datastax</td><td>2014-05-08T00:00:00.000+0000</td></tr><tr><td>Databricks Partners with Simba to Deliver Shark ODBC Driver</td><td>2014-04-30T00:00:00.000+0000</td></tr><tr><td>Databricks Announces Partnership with SAP</td><td>2014-07-01T00:00:00.000+0000</td></tr><tr><td>Integrating Apache Spark and HANA</td><td>2014-07-01T00:00:00.000+0000</td></tr><tr><td>Shark, Spark SQL, Hive on Spark, and the future of SQL on Apache Spark</td><td>2014-07-02T00:00:00.000+0000</td></tr><tr><td>Databricks: Making Big Data Easy</td><td>2014-07-14T00:00:00.000+0000</td></tr><tr><td>New Features in MLlib in Apache Spark 1.0</td><td>2014-07-16T00:00:00.000+0000</td></tr><tr><td>The State of Apache Spark in 2014</td><td>2014-07-19T00:00:00.000+0000</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>2014-07-23T00:00:00.000+0000</td></tr><tr><td>Distributing the Singular Value Decomposition with Apache Spark</td><td>2014-07-22T00:00:00.000+0000</td></tr><tr><td>Spark Summit 2014 Highlights</td><td>2014-07-23T00:00:00.000+0000</td></tr><tr><td>When Stratio Met Apache Spark: A True Love Story</td><td>2014-08-08T00:00:00.000+0000</td></tr><tr><td>Mining Ecommerce Graph Data with Apache Spark at Alibaba Taobao</td><td>2014-08-15T00:00:00.000+0000</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>2014-08-27T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.1</td><td>2014-09-12T00:00:00.000+0000</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>2014-09-16T00:00:00.000+0000</td></tr><tr><td>Apache Spark 1.1: MLlib Performance Improvements</td><td>2014-09-22T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Talend</td><td>2014-09-15T00:00:00.000+0000</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>2014-09-18T00:00:00.000+0000</td></tr><tr><td>Databricks Reference Applications</td><td>2014-09-24T00:00:00.000+0000</td></tr><tr><td>Databricks and O'Reilly Media launch Certification Program for Apache Spark Developers</td><td>2014-09-19T00:00:00.000+0000</td></tr><tr><td>Apache Spark Improves the Economics of Video Distribution at NBC Universal</td><td>2014-09-24T00:00:00.000+0000</td></tr><tr><td>Scalable Decision Trees in MLlib</td><td>2014-09-30T00:00:00.000+0000</td></tr><tr><td>Guavus Embeds Apache Spark into its Operational Intelligence Platform Deployed at the World’s Largest Telcos</td><td>2014-09-25T00:00:00.000+0000</td></tr><tr><td>Apache Spark as a platform for large-scale neuroscience</td><td>2014-10-01T00:00:00.000+0000</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Advertisers' Return on Marketing Investment</td><td>2014-10-07T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Trifacta</td><td>2014-10-09T00:00:00.000+0000</td></tr><tr><td>Apache Spark the fastest open source engine for sorting a petabyte</td><td>2014-10-10T00:00:00.000+0000</td></tr><tr><td>Efficient similarity algorithm now in Apache Spark, thanks to Twitter</td><td>2014-10-20T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Tableau Software</td><td>2014-10-15T00:00:00.000+0000</td></tr><tr><td>Spark Summit East - CFP now open</td><td>2014-10-23T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Faimdata</td><td>2014-10-27T00:00:00.000+0000</td></tr><tr><td>Hortonworks: A shared vision for Apache Spark on Hadoop</td><td>2014-10-31T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Skytree Infinity</td><td>2014-11-25T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Nube Reifier</td><td>2014-12-02T00:00:00.000+0000</td></tr><tr><td>Pearson uses Apache Spark Streaming for next generation adaptive learning platform</td><td>2014-12-09T00:00:00.000+0000</td></tr><tr><td>Apache Spark officially sets a new record in large-scale sorting</td><td>2014-11-05T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Bedrock</td><td>2014-11-14T00:00:00.000+0000</td></tr><tr><td>The Apache Spark Certified Developer Program</td><td>2014-11-15T00:00:00.000+0000</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>2014-11-22T00:00:00.000+0000</td></tr><tr><td>Databricks to run two massive online courses on Apache Spark</td><td>2014-12-02T00:00:00.000+0000</td></tr><tr><td>Application Spotlight: Technicolor Virdata Internet of Things platform</td><td>2014-12-04T00:00:00.000+0000</td></tr><tr><td>Databricks Expands Bay Area Presence, Moves HQ to San Francisco</td><td>2015-01-13T00:00:00.000+0000</td></tr><tr><td>Apache Spark Certified Developer exams available online!</td><td>2015-01-16T00:00:00.000+0000</td></tr><tr><td>Spark Summit East 2015 Agenda is Now Available</td><td>2015-01-20T00:00:00.000+0000</td></tr><tr><td>An introduction to JSON support in Spark SQL</td><td>2015-02-02T00:00:00.000+0000</td></tr><tr><td>Introducing streaming k-means in Apache Spark 1.2</td><td>2015-01-28T00:00:00.000+0000</td></tr><tr><td>Apache Spark selected for Infoworld 2015 Technology of the Year Award</td><td>2015-02-05T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.2</td><td>2014-12-19T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark Packages</td><td>2014-12-22T00:00:00.000+0000</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>2015-01-07T00:00:00.000+0000</td></tr><tr><td>Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform</td><td>2015-01-09T00:00:00.000+0000</td></tr><tr><td>Random Forests and Boosting in MLlib</td><td>2015-01-21T00:00:00.000+0000</td></tr><tr><td>Improved Fault-tolerance and Zero Data Loss in Apache Spark Streaming</td><td>2015-01-15T00:00:00.000+0000</td></tr><tr><td>Big data projects are hungry for simpler and more powerful tools: Survey validates Apache Spark is gaining developer traction!</td><td>2015-01-27T00:00:00.000+0000</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>2015-02-09T00:00:00.000+0000</td></tr><tr><td>Automatic Labs Selects Databricks for Primary Real-Time Data Processing</td><td>2015-02-13T00:00:00.000+0000</td></tr><tr><td>Apache Spark: A review of 2014 and looking ahead to 2015 priorities</td><td>2015-02-14T00:00:00.000+0000</td></tr><tr><td>Extending MemSQL Analytics with Apache Spark</td><td>2015-02-19T00:00:00.000+0000</td></tr><tr><td>Introducing DataFrames in Apache Spark for Large Scale Data Science</td><td>2015-02-17T00:00:00.000+0000</td></tr><tr><td>Databricks at Strata San Jose</td><td>2015-02-24T00:00:00.000+0000</td></tr><tr><td>Databricks: From raw data, to insights and data products in an instant!</td><td>2015-03-04T00:00:00.000+0000</td></tr><tr><td>Radius Intelligence implements Databricks for real-time insights on targeted marketing campaigns</td><td>2015-03-05T00:00:00.000+0000</td></tr><tr><td>Sharethrough Selects Databricks to Discover Hidden Patterns in Ad Serving Platform</td><td>2015-03-10T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.3!</td><td>2015-03-13T00:00:00.000+0000</td></tr><tr><td>PanTera Big Data Visualization Leverages the Power of Databricks</td><td>2015-03-19T00:00:00.000+0000</td></tr><tr><td>Spark’ing an Anti Money Laundering Revolution</td><td>2015-03-17T00:00:00.000+0000</td></tr><tr><td>Databricks Launches \"Jobs\" Feature for Production Workloads</td><td>2015-03-18T00:00:00.000+0000</td></tr><tr><td>Using MongoDB with Apache Spark</td><td>2015-03-20T00:00:00.000+0000</td></tr><tr><td>What's new for Spark SQL in Apache Spark 1.3</td><td>2015-03-24T00:00:00.000+0000</td></tr><tr><td>Topic modeling with LDA: MLlib meets GraphX</td><td>2015-03-25T00:00:00.000+0000</td></tr><tr><td>Improvements to Kafka integration of Spark Streaming</td><td>2015-03-30T00:00:00.000+0000</td></tr><tr><td>Learning how to write Apache Spark applications in Databricks with the Integrated Search feature</td><td>2015-04-02T00:00:00.000+0000</td></tr><tr><td>Apache Spark Turns Five Years Old!</td><td>2015-03-31T00:00:00.000+0000</td></tr><tr><td>Apache Spark 2.0: Rearchitecting Spark for Mobile Platforms</td><td>2015-04-01T00:00:00.000+0000</td></tr><tr><td>Timeful Chooses Databricks to Enable Intelligent Time Management</td><td>2015-04-03T00:00:00.000+0000</td></tr><tr><td>A Look Back at Spark Summit East</td><td>2015-04-08T00:00:00.000+0000</td></tr><tr><td>Deep Dive into Spark SQL's Catalyst Optimizer</td><td>2015-04-13T00:00:00.000+0000</td></tr><tr><td>Running Apache Spark GraphX algorithms on Library of Congress subject heading SKOS</td><td>2015-04-14T00:00:00.000+0000</td></tr><tr><td>Celtra Scales Big Data Analysis Projects Six-Fold with Databricks</td><td>2015-04-15T00:00:00.000+0000</td></tr><tr><td>The Easiest Way to Run Apache Spark Jobs</td><td>2015-04-16T00:00:00.000+0000</td></tr><tr><td>New MLlib Algorithms in Apache Spark 1.3: FP-Growth and Power Iteration Clustering</td><td>2015-04-17T00:00:00.000+0000</td></tr><tr><td>Analyzing Apache Access Logs with Databricks</td><td>2015-04-21T00:00:00.000+0000</td></tr><tr><td>Big Graph Analytics with LynxKite & Apache Spark</td><td>2015-04-23T00:00:00.000+0000</td></tr><tr><td>Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More</td><td>2015-04-24T00:00:00.000+0000</td></tr><tr><td>Project Tungsten: Bringing Apache Spark Closer to Bare Metal</td><td>2015-04-28T00:00:00.000+0000</td></tr><tr><td>Spark Summit 2015 in San Francisco is just around the corner!</td><td>2015-05-11T00:00:00.000+0000</td></tr><tr><td>NTT DATA: Operating Apache Spark clusters at thousands-core scale and use cases for Telco and IoT</td><td>2015-05-14T00:00:00.000+0000</td></tr><tr><td>Tuning Java Garbage Collection for Apache Spark Applications</td><td>2015-05-28T00:00:00.000+0000</td></tr><tr><td>Databricks Launches MOOC: Data Science on Apache Spark</td><td>2015-06-01T00:00:00.000+0000</td></tr><tr><td>Statistical and Mathematical Functions with DataFrames in Apache Spark</td><td>2015-06-02T00:00:00.000+0000</td></tr><tr><td>Simplify Machine Learning on Apache Spark with Databricks</td><td>2015-06-04T00:00:00.000+0000</td></tr><tr><td>Join us for Engineer Office Hours at the Spark Summit</td><td>2015-06-03T00:00:00.000+0000</td></tr><tr><td>Making Databricks better for developers: IDE Integration</td><td>2015-06-05T00:00:00.000+0000</td></tr><tr><td>Announcing SparkR: R on Apache Spark</td><td>2015-06-09T00:00:00.000+0000</td></tr><tr><td>Huawei Embraces Open-Source Apache Spark</td><td>2015-06-10T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.4</td><td>2015-06-11T00:00:00.000+0000</td></tr><tr><td>Databricks and IBM Collaborate to Enhance Apache Spark Machine Learning</td><td>2015-06-15T00:00:00.000+0000</td></tr><tr><td>Databricks is now Generally Available</td><td>2015-06-15T00:00:00.000+0000</td></tr><tr><td>Guest blog: Zen and the Art of Apache Spark Maintenance with Cassandra</td><td>2015-06-16T00:00:00.000+0000</td></tr><tr><td>Guest blog: How Customers Win with Apache Spark on Hadoop</td><td>2015-06-17T00:00:00.000+0000</td></tr><tr><td>A Look Back at Spark Summit 2015</td><td>2015-06-19T00:00:00.000+0000</td></tr><tr><td>Understanding your Apache Spark Application Through Visualization</td><td>2015-06-22T00:00:00.000+0000</td></tr><tr><td>Databricks Launches Second MOOC: Scalable Machine Learning</td><td>2015-06-29T00:00:00.000+0000</td></tr><tr><td>MyFitnessPal Delivers New Feature, Speeds up Pipeline, and Boosts Team Productivity with Databricks</td><td>2015-07-02T00:00:00.000+0000</td></tr><tr><td>Guest blog: PMML Support in Apache Spark's MLlib</td><td>2015-07-02T00:00:00.000+0000</td></tr><tr><td>New Visualizations for Understanding Apache Spark Streaming Applications</td><td>2015-07-08T00:00:00.000+0000</td></tr><tr><td>Announcing SparkHub: A Community Site for Apache Spark</td><td>2015-07-10T00:00:00.000+0000</td></tr><tr><td>Guest blog: SequoiaDB Connector for Apache Spark</td><td>2015-08-03T00:00:00.000+0000</td></tr><tr><td>Introducing R Notebooks in Databricks</td><td>2015-07-13T00:00:00.000+0000</td></tr><tr><td>Introducing Window Functions in Spark SQL</td><td>2015-07-15T00:00:00.000+0000</td></tr><tr><td>Joint Blog Post: Bringing ORC Support into Apache Spark</td><td>2015-07-16T00:00:00.000+0000</td></tr><tr><td>Be Heard with the Spark Survey</td><td>2015-07-21T00:00:00.000+0000</td></tr><tr><td>Yesware Deploys Production Data Pipeline in Record Time with Databricks</td><td>2015-07-23T00:00:00.000+0000</td></tr><tr><td>Using 3rd Party Libraries in Databricks: Apache Spark Packages and Maven Libraries</td><td>2015-07-28T00:00:00.000+0000</td></tr><tr><td>New Features in Machine Learning Pipelines in Apache Spark 1.4</td><td>2015-07-29T00:00:00.000+0000</td></tr><tr><td>Diving into Apache Spark Streaming's Execution Model</td><td>2015-07-30T00:00:00.000+0000</td></tr><tr><td>Helping the Democratization of Big Data</td><td>2015-08-05T00:00:00.000+0000</td></tr><tr><td>From Pandas to Apache Spark's DataFrame</td><td>2015-08-12T00:00:00.000+0000</td></tr><tr><td>Apache Spark 1.5 Preview Now Available in Databricks</td><td>2015-08-18T00:00:00.000+0000</td></tr><tr><td>Spark Summit Europe Full Agenda Available Online</td><td>2015-08-31T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.5</td><td>2015-09-09T00:00:00.000+0000</td></tr><tr><td>Apache Spark 1.5 DataFrame API Highlights: Date/Time/String Handling, Time Intervals, and UDAFs</td><td>2015-09-16T00:00:00.000+0000</td></tr><tr><td>Large Scale Topic Modeling: Improvements to LDA on Apache Spark</td><td>2015-09-22T00:00:00.000+0000</td></tr><tr><td>Easier Spark Code Debugging</td><td>2015-09-23T00:00:00.000+0000</td></tr><tr><td>Spark Survey 2015 Results are now available</td><td>2015-09-24T00:00:00.000+0000</td></tr><tr><td>Improved Frequent Pattern Mining in Apache Spark 1.5: Association Rules and Sequential Patterns</td><td>2015-09-28T00:00:00.000+0000</td></tr><tr><td>Interactive Audience Analytics With Apache Spark and HyperLogLog</td><td>2015-10-13T00:00:00.000+0000</td></tr><tr><td>Audience Modeling With Apache Spark ML Pipelines</td><td>2015-10-20T00:00:00.000+0000</td></tr><tr><td>Generalized Linear Models in SparkR and R Formula Support in MLlib</td><td>2015-10-05T00:00:00.000+0000</td></tr><tr><td>Apache Spark 1.5.1 and What do Version Numbers Mean?</td><td>2015-10-02T00:00:00.000+0000</td></tr><tr><td>Call for Presentations for the 2016 Spark Summit East is Now Open</td><td>2015-10-14T00:00:00.000+0000</td></tr><tr><td>Introducing More Databricks Notebooks Sharing Options</td><td>2015-10-21T00:00:00.000+0000</td></tr><tr><td>Introducing Redshift Data Source for Spark</td><td>2015-10-19T00:00:00.000+0000</td></tr><tr><td>Visualizing Machine Learning Models</td><td>2015-10-27T00:00:00.000+0000</td></tr><tr><td>See You at Spark Summit EU 2015!</td><td>2015-10-26T00:00:00.000+0000</td></tr><tr><td>It's a Wrap! A Lookback at Spark Summit in Amsterdam</td><td>2015-11-06T00:00:00.000+0000</td></tr><tr><td>Announcing the TFOCS for Spark Optimization Package</td><td>2015-11-02T00:00:00.000+0000</td></tr><tr><td>How Yesware is Using Databricks to Transition from Concept to Product</td><td>2015-11-05T00:00:00.000+0000</td></tr><tr><td>Succinct Spark from AMPLab: Queries on Compressed RDDs</td><td>2015-11-10T00:00:00.000+0000</td></tr><tr><td>Elsevier Spark Use Cases with Databricks and Contribution to Apache Spark Packages</td><td>2015-11-11T00:00:00.000+0000</td></tr><tr><td>Elsevier Labs Deploys Databricks for Unified Content Analysis</td><td>2015-11-12T00:00:00.000+0000</td></tr><tr><td>Databricks launches Meetup-in-a-box for Apache Spark Meetup Organizers</td><td>2015-11-19T00:00:00.000+0000</td></tr><tr><td>Building a Just-In-Time Data Warehouse Platform with Databricks</td><td>2015-11-30T00:00:00.000+0000</td></tr><tr><td>Announcing an Apache Spark 1.6 Preview in Databricks</td><td>2015-11-20T00:00:00.000+0000</td></tr><tr><td>New Databricks release: Preview of Apache Spark 1.6, easier search, and more</td><td>2015-11-24T00:00:00.000+0000</td></tr><tr><td>Databricks and H2O Make it Rain with Sparkling Water</td><td>2015-12-02T00:00:00.000+0000</td></tr><tr><td>Guest Blog: Streamliner - An Open Source Apache Spark Streaming Application</td><td>2015-12-18T00:00:00.000+0000</td></tr><tr><td>The Best of The Databricks Blog: Most Read Posts of 2015</td><td>2015-12-22T00:00:00.000+0000</td></tr><tr><td>LendUp Expands Access to Credit with Databricks</td><td>2015-12-28T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark 1.6</td><td>2016-01-04T00:00:00.000+0000</td></tr><tr><td>Introducing Apache Spark Datasets</td><td>2016-01-04T00:00:00.000+0000</td></tr><tr><td>Databricks 2015 Year In Review: Democratizing Access to Data</td><td>2016-01-05T00:00:00.000+0000</td></tr><tr><td>Apache Spark 2015 Year In Review</td><td>2016-01-05T00:00:00.000+0000</td></tr><tr><td>Announcing Apache Spark Essentials for the Public Sector workshop</td><td>2016-01-12T00:00:00.000+0000</td></tr><tr><td>Spark Summit East 2016 Agenda is now available</td><td>2016-01-13T00:00:00.000+0000</td></tr><tr><td>MLlib Highlights in Apache Spark 1.6</td><td>2016-01-21T00:00:00.000+0000</td></tr><tr><td>Deep Learning with Apache Spark and TensorFlow</td><td>2016-01-25T00:00:00.000+0000</td></tr><tr><td>Faster Stateful Stream Processing in Apache Spark Streaming</td><td>2016-02-01T00:00:00.000+0000</td></tr><tr><td>An Illustrated Guide to Advertising Analytics</td><td>2016-02-03T00:00:00.000+0000</td></tr><tr><td>Eyeview Partners with Databricks and AWS to Democratize Data Across a Unified Infrastructure</td><td>2016-02-03T00:00:00.000+0000</td></tr><tr><td>Inneractive Optimizes the Mobile Ad Buying Experience at Scale with Machine Learning on Databricks</td><td>2016-02-04T00:00:00.000+0000</td></tr><tr><td>Auto-scaling scikit-learn with Apache Spark</td><td>2016-02-08T00:00:00.000+0000</td></tr><tr><td>Reshaping Data with Pivot in Apache Spark</td><td>2016-02-09T00:00:00.000+0000</td></tr><tr><td>How Elsevier Labs Implemented Dictionary Annotation at Scale with Apache Spark on Databricks</td><td>2016-02-10T00:00:00.000+0000</td></tr><tr><td>Findify’s Smart Search Gets Smarter with Apache Spark MLlib and Databricks</td><td>2016-02-12T00:00:00.000+0000</td></tr><tr><td>Introducing Databricks Dashboards</td><td>2016-02-17T00:00:00.000+0000</td></tr><tr><td>Introducing Databricks Community Edition: Apache Spark for All</td><td>2016-02-17T00:00:00.000+0000</td></tr><tr><td>A Look Back at Spark Summit East 2016: Thank you NYC!</td><td>2016-02-18T00:00:00.000+0000</td></tr><tr><td>Introducing GraphFrames</td><td>2016-03-03T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar: Jump Start into Apache Spark and Databricks</td><td>2016-03-09T00:00:00.000+0000</td></tr><tr><td>How Sellpoints Launched a New Predictive Analytics Product with Databricks</td><td>2016-03-10T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: Apache Spark 1.5</td><td>2016-03-16T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: How Celtra Optimizes its Advertising Platform with Databricks</td><td>2016-03-15T00:00:00.000+0000</td></tr><tr><td>On-Time Flight Performance with GraphFrames for Apache Spark</td><td>2016-03-16T00:00:00.000+0000</td></tr><tr><td>How to Process IoT Device JSON Data Using Apache Spark Datasets and DataFrames</td><td>2016-03-28T00:00:00.000+0000</td></tr><tr><td>Apache Spark Trending in the Stack Overflow Survey</td><td>2016-03-22T00:00:00.000+0000</td></tr><tr><td>How Metacog Implemented Agile Apache Spark Application Development to Release New Products Twice as Fast</td><td>2016-03-31T00:00:00.000+0000</td></tr><tr><td>Announcing New Databricks APIs for Faster Production Apache Spark Application Deployment</td><td>2016-03-30T00:00:00.000+0000</td></tr><tr><td>Introducing our new eBook: Apache Spark Analytics Made Simple</td><td>2016-03-31T00:00:00.000+0000</td></tr><tr><td>The Unreasonable Effectiveness of Deep Learning on Apache Spark</td><td>2016-04-01T00:00:00.000+0000</td></tr><tr><td>Agenda Announced for #SparkSummit 2016 in San Francisco</td><td>2016-04-04T00:00:00.000+0000</td></tr><tr><td>Continuous Integration and Delivery of Apache Spark Applications at Metacog</td><td>2016-04-06T00:00:00.000+0000</td></tr><tr><td>How DNV GL Uses Databricks to Build Tomorrow’s Energy Grid</td><td>2016-04-07T00:00:00.000+0000</td></tr><tr><td>New Content in Databricks Community Edition</td><td>2016-04-12T00:00:00.000+0000</td></tr><tr><td>GraphFrames On-Demand Webinar and FAQ</td><td>2016-04-21T00:00:00.000+0000</td></tr><tr><td>New eBook Released: Mastering Advanced Analytics with Apache Spark</td><td>2016-04-27T00:00:00.000+0000</td></tr><tr><td>Introducing the Spark Live 2016 Tour</td><td>2016-05-04T00:00:00.000+0000</td></tr><tr><td>How Omega Point Delivers Portfolio Insights for Financial Services with Databricks</td><td>2016-05-03T00:00:00.000+0000</td></tr><tr><td>Spark Saturday DC: A Meetup Summary</td><td>2016-05-09T00:00:00.000+0000</td></tr><tr><td>Technical Preview of Apache Spark 2.0 Now on Databricks</td><td>2016-05-11T00:00:00.000+0000</td></tr><tr><td>Sellpoints Develops Shopper Insights with Databricks</td><td>2016-05-11T00:00:00.000+0000</td></tr><tr><td>Apache Spark MLlib: From Quick Start to Scikit-Learn</td><td>2016-05-18T00:00:00.000+0000</td></tr><tr><td>Just-in-Time Data Warehousing on Databricks: Change Data Capture and Schema On Read</td><td>2016-05-26T00:00:00.000+0000</td></tr><tr><td>6 Reasons to Attend Spark Summit</td><td>2016-05-18T00:00:00.000+0000</td></tr><tr><td>Approximate Algorithms in Apache Spark: HyperLogLog and Quantiles</td><td>2016-05-19T00:00:00.000+0000</td></tr><tr><td>Spark Live Los Angeles is just around the corner</td><td>2016-05-20T00:00:00.000+0000</td></tr><tr><td>Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</td><td>2016-05-23T00:00:00.000+0000</td></tr><tr><td>Genome Sequencing in a Nutshell</td><td>2016-05-24T00:00:00.000+0000</td></tr><tr><td>Parallelizing Genome Variant Analysis</td><td>2016-05-24T00:00:00.000+0000</td></tr><tr><td>Predicting Geographic Population using Genome Variants and K-Means</td><td>2016-05-24T00:00:00.000+0000</td></tr><tr><td>That’s a Wrap! Spark Live Draws Huge Audience in Los Angeles</td><td>2016-05-27T00:00:00.000+0000</td></tr><tr><td>Not Your Father’s Database: How to Use Apache Spark Properly in your Big Data Architecture</td><td>2016-06-02T00:00:00.000+0000</td></tr><tr><td>Apache Spark 2.0 Preview: Machine Learning Model Persistence</td><td>2016-05-31T00:00:00.000+0000</td></tr><tr><td>Achieving End-to-end Security for Apache Spark with Databricks</td><td>2016-06-08T00:00:00.000+0000</td></tr><tr><td>Databricks to Launch First of Five Free Big Data Courses on Apache Spark</td><td>2016-06-01T00:00:00.000+0000</td></tr><tr><td>Apache Spark 2.0: An Anthology of Technical Assets</td><td>2016-06-01T00:00:00.000+0000</td></tr><tr><td>Databricks Community Edition is now Generally Available</td><td>2016-06-07T00:00:00.000+0000</td></tr><tr><td>Another Record-Setting Spark Summit</td><td>2016-06-09T00:00:00.000+0000</td></tr><tr><td>An Introduction to Writing Apache Spark Applications on Databricks</td><td>2016-06-15T00:00:00.000+0000</td></tr><tr><td>SQL Subqueries in Apache Spark 2.0</td><td>2016-06-17T00:00:00.000+0000</td></tr><tr><td>Apache Spark Key Terms, Explained</td><td>2016-06-22T00:00:00.000+0000</td></tr><tr><td>Share your Thoughts in our Apache Spark Survey Today</td><td>2016-06-23T00:00:00.000+0000</td></tr><tr><td>Building Data Science Applications on Databricks</td><td>2016-06-28T00:00:00.000+0000</td></tr><tr><td>Introducing Getting Started with Apache Spark on Databricks</td><td>2016-06-30T00:00:00.000+0000</td></tr><tr><td>New eBook Released: Lessons for Large-Scale Machine Learning Deployments on Apache Spark</td><td>2016-07-06T00:00:00.000+0000</td></tr><tr><td>Edmunds.com Leverages Databricks to Improve Vehicle Data Quality and Customer Experience</td><td>2016-07-05T00:00:00.000+0000</td></tr><tr><td>SparkR Tutorial at useR 2016</td><td>2016-07-07T00:00:00.000+0000</td></tr><tr><td>Apache SparkR On-Demand Webinar and FAQ</td><td>2016-07-12T00:00:00.000+0000</td></tr><tr><td>A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</td><td>2016-07-14T00:00:00.000+0000</td></tr><tr><td>Databricks Bi-Weekly Digest: 7/18/16</td><td>2016-07-18T00:00:00.000+0000</td></tr><tr><td>Code 4 San Francisco Hack Nite Highlights</td><td>2016-07-20T00:00:00.000+0000</td></tr><tr><td>The New MongoDB Connector for Apache Spark In Action: Building a Movie Recommendation Engine</td><td>2016-07-21T00:00:00.000+0000</td></tr><tr><td>Introducing Apache Spark 2.0</td><td>2016-07-26T00:00:00.000+0000</td></tr><tr><td>Continuous Applications: Evolving Streaming in Apache Spark 2.0</td><td>2016-07-28T00:00:00.000+0000</td></tr><tr><td>Structured Streaming In Apache Spark</td><td>2016-07-28T00:00:00.000+0000</td></tr><tr><td>Developing Apache Spark Applications in .NET using Mobius</td><td>2016-08-03T00:00:00.000+0000</td></tr><tr><td>Agenda Announced for #SparkSummit Europe 2016 in Brussels</td><td>2016-08-04T00:00:00.000+0000</td></tr><tr><td>Databricks Bi-Weekly Digest: 8/8/16</td><td>2016-08-08T00:00:00.000+0000</td></tr><tr><td>The Quest for Hidden Treasure: An Apache Spark Connector for the Riak NoSQL database</td><td>2016-08-11T00:00:00.000+0000</td></tr><tr><td>On-demand webinar available: Databricks’ Data Pipeline</td><td>2016-08-16T00:00:00.000+0000</td></tr><tr><td>How to use SparkSession in Apache Spark 2.0</td><td>2016-08-15T00:00:00.000+0000</td></tr><tr><td>Notebook Workflows: The Easiest Way to Implement Apache Spark Pipelines</td><td>2016-08-30T00:00:00.000+0000</td></tr><tr><td>Databricks Bi-Weekly Digest: 8/31/16</td><td>2016-08-31T00:00:00.000+0000</td></tr><tr><td>Apache Spark @Scale: A 60 TB+ production use case from Facebook</td><td>2016-08-31T00:00:00.000+0000</td></tr><tr><td>Writing Data Engineering Pipelines in Apache Spark on Databricks</td><td>2016-09-06T00:00:00.000+0000</td></tr><tr><td>Apache Spark Earns Datanami Awards for Machine Learning, Real-time Analytics, and More</td><td>2016-09-19T00:00:00.000+0000</td></tr><tr><td>How to Evaluate a Cloud-based Apache Spark Platform</td><td>2016-09-28T00:00:00.000+0000</td></tr><tr><td>Apache Spark Survey 2016 Results Now Available</td><td>2016-09-27T00:00:00.000+0000</td></tr><tr><td>Voice from CERN: Apache Spark 2.0 Performance Improvements Investigated With Flame Graphs</td><td>2016-10-03T00:00:00.000+0000</td></tr><tr><td>Databricks Completes SOC 2 Type 1 Certification</td><td>2016-10-04T00:00:00.000+0000</td></tr><tr><td>Databricks Bi-Weekly Apache Spark Digest: 10/4/16</td><td>2016-10-04T00:00:00.000+0000</td></tr><tr><td>New Webinar: How Edmunds.com leverages Apache Spark on Databricks to Improve Customer Conversion</td><td>2016-10-05T00:00:00.000+0000</td></tr><tr><td>New Webinar: Databricks for Data Engineers</td><td>2016-10-12T00:00:00.000+0000</td></tr><tr><td>Using AWS Lambda with Databricks for ETL Automation and ML Model Serving</td><td>2016-10-11T00:00:00.000+0000</td></tr><tr><td>7 Tips to Debug Apache Spark Code Faster with Databricks</td><td>2016-10-18T00:00:00.000+0000</td></tr><tr><td>GPU Acceleration in Databricks</td><td>2016-10-27T00:00:00.000+0000</td></tr><tr><td>Running Apache Spark Clusters with Spot Instances in Databricks</td><td>2016-10-25T00:00:00.000+0000</td></tr><tr><td>Databricks Voices From Spark Summit EU 2016 Day 1</td><td>2016-10-26T00:00:00.000+0000</td></tr><tr><td>Databricks Voices From Spark Summit EU 2016 Day 2</td><td>2016-10-27T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: Databricks for Data Engineers</td><td>2016-11-01T00:00:00.000+0000</td></tr><tr><td>How HomeAway is Transforming the Vacation Rental Industry with Databricks</td><td>2016-11-03T00:00:00.000+0000</td></tr><tr><td>Databricks Launches a Comprehensive Guide for Its Product and Apache Spark</td><td>2016-11-10T00:00:00.000+0000</td></tr><tr><td>New Webinar: How to Evaluate Cloud-based Apache Spark Platforms</td><td>2016-11-10T00:00:00.000+0000</td></tr><tr><td>$1.44 per terabyte: setting a new world record with Apache Spark</td><td>2016-11-15T00:00:00.000+0000</td></tr><tr><td>Databricks Bi-Weekly Apache Spark Digest: 11/16/16</td><td>2016-11-16T00:00:00.000+0000</td></tr><tr><td>Oil and Gas Asset Optimization with AWS Kinesis, RDS, and Databricks</td><td>2016-11-16T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: How to Evaluate Cloud-based Apache Spark Platforms</td><td>2016-11-23T00:00:00.000+0000</td></tr><tr><td>Integrating Apache Airflow and Databricks: Building ETL pipelines with Apache Spark</td><td>2016-12-08T00:00:00.000+0000</td></tr><tr><td>Apache Spark Scala Library Development with Databricks</td><td>2016-12-12T00:00:00.000+0000</td></tr><tr><td>On Demand Webinar and FAQ: Apache Spark MLlib 2.x: Migrating ML Workloads to DataFrames</td><td>2016-12-14T00:00:00.000+0000</td></tr><tr><td>Scalable Partition Handling for Cloud-Native Architecture in Apache Spark 2.1</td><td>2016-12-15T00:00:00.000+0000</td></tr><tr><td>10 Things I Wish I Knew Before Using Apache SparkR</td><td>2016-12-28T00:00:00.000+0000</td></tr><tr><td>Deep Learning on Databricks</td><td>2016-12-21T00:00:00.000+0000</td></tr><tr><td>Top 10 Apache Spark Blog Posts from 2016</td><td>2016-12-30T00:00:00.000+0000</td></tr><tr><td>Introducing Apache Spark 2.1</td><td>2016-12-29T00:00:00.000+0000</td></tr><tr><td>Databricks and Apache Spark 2016 Year in Review</td><td>2017-01-04T00:00:00.000+0000</td></tr><tr><td>Spark Live 2016 Tour Recap</td><td>2017-01-03T00:00:00.000+0000</td></tr><tr><td>5 Can’t Miss Talks at Spark Summit East 2017</td><td>2017-01-09T00:00:00.000+0000</td></tr><tr><td>5 Reasons to Attend Spark Summit East 2017</td><td>2017-01-10T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: Apache Spark - The Unified Engine for All Workloads</td><td>2017-01-18T00:00:00.000+0000</td></tr><tr><td>Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</td><td>2017-01-19T00:00:00.000+0000</td></tr><tr><td>Integration of AWS Data Pipeline with Databricks: Building ETL pipelines with Apache Spark</td><td>2017-01-23T00:00:00.000+0000</td></tr><tr><td>Delivering Exceptional Care Through Data-Driven Medicine</td><td>2017-01-25T00:00:00.000+0000</td></tr><tr><td>Integrating Your Central Apache Hive Metastore with Apache Spark on Databricks</td><td>2017-01-30T00:00:00.000+0000</td></tr><tr><td>Announcing the Spark Live 2017 World Tour</td><td>2017-01-31T00:00:00.000+0000</td></tr><tr><td>Intel’s BigDL on Databricks</td><td>2017-02-09T00:00:00.000+0000</td></tr><tr><td>Spark Summit East 2017: Another Record-Setting Spark Summit</td><td>2017-02-10T00:00:00.000+0000</td></tr><tr><td>Anonymizing Datasets at Scale Leveraging Databricks Interoperability</td><td>2017-02-13T00:00:00.000+0000</td></tr><tr><td>Processing a Trillion Rows Per Second on a Single Machine: How Can Nested Loop Joins be this Fast?</td><td>2017-02-16T00:00:00.000+0000</td></tr><tr><td>Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1</td><td>2017-02-23T00:00:00.000+0000</td></tr><tr><td>How Apache Spark on Databricks is Taming the Wild West of Wi-Fi</td><td>2017-02-27T00:00:00.000+0000</td></tr><tr><td>Voice from Facebook: Using Apache Spark for Large-Scale Language Model Training</td><td>2017-02-28T00:00:00.000+0000</td></tr><tr><td>Delivering a Personalized Shopping Experience with Apache Spark on Databricks</td><td>2017-03-31T00:00:00.000+0000</td></tr><tr><td>Analyse One Year of Radio Station Songs Aired with Apache Spark, Spark SQL, Spotify, and Databricks</td><td>2017-03-27T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: Apache Spark MLlib 2.x: How to Productionize your Machine Learning Models</td><td>2017-03-28T00:00:00.000+0000</td></tr><tr><td>The Tenth Spark Summit with a Terrific Agenda for All</td><td>2017-03-30T00:00:00.000+0000</td></tr><tr><td>Next Generation Physical Planning in Apache Spark</td><td>2017-04-01T00:00:00.000+0000</td></tr><tr><td>Take Reports From Concept to Production with PySpark and Databricks</td><td>2017-04-03T00:00:00.000+0000</td></tr><tr><td>Real-Time End-to-End Integration with Apache Kafka in Apache Spark’s Structured Streaming</td><td>2017-04-04T00:00:00.000+0000</td></tr><tr><td>Query Watchdog: Handling Disruptive Queries in Spark SQL</td><td>2017-04-17T00:00:00.000+0000</td></tr><tr><td>Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</td><td>2017-04-26T00:00:00.000+0000</td></tr><tr><td>Event-time Aggregation and Watermarking in Apache Spark’s Structured Streaming</td><td>2017-05-08T00:00:00.000+0000</td></tr><tr><td>Detecting Abuse at Scale: Locality Sensitive Hashing at Uber Engineering</td><td>2017-05-09T00:00:00.000+0000</td></tr><tr><td>Persistent Clusters: Simplifying Cluster Management for Analytics</td><td>2017-05-19T00:00:00.000+0000</td></tr><tr><td>Taking Apache Spark’s Structured Streaming to Production</td><td>2017-05-18T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: Deep Learning and Apache Spark: Workflows and Best Practices</td><td>2017-05-23T00:00:00.000+0000</td></tr><tr><td>Running Streaming Jobs Once a Day For 10x Cost Savings</td><td>2017-05-22T00:00:00.000+0000</td></tr><tr><td>Databricks Runtime 3.0 Beta Delivers Cloud Optimized Apache Spark</td><td>2017-05-24T00:00:00.000+0000</td></tr><tr><td>Working with Nested Data Using Higher Order Functions in SQL on Databricks</td><td>2017-05-24T00:00:00.000+0000</td></tr><tr><td>Using sparklyr in Databricks</td><td>2017-05-25T00:00:00.000+0000</td></tr><tr><td>Bay Area Apache Spark Meetup Summary</td><td>2017-05-26T00:00:00.000+0000</td></tr><tr><td>Entropy-based Log Redaction for Apache Spark on Databricks</td><td>2017-05-30T00:00:00.000+0000</td></tr><tr><td>Top 5 Reasons for Choosing S3 over HDFS</td><td>2017-05-31T00:00:00.000+0000</td></tr><tr><td>Transactional Writes to Cloud Storage on Databricks</td><td>2017-05-31T00:00:00.000+0000</td></tr><tr><td>Apache Spark Cluster Monitoring with Databricks and Datadog</td><td>2017-06-01T00:00:00.000+0000</td></tr><tr><td>It's Almost Time for Spark Summit 2017 in San Francisco</td><td>2017-06-01T00:00:00.000+0000</td></tr><tr><td>Integrating Apache Spark with Cucumber for Behavioral-Driven Development</td><td>2017-06-02T00:00:00.000+0000</td></tr><tr><td>Making Apache Spark the Fastest Open Source Streaming Engine</td><td>2017-06-06T00:00:00.000+0000</td></tr><tr><td>A Vision for Making Deep Learning Simple</td><td>2017-06-06T00:00:00.000+0000</td></tr><tr><td>Sharing Knowledge with the Community in a Preview of Apache Spark: The Definitive Guide</td><td>2017-06-05T00:00:00.000+0000</td></tr><tr><td>Databricks Serverless: Next Generation Resource Management for Apache Spark</td><td>2017-06-07T00:00:00.000+0000</td></tr><tr><td>10th Spark Summit Sets Another Record of Attendance</td><td>2017-06-09T00:00:00.000+0000</td></tr><tr><td>Analysing Metro Operations Using Apache Spark on Databricks</td><td>2017-06-14T00:00:00.000+0000</td></tr><tr><td>Five Spark SQL Utility Functions to Extract and Explore Complex Data Types</td><td>2017-06-13T00:00:00.000+0000</td></tr><tr><td>Managing and Securing Credentials in Databricks for Apache Spark Jobs</td><td>2017-06-20T00:00:00.000+0000</td></tr><tr><td>Parallelizing Large Simulations with Apache SparkR on Databricks</td><td>2017-06-23T00:00:00.000+0000</td></tr><tr><td>Declarative Infrastructure with the Jsonnet Templating Language</td><td>2017-06-26T00:00:00.000+0000</td></tr><tr><td>4 SQL High-Order and Lambda Functions to Examine Complex and Structured Data in Databricks</td><td>2017-06-27T00:00:00.000+0000</td></tr><tr><td>Serverless Continuous Delivery with Databricks and AWS CodePipeline</td><td>2017-07-13T00:00:00.000+0000</td></tr><tr><td>Benchmarking Big Data SQL Platforms in the Cloud</td><td>2017-07-12T00:00:00.000+0000</td></tr><tr><td>Introducing Apache Spark 2.2</td><td>2017-07-11T00:00:00.000+0000</td></tr><tr><td>Integrating Apache Airflow with Databricks</td><td>2017-07-19T00:00:00.000+0000</td></tr><tr><td>Breaking the “curse of dimensionality” in Genomics using “wide” Random Forests</td><td>2017-07-26T00:00:00.000+0000</td></tr><tr><td>On-Demand Webinar and FAQ: Accelerate Data Science with Better Data Engineering on Databricks</td><td>2017-07-31T00:00:00.000+0000</td></tr><tr><td>Databricks Named as a Strong Performer in The Forrester Wave: Insight Platforms-as-a-Service, Q3 2017</td><td>2017-08-08T00:00:00.000+0000</td></tr><tr><td>Apache Spark’s Structured Streaming with Amazon Kinesis on Databricks</td><td>2017-08-09T00:00:00.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Create the temporary view `DatabricksBlog2` to capture the conversion and flattening of the `publishedOn` column."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW DatabricksBlog2 AS\n  SELECT *, \n         cast(dates.publishedOn AS timestamp) AS publishedOn \n  FROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Now that we have this temporary view, we can use `DESCRIBE` to check its schema and confirm the timestamp conversion."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DatabricksBlog2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>authors</td><td>array<string></td><td>null</td></tr><tr><td>categories</td><td>array<string></td><td>null</td></tr><tr><td>content</td><td>string</td><td>null</td></tr><tr><td>creator</td><td>string</td><td>null</td></tr><tr><td>dates</td><td>struct<createdOn:string,publishedOn:string,tz:string></td><td>null</td></tr><tr><td>description</td><td>string</td><td>null</td></tr><tr><td>id</td><td>bigint</td><td>null</td></tr><tr><td>link</td><td>string</td><td>null</td></tr><tr><td>slug</td><td>string</td><td>null</td></tr><tr><td>status</td><td>string</td><td>null</td></tr><tr><td>title</td><td>string</td><td>null</td></tr><tr><td>publishedOn</td><td>timestamp</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["-sandbox\nNow the dates are represented by a `timestamp` data type, query for articles within certain date ranges (such as getting a list of all articles published in 2013), and format the date for presentation purposes.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">built-in functions</a>, for a long list of date-specific functions."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       date_format(publishedOn, \"MMM dd, yyyy\") AS date, \n       link \nFROM DatabricksBlog2\nWHERE year(publishedOn) = 2013\nORDER BY publishedOn"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>date</th><th>link</th></tr></thead><tbody><tr><td>Databricks and the Apache Spark Platform</td><td>Oct 27, 2013</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td></tr><tr><td>The Growing Apache Spark Community</td><td>Oct 28, 2013</td><td>https://databricks.com/blog/2013/10/27/the-growing-spark-community.html</td></tr><tr><td>Databricks and Cloudera Partner to Support Apache Spark</td><td>Oct 29, 2013</td><td>https://databricks.com/blog/2013/10/28/databricks-and-cloudera-partner-to-support-spark.html</td></tr><tr><td>Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications</td><td>Nov 22, 2013</td><td>https://databricks.com/blog/2013/11/21/putting-spark-to-use.html</td></tr><tr><td>Highlights From Spark Summit 2013</td><td>Dec 19, 2013</td><td>https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html</td></tr><tr><td>Apache Spark 0.8.1 Released</td><td>Dec 20, 2013</td><td>https://databricks.com/blog/2013/12/19/release-0_8_1.html</td></tr></tbody></table></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["## Array Data\n\nThe table also contains array columns. \n\nEasily determine the size of each array using the built-in `size(..)` function with array columns."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/w9vj8mjpf7?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/w9vj8mjpf7?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT size(authors), \n       authors \nFROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>size(authors)</th><th>authors</th></tr></thead><tbody><tr><td>1</td><td>List(Tomer Shiran (VP of Product Management at MapR))</td></tr><tr><td>1</td><td>List(Tathagata Das)</td></tr><tr><td>1</td><td>List(Steven Hillion)</td></tr><tr><td>2</td><td>List(Michael Armbrust, Reynold Xin)</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>2</td><td>List(Ali Ghodsi, Ahir Reddy)</td></tr><tr><td>2</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td></tr><tr><td>2</td><td>List(Jai Ranganathan, Matei Zaharia)</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Ion Stoica)</td></tr><tr><td>2</td><td>List(Ahir Reddy, Reynold Xin)</td></tr><tr><td>1</td><td>List(Pat McDonough)</td></tr><tr><td>1</td><td>List(Ion Stoica)</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>1</td><td>List(Andy Konwinski)</td></tr><tr><td>1</td><td>List(Pat McDonough)</td></tr><tr><td>1</td><td>List(Ion Stoica)</td></tr><tr><td>1</td><td>List(Matei Zaharia)</td></tr><tr><td>2</td><td>List(Ion Stoica, Matei Zaharia)</td></tr><tr><td>1</td><td>List(Arsalan Tavakoli-Shiraji)</td></tr><tr><td>2</td><td>List(Prashant Sharma, Matei Zaharia)</td></tr><tr><td>1</td><td>List(Databricks Training Team)</td></tr><tr><td>1</td><td>List(Claudiu Barbura (Sr. Dir. of Engineering at Atigeo LLC))</td></tr><tr><td>1</td><td>List(Sarabjeet Chugh (Head of Hadoop Product Management at Pivotal Inc.))</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>2</td><td>List(Michael Armbrust, Zongheng Yang)</td></tr><tr><td>1</td><td>List(Michael Hiskey (VP at MicroStrategy Inc.))</td></tr><tr><td>1</td><td>List(Christopher Nguyen (CEO &amp; Co-Founder of Adatao))</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Dean Wampler (Typesafe))</td></tr><tr><td>1</td><td>List(Hari Kodakalla (EVP at Apervi Inc.))</td></tr><tr><td>1</td><td>List(Bill Kehoe (Big Data Architect at Qlik))</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Costin Leau (Engineer at Elasticsearch))</td></tr><tr><td>1</td><td>List(Jake Cornelius (SVP of Product Management at Pentaho))</td></tr><tr><td>1</td><td>List(SriSatish Ambati (CEO of 0xData))</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Arsalan Tavakoli-Shiraji)</td></tr><tr><td>1</td><td>List(Arsalan Tavakoli-Shiraji)</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Databricks Press Office)</td></tr><tr><td>1</td><td>List(Arsalan Tavakoli-Shiraji)</td></tr><tr><td>1</td><td>List(Reynold Xin)</td></tr><tr><td>1</td><td>List(Ion Stoica)</td></tr><tr><td>1</td><td>List(Xiangrui Meng)</td></tr><tr><td>1</td><td>List(Matei Zaharia)</td></tr><tr><td>3</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td></tr><tr><td>2</td><td>List(Li Pu, Reza Zadeh)</td></tr><tr><td>1</td><td>List(Scott Walent)</td></tr><tr><td>1</td><td>List(Oscar Mendez (CEO of Stratio))</td></tr><tr><td>2</td><td>List(Andy Huang (Alibaba Taobao Data Mining Team), Wei Wu (Alibaba Taobao Data Mining Team))</td></tr><tr><td>4</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>3</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td></tr><tr><td>2</td><td>List(Burak Yavuz, Xiangrui Meng)</td></tr><tr><td>1</td><td>List(Gavin Targonski (Product Management at Talend))</td></tr><tr><td>2</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td></tr><tr><td>1</td><td>List(Vida Ha)</td></tr><tr><td>2</td><td>List(John Tripier, Paco Nathan)</td></tr><tr><td>1</td><td>List(Christopher Burdorf (Senior Software Engineer at NBC Universal))</td></tr><tr><td>2</td><td>List(Manish Amde (Origami Logic), Joseph Bradley (Databricks))</td></tr><tr><td>1</td><td>List(Eric Carr (VP Core Systems Group at Guavus))</td></tr><tr><td>1</td><td>List(Jeremy Freeman (Freeman Lab))</td></tr><tr><td>1</td><td>List(Russell Cardullo (Sharethrough))</td></tr><tr><td>1</td><td>List(Sean Kandel (CTO at Trifacta))</td></tr><tr><td>1</td><td>List(Reynold Xin)</td></tr><tr><td>1</td><td>List(Reza Zadeh)</td></tr><tr><td>1</td><td>List(Jeff Feng (Product Manager at Tableau Software))</td></tr><tr><td>1</td><td>List(Scott Walent)</td></tr><tr><td>2</td><td>List(Ari Himmel (CEO at Faimdata), Nan Zhu (Chief Architect at Faimdata))</td></tr><tr><td>1</td><td>List(John Kreisa (VP of Strategic Marketing at Hortonworks))</td></tr><tr><td>1</td><td>List(Sachin Chawla (VP of Engineering))</td></tr><tr><td>1</td><td>List(Sonal Goyal (CEO))</td></tr><tr><td>1</td><td>List( Dibyendu Bhattacharya (Big Data Architect))</td></tr><tr><td>1</td><td>List(Reynold Xin)</td></tr><tr><td>1</td><td>List(Matt MacKinnon (Director of Product Management at Zaloni))</td></tr><tr><td>2</td><td>List(John Tripier, Paco Nathan)</td></tr><tr><td>3</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td></tr><tr><td>2</td><td>List(Ameet Talwalkar, Anthony Joseph)</td></tr><tr><td>1</td><td>List(Lieven Gesquiere (Virdata Lead Core R&D))</td></tr><tr><td>1</td><td>List(by Databricks Press Office)</td></tr><tr><td>1</td><td>List(Kavitha Mariappan)</td></tr><tr><td>1</td><td>List(Kavitha Mariappan)</td></tr><tr><td>1</td><td>List(Yin Huai (Databricks))</td></tr><tr><td>1</td><td>List(Jeremy Freeman (Howard Hughes Medical Institute))</td></tr><tr><td>1</td><td>List(Dave Wang (Databricks))</td></tr><tr><td>1</td><td>List(Patrick Wendell)</td></tr><tr><td>2</td><td>List(Xiangrui Meng, Patrick Wendell)</td></tr><tr><td>4</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td></tr><tr><td>1</td><td>List(Michael Armbrust)</td></tr><tr><td>2</td><td>List(Joseph K. Bradley (Databricks), Manish Amde (Origami Logic))</td></tr><tr><td>1</td><td>List(Tathagata Das)</td></tr><tr><td>1</td><td>List(Kavitha Mariappan)</td></tr><tr><td>4</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr><tr><td>-1</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["Pull the first element from the array `authors` using an array subscript operator."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT authors[0] AS primaryAuthor \nFROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>primaryAuthor</th></tr></thead><tbody><tr><td>Tomer Shiran (VP of Product Management at MapR)</td></tr><tr><td>Tathagata Das</td></tr><tr><td>Steven Hillion</td></tr><tr><td>Michael Armbrust</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Ali Ghodsi</td></tr><tr><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td></tr><tr><td>Jai Ranganathan</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Ion Stoica</td></tr><tr><td>Ahir Reddy</td></tr><tr><td>Pat McDonough</td></tr><tr><td>Ion Stoica</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Andy Konwinski</td></tr><tr><td>Pat McDonough</td></tr><tr><td>Ion Stoica</td></tr><tr><td>Matei Zaharia</td></tr><tr><td>Ion Stoica</td></tr><tr><td>Arsalan Tavakoli-Shiraji</td></tr><tr><td>Prashant Sharma</td></tr><tr><td>Databricks Training Team</td></tr><tr><td>Claudiu Barbura (Sr. Dir. of Engineering at Atigeo LLC)</td></tr><tr><td>Sarabjeet Chugh (Head of Hadoop Product Management at Pivotal Inc.)</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Michael Armbrust</td></tr><tr><td>Michael Hiskey (VP at MicroStrategy Inc.)</td></tr><tr><td>Christopher Nguyen (CEO &amp; Co-Founder of Adatao)</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Dean Wampler (Typesafe)</td></tr><tr><td>Hari Kodakalla (EVP at Apervi Inc.)</td></tr><tr><td>Bill Kehoe (Big Data Architect at Qlik)</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Costin Leau (Engineer at Elasticsearch)</td></tr><tr><td>Jake Cornelius (SVP of Product Management at Pentaho)</td></tr><tr><td>SriSatish Ambati (CEO of 0xData)</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Arsalan Tavakoli-Shiraji</td></tr><tr><td>Arsalan Tavakoli-Shiraji</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Databricks Press Office</td></tr><tr><td>Arsalan Tavakoli-Shiraji</td></tr><tr><td>Reynold Xin</td></tr><tr><td>Ion Stoica</td></tr><tr><td>Xiangrui Meng</td></tr><tr><td>Matei Zaharia</td></tr><tr><td>Burak Yavuz</td></tr><tr><td>Li Pu</td></tr><tr><td>Scott Walent</td></tr><tr><td>Oscar Mendez (CEO of Stratio)</td></tr><tr><td>Andy Huang (Alibaba Taobao Data Mining Team)</td></tr><tr><td>Doris Xin</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Arsalan Tavakoli-Shiraji</td></tr><tr><td>Burak Yavuz</td></tr><tr><td>Gavin Targonski (Product Management at Talend)</td></tr><tr><td>Nick Pentreath (Graphflow)</td></tr><tr><td>Vida Ha</td></tr><tr><td>John Tripier</td></tr><tr><td>Christopher Burdorf (Senior Software Engineer at NBC Universal)</td></tr><tr><td>Manish Amde (Origami Logic)</td></tr><tr><td>Eric Carr (VP Core Systems Group at Guavus)</td></tr><tr><td>Jeremy Freeman (Freeman Lab)</td></tr><tr><td>Russell Cardullo (Sharethrough)</td></tr><tr><td>Sean Kandel (CTO at Trifacta)</td></tr><tr><td>Reynold Xin</td></tr><tr><td>Reza Zadeh</td></tr><tr><td>Jeff Feng (Product Manager at Tableau Software)</td></tr><tr><td>Scott Walent</td></tr><tr><td>Ari Himmel (CEO at Faimdata)</td></tr><tr><td>John Kreisa (VP of Strategic Marketing at Hortonworks)</td></tr><tr><td>Sachin Chawla (VP of Engineering)</td></tr><tr><td>Sonal Goyal (CEO)</td></tr><tr><td> Dibyendu Bhattacharya (Big Data Architect)</td></tr><tr><td>Reynold Xin</td></tr><tr><td>Matt MacKinnon (Director of Product Management at Zaloni)</td></tr><tr><td>John Tripier</td></tr><tr><td>Luis Quintela (Sr. Manager of Big Data Analytics)</td></tr><tr><td>Ameet Talwalkar</td></tr><tr><td>Lieven Gesquiere (Virdata Lead Core R&D)</td></tr><tr><td>by Databricks Press Office</td></tr><tr><td>Kavitha Mariappan</td></tr><tr><td>Kavitha Mariappan</td></tr><tr><td>Yin Huai (Databricks)</td></tr><tr><td>Jeremy Freeman (Howard Hughes Medical Institute)</td></tr><tr><td>Dave Wang (Databricks)</td></tr><tr><td>Patrick Wendell</td></tr><tr><td>Xiangrui Meng</td></tr><tr><td>Xiangrui Meng</td></tr><tr><td>Michael Armbrust</td></tr><tr><td>Joseph K. Bradley (Databricks)</td></tr><tr><td>Tathagata Das</td></tr><tr><td>Kavitha Mariappan</td></tr><tr><td>Holden Karau</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr><tr><td>null</td></tr></tbody></table></div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["### Explode\n\nThe `explode` function allows you to split an array column into multiple rows, copying all the other columns into each new row. \n\nFor example, you can split the column `authors` into the column `author`, with one author per row."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/h8tv263d04?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/h8tv263d04?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       authors, \n       explode(authors) AS author, \n       link \nFROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>authors</th><th>author</th><th>link</th></tr></thead><tbody><tr><td>MapR Integrates the Complete Apache Spark Stack</td><td>List(Tomer Shiran (VP of Product Management at MapR))</td><td>Tomer Shiran (VP of Product Management at MapR)</td><td>https://databricks.com/blog/2014/04/10/mapr-integrates-spark-stack.html</td></tr><tr><td>Apache Spark 0.9.1 Released</td><td>List(Tathagata Das)</td><td>Tathagata Das</td><td>https://databricks.com/blog/2014/04/09/spark-0_9_1-released.html</td></tr><tr><td>Application Spotlight: Alpine Data Labs</td><td>List(Steven Hillion)</td><td>Steven Hillion</td><td>https://databricks.com/blog/2014/03/31/application-spotlight-alpine.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Apache Spark 0.9.0 Released</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/02/03/release-0_9_0.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ali Ghodsi</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Michael Ruggiero (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>List(Jai Ranganathan, Matei Zaharia)</td><td>Jai Ranganathan</td><td>https://databricks.com/blog/2014/03/20/apache-spark-a-delight-for-developers.html</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>List(Jai Ranganathan, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2014/03/20/apache-spark-a-delight-for-developers.html</td></tr><tr><td>Databricks announces \"Certified on Apache Spark\" Program</td><td>List(Databricks Press Office)</td><td>Databricks Press Office</td><td>https://databricks.com/blog/2014/03/18/spark-certification.html</td></tr><tr><td>Apache Spark Now a Top-level Apache Project</td><td>List(Ion Stoica)</td><td>Ion Stoica</td><td>https://databricks.com/blog/2014/03/02/spark-apache-top-level-project.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>Databricks at the O'Reilly Strata Conference 2014</td><td>List(Pat McDonough)</td><td>Pat McDonough</td><td>https://databricks.com/blog/2014/02/10/strata-santa-clara-2014.html</td></tr><tr><td>Apache Spark and Hadoop: Working Together</td><td>List(Ion Stoica)</td><td>Ion Stoica</td><td>https://databricks.com/blog/2014/01/21/spark-and-hadoop.html</td></tr><tr><td>Apache Spark 0.8.1 Released</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2013/12/19/release-0_8_1.html</td></tr><tr><td>Highlights From Spark Summit 2013</td><td>List(Andy Konwinski)</td><td>Andy Konwinski</td><td>https://databricks.com/blog/2013/12/18/spark-summit-2013-follow-up.html</td></tr><tr><td>Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications</td><td>List(Pat McDonough)</td><td>Pat McDonough</td><td>https://databricks.com/blog/2013/11/21/putting-spark-to-use.html</td></tr><tr><td>Databricks and Cloudera Partner to Support Apache Spark</td><td>List(Ion Stoica)</td><td>Ion Stoica</td><td>https://databricks.com/blog/2013/10/28/databricks-and-cloudera-partner-to-support-spark.html</td></tr><tr><td>The Growing Apache Spark Community</td><td>List(Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2013/10/27/the-growing-spark-community.html</td></tr><tr><td>Databricks and the Apache Spark Platform</td><td>List(Ion Stoica, Matei Zaharia)</td><td>Ion Stoica</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td></tr><tr><td>Databricks and the Apache Spark Platform</td><td>List(Ion Stoica, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td></tr><tr><td>Databricks and MapR</td><td>List(Arsalan Tavakoli-Shiraji)</td><td>Arsalan Tavakoli-Shiraji</td><td>https://databricks.com/blog/2014/04/10/partnership-between-databricks-and-mapr.html</td></tr><tr><td>Making Apache Spark Easier to Use in Java with Java 8</td><td>List(Prashant Sharma, Matei Zaharia)</td><td>Prashant Sharma</td><td>https://databricks.com/blog/2014/04/14/spark-with-java-8.html</td></tr><tr><td>Making Apache Spark Easier to Use in Java with Java 8</td><td>List(Prashant Sharma, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2014/04/14/spark-with-java-8.html</td></tr><tr><td>Databricks Announces Apache Spark Training Workshops</td><td>List(Databricks Training Team)</td><td>Databricks Training Team</td><td>https://databricks.com/blog/2014/06/02/databricks-hands-on-technical-workshops.html</td></tr><tr><td>Application Spotlight: Atigeo xPatterns</td><td>List(Claudiu Barbura (Sr. Dir. of Engineering at Atigeo LLC))</td><td>Claudiu Barbura (Sr. Dir. of Engineering at Atigeo LLC)</td><td>https://databricks.com/blog/2014/05/22/application-spotlight-atigeo-xpatterns.html</td></tr><tr><td>Pivotal Hadoop Integrates the Full Apache Spark Stack</td><td>List(Sarabjeet Chugh (Head of Hadoop Product Management at Pivotal Inc.))</td><td>Sarabjeet Chugh (Head of Hadoop Product Management at Pivotal Inc.)</td><td>https://databricks.com/blog/2014/05/23/pivotal-hadoop-integrates-the-full-apache-spark-stack.html</td></tr><tr><td>Announcing Apache Spark 1.0</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/05/30/announcing-spark-1-0.html</td></tr><tr><td>Exciting Performance Improvements on the Horizon for Spark SQL</td><td>List(Michael Armbrust, Zongheng Yang)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html</td></tr><tr><td>Exciting Performance Improvements on the Horizon for Spark SQL</td><td>List(Michael Armbrust, Zongheng Yang)</td><td>Zongheng Yang</td><td>https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html</td></tr><tr><td>MicroStrategy \"Certified on Apache Spark\"</td><td>List(Michael Hiskey (VP at MicroStrategy Inc.))</td><td>Michael Hiskey (VP at MicroStrategy Inc.)</td><td>https://databricks.com/blog/2014/06/04/microstrategy-certified-on-spark.html</td></tr><tr><td>Application Spotlight: Arimo</td><td>List(Christopher Nguyen (CEO &amp; Co-Founder of Adatao))</td><td>Christopher Nguyen (CEO &amp; Co-Founder of Adatao)</td><td>https://databricks.com/blog/2014/06/11/application-spotlight-arimo.html</td></tr><tr><td>Spark Summit 2014 Brings Together Apache Spark Community</td><td>List(Databricks Press Office)</td><td>Databricks Press Office</td><td>https://databricks.com/blog/2014/06/11/spark-summit-2014-brings-together-apache-spark-community.html</td></tr><tr><td>Application Spotlight: Lightbend</td><td>List(Dean Wampler (Typesafe))</td><td>Dean Wampler (Typesafe)</td><td>https://databricks.com/blog/2014/06/13/application-spotlight-lightbend.html</td></tr><tr><td>Application Spotlight: Apervi</td><td>List(Hari Kodakalla (EVP at Apervi Inc.))</td><td>Hari Kodakalla (EVP at Apervi Inc.)</td><td>https://databricks.com/blog/2014/06/23/application-spotlight-apervi.html</td></tr><tr><td>Application Spotlight: Qlik</td><td>List(Bill Kehoe (Big Data Architect at Qlik))</td><td>Bill Kehoe (Big Data Architect at Qlik)</td><td>https://databricks.com/blog/2014/06/24/application-spotlight-qlik.html</td></tr><tr><td>Databricks Launches \"Certified Apache Spark Distribution\" Program</td><td>List(Databricks Press Office)</td><td>Databricks Press Office</td><td>https://databricks.com/blog/2014/06/26/databricks-launches-certified-spark-distribution-program.html</td></tr><tr><td>Application Spotlight: Elasticsearch</td><td>List(Costin Leau (Engineer at Elasticsearch))</td><td>Costin Leau (Engineer at Elasticsearch)</td><td>https://databricks.com/blog/2014/06/27/application-spotlight-elasticsearch.html</td></tr><tr><td>Application Spotlight: Pentaho</td><td>List(Jake Cornelius (SVP of Product Management at Pentaho))</td><td>Jake Cornelius (SVP of Product Management at Pentaho)</td><td>https://databricks.com/blog/2014/06/30/application-spotlight-pentaho.html</td></tr><tr><td>Sparkling Water = H20 + Apache Spark</td><td>List(SriSatish Ambati (CEO of 0xData))</td><td>SriSatish Ambati (CEO of 0xData)</td><td>https://databricks.com/blog/2014/06/30/sparkling-water-h20-spark.html</td></tr><tr><td>Databricks Unveils Apache Spark-Based Cloud Platform; Announces Series B Funding</td><td>List(Databricks Press Office)</td><td>Databricks Press Office</td><td>https://databricks.com/blog/2014/06/30/databricks-unveils-spark-based-cloud-platform.html</td></tr><tr><td>Databricks Application Spotlight at Spark Summit 2014</td><td>List(Arsalan Tavakoli-Shiraji)</td><td>Arsalan Tavakoli-Shiraji</td><td>https://databricks.com/blog/2014/04/28/databricks-application-spotlight-at-spark-summit-2014.html</td></tr><tr><td>Databricks and Datastax</td><td>List(Arsalan Tavakoli-Shiraji)</td><td>Arsalan Tavakoli-Shiraji</td><td>https://databricks.com/blog/2014/05/08/databricks-and-datastax.html</td></tr><tr><td>Databricks Partners with Simba to Deliver Shark ODBC Driver</td><td>List(Databricks Press Office)</td><td>Databricks Press Office</td><td>https://databricks.com/blog/2014/04/30/databricks-partners-with-simba-to-deliver-shark-odbc-driver.html</td></tr><tr><td>Databricks Announces Partnership with SAP</td><td>List(Databricks Press Office)</td><td>Databricks Press Office</td><td>https://databricks.com/blog/2014/07/01/databricks-announces-partnership-with-sap.html</td></tr><tr><td>Integrating Apache Spark and HANA</td><td>List(Arsalan Tavakoli-Shiraji)</td><td>Arsalan Tavakoli-Shiraji</td><td>https://databricks.com/blog/2014/07/01/integrating-spark-and-hana.html</td></tr><tr><td>Shark, Spark SQL, Hive on Spark, and the future of SQL on Apache Spark</td><td>List(Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html</td></tr><tr><td>Databricks: Making Big Data Easy</td><td>List(Ion Stoica)</td><td>Ion Stoica</td><td>https://databricks.com/blog/2014/07/14/databricks-cloud-making-big-data-easy.html</td></tr><tr><td>New Features in MLlib in Apache Spark 1.0</td><td>List(Xiangrui Meng)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/07/16/new-features-in-mllib-in-spark-1-0.html</td></tr><tr><td>The State of Apache Spark in 2014</td><td>List(Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2014/07/18/the-state-of-apache-spark-in-2014.html</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>Burak Yavuz</td><td>https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html</td></tr><tr><td>Distributing the Singular Value Decomposition with Apache Spark</td><td>List(Li Pu, Reza Zadeh)</td><td>Li Pu</td><td>https://databricks.com/blog/2014/07/21/distributing-the-singular-value-decomposition-with-spark.html</td></tr><tr><td>Distributing the Singular Value Decomposition with Apache Spark</td><td>List(Li Pu, Reza Zadeh)</td><td>Reza Zadeh</td><td>https://databricks.com/blog/2014/07/21/distributing-the-singular-value-decomposition-with-spark.html</td></tr><tr><td>Spark Summit 2014 Highlights</td><td>List(Scott Walent)</td><td>Scott Walent</td><td>https://databricks.com/blog/2014/07/22/spark-summit-2014-highlights.html</td></tr><tr><td>When Stratio Met Apache Spark: A True Love Story</td><td>List(Oscar Mendez (CEO of Stratio))</td><td>Oscar Mendez (CEO of Stratio)</td><td>https://databricks.com/blog/2014/08/08/when-stratio-met-spark-a-true-love-story.html</td></tr><tr><td>Mining Ecommerce Graph Data with Apache Spark at Alibaba Taobao</td><td>List(Andy Huang (Alibaba Taobao Data Mining Team), Wei Wu (Alibaba Taobao Data Mining Team))</td><td>Andy Huang (Alibaba Taobao Data Mining Team)</td><td>https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html</td></tr><tr><td>Mining Ecommerce Graph Data with Apache Spark at Alibaba Taobao</td><td>List(Andy Huang (Alibaba Taobao Data Mining Team), Wei Wu (Alibaba Taobao Data Mining Team))</td><td>Wei Wu (Alibaba Taobao Data Mining Team)</td><td>https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Doris Xin</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Burak Yavuz</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Hossein Falaki</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Announcing Apache Spark 1.1</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/09/11/announcing-spark-1-1.html</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>Arsalan Tavakoli-Shiraji</td><td>https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>Tathagata Das</td><td>https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html</td></tr><tr><td>Apache Spark 1.1: MLlib Performance Improvements</td><td>List(Burak Yavuz, Xiangrui Meng)</td><td>Burak Yavuz</td><td>https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html</td></tr><tr><td>Apache Spark 1.1: MLlib Performance Improvements</td><td>List(Burak Yavuz, Xiangrui Meng)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html</td></tr><tr><td>Application Spotlight: Talend</td><td>List(Gavin Targonski (Product Management at Talend))</td><td>Gavin Targonski (Product Management at Talend)</td><td>https://databricks.com/blog/2014/09/15/application-spotlight-talend.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Nick Pentreath (Graphflow)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Kan Zhang (IBM)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr><tr><td>Databricks Reference Applications</td><td>List(Vida Ha)</td><td>Vida Ha</td><td>https://databricks.com/blog/2014/09/23/databricks-reference-applications.html</td></tr><tr><td>Databricks and O'Reilly Media launch Certification Program for Apache Spark Developers</td><td>List(John Tripier, Paco Nathan)</td><td>John Tripier</td><td>https://databricks.com/blog/2014/09/18/databricks-and-oreilly-media-launch-certification-program-for-apache-spark-developers.html</td></tr><tr><td>Databricks and O'Reilly Media launch Certification Program for Apache Spark Developers</td><td>List(John Tripier, Paco Nathan)</td><td>Paco Nathan</td><td>https://databricks.com/blog/2014/09/18/databricks-and-oreilly-media-launch-certification-program-for-apache-spark-developers.html</td></tr><tr><td>Apache Spark Improves the Economics of Video Distribution at NBC Universal</td><td>List(Christopher Burdorf (Senior Software Engineer at NBC Universal))</td><td>Christopher Burdorf (Senior Software Engineer at NBC Universal)</td><td>https://databricks.com/blog/2014/09/24/apache-spark-improves-the-economics-of-video-distribution-at-nbc-universal.html</td></tr><tr><td>Scalable Decision Trees in MLlib</td><td>List(Manish Amde (Origami Logic), Joseph Bradley (Databricks))</td><td>Manish Amde (Origami Logic)</td><td>https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html</td></tr><tr><td>Scalable Decision Trees in MLlib</td><td>List(Manish Amde (Origami Logic), Joseph Bradley (Databricks))</td><td>Joseph Bradley (Databricks)</td><td>https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html</td></tr><tr><td>Guavus Embeds Apache Spark into its Operational Intelligence Platform Deployed at the World’s Largest Telcos</td><td>List(Eric Carr (VP Core Systems Group at Guavus))</td><td>Eric Carr (VP Core Systems Group at Guavus)</td><td>https://databricks.com/blog/2014/09/25/guavus-embeds-apache-spark-into-its-operational-intelligence-platform-deployed-at-the-worlds-largest-telcos.html</td></tr><tr><td>Apache Spark as a platform for large-scale neuroscience</td><td>List(Jeremy Freeman (Freeman Lab))</td><td>Jeremy Freeman (Freeman Lab)</td><td>https://databricks.com/blog/2014/10/01/spark-as-a-platform-for-large-scale-neuroscience.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Advertisers' Return on Marketing Investment</td><td>List(Russell Cardullo (Sharethrough))</td><td>Russell Cardullo (Sharethrough)</td><td>https://databricks.com/blog/2014/10/07/sharethrough-uses-spark-streaming-to-optimize-advertisers-return-on-marketing-investment.html</td></tr><tr><td>Application Spotlight: Trifacta</td><td>List(Sean Kandel (CTO at Trifacta))</td><td>Sean Kandel (CTO at Trifacta)</td><td>https://databricks.com/blog/2014/10/09/application-spotlight-trifacta.html</td></tr><tr><td>Apache Spark the fastest open source engine for sorting a petabyte</td><td>List(Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html</td></tr><tr><td>Efficient similarity algorithm now in Apache Spark, thanks to Twitter</td><td>List(Reza Zadeh)</td><td>Reza Zadeh</td><td>https://databricks.com/blog/2014/10/20/efficient-similarity-algorithm-now-in-spark-twitter.html</td></tr><tr><td>Application Spotlight: Tableau Software</td><td>List(Jeff Feng (Product Manager at Tableau Software))</td><td>Jeff Feng (Product Manager at Tableau Software)</td><td>https://databricks.com/blog/2014/10/15/application-spotlight-tableau-software.html</td></tr><tr><td>Spark Summit East - CFP now open</td><td>List(Scott Walent)</td><td>Scott Walent</td><td>https://databricks.com/blog/2014/10/23/spark-summit-east-cfp-now-open.html</td></tr><tr><td>Application Spotlight: Faimdata</td><td>List(Ari Himmel (CEO at Faimdata), Nan Zhu (Chief Architect at Faimdata))</td><td>Ari Himmel (CEO at Faimdata)</td><td>https://databricks.com/blog/2014/10/27/application-spotlight-faimdata.html</td></tr><tr><td>Application Spotlight: Faimdata</td><td>List(Ari Himmel (CEO at Faimdata), Nan Zhu (Chief Architect at Faimdata))</td><td>Nan Zhu (Chief Architect at Faimdata)</td><td>https://databricks.com/blog/2014/10/27/application-spotlight-faimdata.html</td></tr><tr><td>Hortonworks: A shared vision for Apache Spark on Hadoop</td><td>List(John Kreisa (VP of Strategic Marketing at Hortonworks))</td><td>John Kreisa (VP of Strategic Marketing at Hortonworks)</td><td>https://databricks.com/blog/2014/10/31/hortonworks-a-shared-vision-for-apache-spark-on-hadoop.html</td></tr><tr><td>Application Spotlight: Skytree Infinity</td><td>List(Sachin Chawla (VP of Engineering))</td><td>Sachin Chawla (VP of Engineering)</td><td>https://databricks.com/blog/2014/11/24/application-spotlight-skytree-infinity.html</td></tr><tr><td>Application Spotlight: Nube Reifier</td><td>List(Sonal Goyal (CEO))</td><td>Sonal Goyal (CEO)</td><td>https://databricks.com/blog/2014/12/02/application-spotlight-nube-reifier.html</td></tr><tr><td>Pearson uses Apache Spark Streaming for next generation adaptive learning platform</td><td>List( Dibyendu Bhattacharya (Big Data Architect))</td><td> Dibyendu Bhattacharya (Big Data Architect)</td><td>https://databricks.com/blog/2014/12/08/pearson-uses-spark-streaming-for-next-generation-adaptive-learning-platform.html</td></tr><tr><td>Apache Spark officially sets a new record in large-scale sorting</td><td>List(Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html</td></tr><tr><td>Application Spotlight: Bedrock</td><td>List(Matt MacKinnon (Director of Product Management at Zaloni))</td><td>Matt MacKinnon (Director of Product Management at Zaloni)</td><td>https://databricks.com/blog/2014/11/14/application-spotlight-bedrock.html</td></tr><tr><td>The Apache Spark Certified Developer Program</td><td>List(John Tripier, Paco Nathan)</td><td>John Tripier</td><td>https://databricks.com/blog/2014/11/14/the-spark-certified-developer-program.html</td></tr><tr><td>The Apache Spark Certified Developer Program</td><td>List(John Tripier, Paco Nathan)</td><td>Paco Nathan</td><td>https://databricks.com/blog/2014/11/14/the-spark-certified-developer-program.html</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>Luis Quintela (Sr. Manager of Big Data Analytics)</td><td>https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>Yan Breek (Data Scientist)</td><td>https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>Girish Kathalagiri (Data Analytics Engineer)</td><td>https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html</td></tr><tr><td>Databricks to run two massive online courses on Apache Spark</td><td>List(Ameet Talwalkar, Anthony Joseph)</td><td>Ameet Talwalkar</td><td>https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html</td></tr><tr><td>Databricks to run two massive online courses on Apache Spark</td><td>List(Ameet Talwalkar, Anthony Joseph)</td><td>Anthony Joseph</td><td>https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html</td></tr><tr><td>Application Spotlight: Technicolor Virdata Internet of Things platform</td><td>List(Lieven Gesquiere (Virdata Lead Core R&D))</td><td>Lieven Gesquiere (Virdata Lead Core R&D)</td><td>https://databricks.com/blog/2014/12/03/application-spotlight-technicolor-virdata-internet-of-things-platform.html</td></tr><tr><td>Databricks Expands Bay Area Presence, Moves HQ to San Francisco</td><td>List(by Databricks Press Office)</td><td>by Databricks Press Office</td><td>https://databricks.com/blog/2015/01/13/databricks-expands-bay-area-presence-moves-hq-to-san-francisco.html</td></tr><tr><td>Apache Spark Certified Developer exams available online!</td><td>List(Kavitha Mariappan)</td><td>Kavitha Mariappan</td><td>https://databricks.com/blog/2015/01/16/spark-certified-developer-exams-available-online.html</td></tr><tr><td>Spark Summit East 2015 Agenda is Now Available</td><td>List(Kavitha Mariappan)</td><td>Kavitha Mariappan</td><td>https://databricks.com/blog/2015/01/20/spark-summit-east-2015-agenda-is-now-available.html</td></tr><tr><td>An introduction to JSON support in Spark SQL</td><td>List(Yin Huai (Databricks))</td><td>Yin Huai (Databricks)</td><td>https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html</td></tr><tr><td>Introducing streaming k-means in Apache Spark 1.2</td><td>List(Jeremy Freeman (Howard Hughes Medical Institute))</td><td>Jeremy Freeman (Howard Hughes Medical Institute)</td><td>https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html</td></tr><tr><td>Apache Spark selected for Infoworld 2015 Technology of the Year Award</td><td>List(Dave Wang (Databricks))</td><td>Dave Wang (Databricks)</td><td>https://databricks.com/blog/2015/02/05/apache-spark-selected-for-infoworld-2015-technology-of-the-year-award.html</td></tr><tr><td>Announcing Apache Spark 1.2</td><td>List(Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/12/19/announcing-spark-1-2.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Joseph Bradley</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Evan Sparks (UC Berkeley)</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Shivaram Venkataraman (UC Berkeley)</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform</td><td>List(Michael Armbrust)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</td></tr><tr><td>Random Forests and Boosting in MLlib</td><td>List(Joseph K. Bradley (Databricks), Manish Amde (Origami Logic))</td><td>Joseph K. Bradley (Databricks)</td><td>https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html</td></tr><tr><td>Random Forests and Boosting in MLlib</td><td>List(Joseph K. Bradley (Databricks), Manish Amde (Origami Logic))</td><td>Manish Amde (Origami Logic)</td><td>https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html</td></tr><tr><td>Improved Fault-tolerance and Zero Data Loss in Apache Spark Streaming</td><td>List(Tathagata Das)</td><td>Tathagata Das</td><td>https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html</td></tr><tr><td>Big data projects are hungry for simpler and more powerful tools: Survey validates Apache Spark is gaining developer traction!</td><td>List(Kavitha Mariappan)</td><td>Kavitha Mariappan</td><td>https://databricks.com/blog/2015/01/27/big-data-projects-are-hungry-for-simpler-and-more-powerful-tools-survey-validates-apache-spark-is-gaining-developer-traction.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Holden Karau</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Andy Konwinski</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr></tbody></table></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["It's more obvious to restrict the output to articles that have multiple authors, and sort by the title."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title, \n       authors, \n       explode(authors) AS author, \n       link \nFROM DatabricksBlog \nWHERE size(authors) > 1 \nORDER BY title"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>authors</th><th>author</th><th>link</th></tr></thead><tbody><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Holden Karau</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Andy Konwinski</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>\"Learning Spark\" book available from O'Reilly</td><td>List(Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2015/02/09/learning-spark-book-available-from-oreilly.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>AMPLab updates the Big Data Benchmark</td><td>List(Ahir Reddy, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/02/12/big-data-benchmark.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Announcing Apache Spark Packages</td><td>List(Xiangrui Meng, Patrick Wendell)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/12/22/announcing-spark-packages.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Nick Pentreath (Graphflow)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr><tr><td>Apache Spark 1.1: Bringing Hadoop Input/Output Formats to PySpark</td><td>List(Nick Pentreath (Graphflow), Kan Zhang (IBM))</td><td>Kan Zhang (IBM)</td><td>https://databricks.com/blog/2014/09/17/spark-1-1-bringing-hadoop-inputoutput-formats-to-pyspark.html</td></tr><tr><td>Apache Spark 1.1: MLlib Performance Improvements</td><td>List(Burak Yavuz, Xiangrui Meng)</td><td>Burak Yavuz</td><td>https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html</td></tr><tr><td>Apache Spark 1.1: MLlib Performance Improvements</td><td>List(Burak Yavuz, Xiangrui Meng)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>Arsalan Tavakoli-Shiraji</td><td>https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>Tathagata Das</td><td>https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html</td></tr><tr><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>Patrick Wendell</td><td>https://databricks.com/blog/2014/09/16/spark-1-1-the-state-of-spark-streaming.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ali Ghodsi</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Apache Spark In MapReduce (SIMR)</td><td>List(Ali Ghodsi, Ahir Reddy)</td><td>Ahir Reddy</td><td>https://databricks.com/blog/2014/01/01/simr.html</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>List(Jai Ranganathan, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2014/03/20/apache-spark-a-delight-for-developers.html</td></tr><tr><td>Apache Spark: A Delight for Developers</td><td>List(Jai Ranganathan, Matei Zaharia)</td><td>Jai Ranganathan</td><td>https://databricks.com/blog/2014/03/20/apache-spark-a-delight-for-developers.html</td></tr><tr><td>Application Spotlight: Faimdata</td><td>List(Ari Himmel (CEO at Faimdata), Nan Zhu (Chief Architect at Faimdata))</td><td>Ari Himmel (CEO at Faimdata)</td><td>https://databricks.com/blog/2014/10/27/application-spotlight-faimdata.html</td></tr><tr><td>Application Spotlight: Faimdata</td><td>List(Ari Himmel (CEO at Faimdata), Nan Zhu (Chief Architect at Faimdata))</td><td>Nan Zhu (Chief Architect at Faimdata)</td><td>https://databricks.com/blog/2014/10/27/application-spotlight-faimdata.html</td></tr><tr><td>Databricks and O'Reilly Media launch Certification Program for Apache Spark Developers</td><td>List(John Tripier, Paco Nathan)</td><td>Paco Nathan</td><td>https://databricks.com/blog/2014/09/18/databricks-and-oreilly-media-launch-certification-program-for-apache-spark-developers.html</td></tr><tr><td>Databricks and O'Reilly Media launch Certification Program for Apache Spark Developers</td><td>List(John Tripier, Paco Nathan)</td><td>John Tripier</td><td>https://databricks.com/blog/2014/09/18/databricks-and-oreilly-media-launch-certification-program-for-apache-spark-developers.html</td></tr><tr><td>Databricks and the Apache Spark Platform</td><td>List(Ion Stoica, Matei Zaharia)</td><td>Ion Stoica</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td></tr><tr><td>Databricks and the Apache Spark Platform</td><td>List(Ion Stoica, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2013/10/27/databricks-and-the-apache-spark-platform.html</td></tr><tr><td>Databricks to run two massive online courses on Apache Spark</td><td>List(Ameet Talwalkar, Anthony Joseph)</td><td>Ameet Talwalkar</td><td>https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html</td></tr><tr><td>Databricks to run two massive online courses on Apache Spark</td><td>List(Ameet Talwalkar, Anthony Joseph)</td><td>Anthony Joseph</td><td>https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html</td></tr><tr><td>Distributing the Singular Value Decomposition with Apache Spark</td><td>List(Li Pu, Reza Zadeh)</td><td>Reza Zadeh</td><td>https://databricks.com/blog/2014/07/21/distributing-the-singular-value-decomposition-with-spark.html</td></tr><tr><td>Distributing the Singular Value Decomposition with Apache Spark</td><td>List(Li Pu, Reza Zadeh)</td><td>Li Pu</td><td>https://databricks.com/blog/2014/07/21/distributing-the-singular-value-decomposition-with-spark.html</td></tr><tr><td>Exciting Performance Improvements on the Horizon for Spark SQL</td><td>List(Michael Armbrust, Zongheng Yang)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html</td></tr><tr><td>Exciting Performance Improvements on the Horizon for Spark SQL</td><td>List(Michael Armbrust, Zongheng Yang)</td><td>Zongheng Yang</td><td>https://databricks.com/blog/2014/06/02/exciting-performance-improvements-on-the-horizon-for-spark-sql.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Shivaram Venkataraman (UC Berkeley)</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Evan Sparks (UC Berkeley)</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>ML Pipelines: A New High-Level API for MLlib</td><td>List(Xiangrui Meng, Joseph Bradley, Evan Sparks (UC Berkeley), Shivaram Venkataraman (UC Berkeley))</td><td>Joseph Bradley</td><td>https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</td></tr><tr><td>Making Apache Spark Easier to Use in Java with Java 8</td><td>List(Prashant Sharma, Matei Zaharia)</td><td>Prashant Sharma</td><td>https://databricks.com/blog/2014/04/14/spark-with-java-8.html</td></tr><tr><td>Making Apache Spark Easier to Use in Java with Java 8</td><td>List(Prashant Sharma, Matei Zaharia)</td><td>Matei Zaharia</td><td>https://databricks.com/blog/2014/04/14/spark-with-java-8.html</td></tr><tr><td>Mining Ecommerce Graph Data with Apache Spark at Alibaba Taobao</td><td>List(Andy Huang (Alibaba Taobao Data Mining Team), Wei Wu (Alibaba Taobao Data Mining Team))</td><td>Andy Huang (Alibaba Taobao Data Mining Team)</td><td>https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html</td></tr><tr><td>Mining Ecommerce Graph Data with Apache Spark at Alibaba Taobao</td><td>List(Andy Huang (Alibaba Taobao Data Mining Team), Wei Wu (Alibaba Taobao Data Mining Team))</td><td>Wei Wu (Alibaba Taobao Data Mining Team)</td><td>https://databricks.com/blog/2014/08/14/mining-graph-data-with-spark-at-alibaba-taobao.html</td></tr><tr><td>Random Forests and Boosting in MLlib</td><td>List(Joseph K. Bradley (Databricks), Manish Amde (Origami Logic))</td><td>Manish Amde (Origami Logic)</td><td>https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html</td></tr><tr><td>Random Forests and Boosting in MLlib</td><td>List(Joseph K. Bradley (Databricks), Manish Amde (Origami Logic))</td><td>Joseph K. Bradley (Databricks)</td><td>https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>Luis Quintela (Sr. Manager of Big Data Analytics)</td><td>https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>Yan Breek (Data Scientist)</td><td>https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html</td></tr><tr><td>Samsung SDS uses Apache Spark for prescriptive analytics at large scale</td><td>List(Luis Quintela (Sr. Manager of Big Data Analytics), Yan Breek (Data Scientist), Girish Kathalagiri (Data Analytics Engineer))</td><td>Girish Kathalagiri (Data Analytics Engineer)</td><td>https://databricks.com/blog/2014/11/21/samsung-sds-uses-spark-for-prescriptive-analytics-at-large-scale.html</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html</td></tr><tr><td>Scalable Collaborative Filtering with Apache Spark MLlib</td><td>List(Burak Yavuz, Xiangrui Meng, Reynold Xin)</td><td>Burak Yavuz</td><td>https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html</td></tr><tr><td>Scalable Decision Trees in MLlib</td><td>List(Manish Amde (Origami Logic), Joseph Bradley (Databricks))</td><td>Joseph Bradley (Databricks)</td><td>https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html</td></tr><tr><td>Scalable Decision Trees in MLlib</td><td>List(Manish Amde (Origami Logic), Joseph Bradley (Databricks))</td><td>Manish Amde (Origami Logic)</td><td>https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Russell Cardullo (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr><tr><td>Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time</td><td>List(Russell Cardullo (Data Infrastructure Engineer at Sharethrough), Michael Ruggiero (Data Infrastructure Engineer at Sharethrough))</td><td>Michael Ruggiero (Data Infrastructure Engineer at Sharethrough)</td><td>https://databricks.com/blog/2014/03/25/sharethrough-and-spark-streaming.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Michael Armbrust</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td><td>List(Michael Armbrust, Reynold Xin)</td><td>Reynold Xin</td><td>https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Doris Xin</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Burak Yavuz</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Xiangrui Meng</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>Statistics Functionality in Apache Spark 1.1</td><td>List(Doris Xin, Burak Yavuz, Xiangrui Meng, Hossein Falaki)</td><td>Hossein Falaki</td><td>https://databricks.com/blog/2014/08/27/statistics-functionality-in-spark.html</td></tr><tr><td>The Apache Spark Certified Developer Program</td><td>List(John Tripier, Paco Nathan)</td><td>John Tripier</td><td>https://databricks.com/blog/2014/11/14/the-spark-certified-developer-program.html</td></tr><tr><td>The Apache Spark Certified Developer Program</td><td>List(John Tripier, Paco Nathan)</td><td>Paco Nathan</td><td>https://databricks.com/blog/2014/11/14/the-spark-certified-developer-program.html</td></tr></tbody></table></div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["### Lateral View\nThe data has multiple columns with nested objects.  In this case, the data has multiple dates, authors, and categories.\n\nTake a look at the blog entry **Apache Spark 1.1: The State of Spark Streaming**:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.publishedOn, title, authors, categories\nFROM DatabricksBlog\nWHERE title = \"Apache Spark 1.1: The State of Spark Streaming\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>publishedOn</th><th>title</th><th>authors</th><th>categories</th></tr></thead><tbody><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>List(Arsalan Tavakoli-Shiraji, Tathagata Das, Patrick Wendell)</td><td>List(Apache Spark, Engineering Blog, Streaming)</td></tr></tbody></table></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["Next, use `LATERAL VIEW` to explode multiple columns at once, in this case, the columns `authors` and `categories`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT dates.publishedOn, title, author, category\nFROM DatabricksBlog\nLATERAL VIEW explode(authors) exploded_authors_view AS author\nLATERAL VIEW explode(categories) exploded_categories AS category\nWHERE title = \"Apache Spark 1.1: The State of Spark Streaming\"\nORDER BY author, category"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>publishedOn</th><th>title</th><th>author</th><th>category</th></tr></thead><tbody><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Arsalan Tavakoli-Shiraji</td><td>Streaming</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Patrick Wendell</td><td>Streaming</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Apache Spark</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Engineering Blog</td></tr><tr><td>2014-09-16</td><td>Apache Spark 1.1: The State of Spark Streaming</td><td>Tathagata Das</td><td>Streaming</td></tr></tbody></table></div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["## Exercise 1\n\nIdentify all the articles written or co-written by Michael Armbrust."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the table `DatabricksBlog`, create a temporary view called `ArticlesByMichael` where:\n0. Michael Armbrust is the author\n0. The data set contains the column `title` (it may contain others)\n0. It contains only one record per article\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">built-in functions</a>.  \n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Include the column `authors` in your view, to help you debug your solution."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW ArticlesByMichael AS\n  SELECT title,author \n  FROM (\n  SELECT title,explode(authors) as author\n  FROM DatabricksBlog)\n  WHERE author='Michael Armbrust'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":44},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\nresultsDF = spark.sql(\"select title from ArticlesByMichael order by title\")\ndbTest(\"SQL-L5-articlesByMichael-count\", 3, resultsDF.count())\n\nresults = [r[0] for r in resultsDF.collect()]\ndbTest(\"SQL-L5-articlesByMichael-0\", \"Exciting Performance Improvements on the Horizon for Spark SQL\", results[0])\ndbTest(\"SQL-L5-articlesByMichael-1\", \"Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform\", results[1])\ndbTest(\"SQL-L5-articlesByMichael-2\", \"Spark SQL: Manipulating Structured Data Using Apache Spark\", results[2])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["### Step 2\nShow the list of Michael Armbrust's articles."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT title FROM ArticlesByMichael"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th></tr></thead><tbody><tr><td>Spark SQL: Manipulating Structured Data Using Apache Spark</td></tr><tr><td>Exciting Performance Improvements on the Horizon for Spark SQL</td></tr><tr><td>Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform</td></tr></tbody></table></div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["## Exercise 2\n\nIdentify the complete set of categories used in the Databricks blog articles."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nStarting with the table `DatabricksBlog`, create another view called `UniqueCategories` where:\n0. The data set contains the one column `category` (and no others)\n0. This list of categories should be unique"],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW UniqueCategories AS\n  SELECT DISTINCT explode(categories) AS category\n  FROM DatabricksBlog"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":50},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\nresultsCount = spark.sql(\"SELECT category FROM UniqueCategories order by category\")\n\ndbTest(\"SQL-L5-uniqueCategories-count\", 12, resultsCount.count())\n\nresults = [r[0] for r in resultsCount.collect()]\ndbTest(\"SQL-L5-uniqueCategories-0\", \"Announcements\", results[0])\ndbTest(\"SQL-L5-uniqueCategories-1\", \"Apache Spark\", results[1])\ndbTest(\"SQL-L5-uniqueCategories-2\", \"Company Blog\", results[2])\n\ndbTest(\"SQL-L5-uniqueCategories-9\", \"Platform\", results[9])\ndbTest(\"SQL-L5-uniqueCategories-10\", \"Product\", results[10])\ndbTest(\"SQL-L5-uniqueCategories-11\", \"Streaming\", results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["### Step 2\nShow the complete list of categories."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM UniqueCategories"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th></tr></thead><tbody><tr><td>Customers</td></tr><tr><td>Machine Learning</td></tr><tr><td>Apache Spark</td></tr><tr><td>Announcements</td></tr><tr><td>Company Blog</td></tr><tr><td>Engineering Blog</td></tr><tr><td>Ecosystem</td></tr><tr><td>Streaming</td></tr><tr><td>Events</td></tr><tr><td>Platform</td></tr><tr><td>Partners</td></tr><tr><td>Product</td></tr></tbody></table></div>"]}}],"execution_count":53},{"cell_type":"markdown","source":["## Exercise 3\n\nCount how many times each category is referenced in the Databricks blog."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the table `DatabricksBlog`, create a temporary view called `TotalArticlesByCategory` where:\n0. The new table contains two columns, `category` and `total`\n0. The `category` column is a single, distinct category (similar to the last exercise)\n0. The `total` column is the total number of articles in that category\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You need either multiple views or a `LATERAL VIEW` to solve this.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Because articles can be tagged with multiple categories, the sum of the totals adds up to more than the total number of articles."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW TotalArticlesByCategory AS\n  SELECT category,count(title) as total\n  FROM DatabricksBlog\n  LATERAL VIEW explode(categories) exploded_categories AS category\n  GROUP BY category"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":56},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\nresultsDF = spark.sql(\"SELECT category, total FROM TotalArticlesByCategory ORDER BY category\")\ndbTest(\"SQL-L5-articlesByCategory-count\", 12, resultsDF.count())\n\nresults = [ (r[0]+\" w/\"+str(r[1])) for r in resultsDF.collect()]\n\ndbTest(\"SQL-L5-articlesByCategory-0\", \"Announcements w/72\", results[0])\ndbTest(\"SQL-L5-articlesByCategory-1\", \"Apache Spark w/132\", results[1])\ndbTest(\"SQL-L5-articlesByCategory-2\", \"Company Blog w/224\", results[2])\n\ndbTest(\"SQL-L5-articlesByCategory-9\", \"Platform w/4\", results[9])\ndbTest(\"SQL-L5-articlesByCategory-10\", \"Product w/83\", results[10])\ndbTest(\"SQL-L5-articlesByCategory-11\", \"Streaming w/21\", results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":57},{"cell_type":"markdown","source":["### Step 2\nDisplay the totals of each category, order by `category`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT category,total\nFROM TotalArticlesByCategory\nORDER BY category"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total</th></tr></thead><tbody><tr><td>Announcements</td><td>72</td></tr><tr><td>Apache Spark</td><td>132</td></tr><tr><td>Company Blog</td><td>224</td></tr><tr><td>Customers</td><td>34</td></tr><tr><td>Ecosystem</td><td>21</td></tr><tr><td>Engineering Blog</td><td>141</td></tr><tr><td>Events</td><td>52</td></tr><tr><td>Machine Learning</td><td>38</td></tr><tr><td>Partners</td><td>50</td></tr><tr><td>Platform</td><td>4</td></tr><tr><td>Product</td><td>83</td></tr><tr><td>Streaming</td><td>21</td></tr></tbody></table></div>"]}}],"execution_count":59},{"cell_type":"markdown","source":["## Summary\n\n* Spark SQL allows you to query and manipulate structured and semi-structured data\n* Spark SQL's built-in functions provide powerful primitives for querying complex schemas"],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is the syntax for accessing nested columns?  \n**A:** Use the dot notation: ```SELECT dates.publishedOn```\n\n**Q:** What is the syntax for accessing the first element in an array?  \n**A:** Use the [subscript] notation:  ```SELECT authors[0]```\n\n**Q:** What is the syntax for expanding an array into multiple rows?  \n**A:** Use the explode keyword, either:  \n```SELECT explode(authors) as Author``` or  \n```LATERAL VIEW explode(authors) exploded_authors_view AS author```"],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/spark/latest/spark-sql/index.html\" target=\"_blank\">Spark SQL Reference</a>\n* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>\n* <a href=\"https://stackoverflow.com/questions/36876959/sparksql-can-i-explode-two-different-variables-in-the-same-query\" target=\"_blank\">SparkSQL: Can I explode two different variables in the same query? (StackOverflow)</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"05-Querying-JSON","notebookId":1627853826155972},"nbformat":4,"nbformat_minor":0}
